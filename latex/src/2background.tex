\chapter{Notation and Terminology}\label{chap:notation}

\inlinetodo{This section has to be done}
Let \(A\) be an operator, then \(A^\ast\) is the adjoint operation. class of linear bounded
operators from two vector spaces.
Hilbert space, Real numbers, matrix, vector, approx linear bounded operators, transpose

\chapter{Imaging Modalities}\label{chap:imaging_modalities}

A large number of different fields boil down to a very similar problem statement. Based on a given
measurement, how can one retrieve the original measured object, assuming the measurement process is
known. This is also often referred to as reconstruction form projections~\cite{herman_basis_2015}.
This is presicly, the definition of reconstruction used throughout this thesis.

These fields include, but are not limited to, variation methods in imaging (e.g.\ image denoising,
inpainting, super resolution and more~\cite{scherzer_variational_2009}), X-ray attenuation CT,
differential phase-contrast CT, X-ray dark-field contrast CT, light-field tomography, seismic
imaging, nondestructive material testing~\cite{carpio_inverse_2008}. All of these problems are part
of the class of so called \textit{inverse problems}.

A little detour to motivate the important of inverse problems: The Hubble Space Telescope was
launched into space in 1990. However, it initially had a problems with the spherical aberration in
its optics. A hardware fix was deployed in 1993. But due to the cost of the operation in general,
the images of the telescope were still used and lot of processing went into the images. A lot of
different techniques were used to recover as much information as possible. This includes techniques
involving the solution of inverse problem similar as discussed in this thesis. The details are out
of scope, but the interested reader can look into~\cite{white_restoration_1992,adorf_hubble_1995}.

\begin{definition}[Inverse Problem]\label{def:inverse_problem}
	Loosely speaking, the solution to inverse problems is the cause of an effect. Turning this
	into the mathematical setting, let \(H\) and \(K\) be Hilbert spaces (i.e.\ a vector space,
	which has a scalar product and is complete), then let \(m \in K\) the measurements and \(f
	\in H\) the original cause. The system which causes the effect can be modeled by a
	linear bounded operator \(A \in \mathscr{L}(H, K)\). Then the problem is formulated as
	\[ A f = m \]
	The inverse problem is to retrieve \(f\) (the cause) given \(m\) (the measurements) and \(A\)
	(the physical model).
\end{definition}

I want to point out a minor, but important detail. Different imaging modalities, which vary
drastically in scope of physical properties, can be mathematically reduces to a common concept. This
enables reasoning on common ground. Improvements in one field, can benefit other fields. Further, a
common language can be used. This, I personally find very much fascinating and is one of my
motivations to keep learning.

Back to the inverse problems. Broadly speaking, problems in math can be categorized in to
categories. Problems are either \textit{well-posed} or they are \textit{ill-posed}. Following the
definition of Hadamard~\cite{hadamard_sur_1902} a problem is well posed if all of the following
properties are fulfilled:

\begin{itemize}
	\item \textbf{Existence}: The problem has a solution.
	\item \textbf{Uniqueness}: The solution is the only solution.
	\item \textbf{Stability}: The solution depends continuously on the data.
\end{itemize}

If any of these conditions doesn't hold it is ill-posed. An example for well-posed problems is the
heat equation with specified initial conditions. Inverse problems are basically all
ill-posed~\cite{hansen_discrete_2010}, at least the ones considereds in this thesis.

For an ill-posed inverse problem, either the inverse \(A^{-1}\) does not exists, the solution does
not lie in \(K\), or is not continuous (c.f.~\cite[chap. 4]{natterer_mathematics_1986}). From a
practical standpoint the first two properties can be solved quite easily. If no solution exists, one
can substitute it by a different problem, e.g.\ the looking for the solution to the least squares
problem. If multiple or infinitely many solutions exists, one can choose the one with minimal norm.
However, if the third property is violated, the solutions to a system with two close measurements
\(g\) and \(g^\epsilon\) need not to be close.

From a practical point this is quite important. Imagen two blurred images of the same scene, that
only differ slightly from one another. If the stability criterion is violated, the solution to the
inverse problem trying to deblur the images, need not to be close to each other. Hence, proper care
needs to be taken in the development of algorithms, that search for solutions for the inverse
problem. See \citeauthor{hansen_discrete_2010}\cite{hansen_discrete_2010} for some nice illustrative
examples for each property.

In the specific field of tomographic reconstruction, the challenge is to reconstruction an image
from a finite set of projections. Specifically, most of the applications considered are projections
from X-ray sources. However, other imaging modalities such as light field microscopy exists.

An important aspect of of tomographic reconstruction is the so called \textit{forward model}. It is
the mathematical abstraction of the physical measuring process (\(A\) in
\autoref{def:inverse_problem}). Throughout this thesis, X-rays are treated as infinitely thin rays,
and therefore, mathematically a single X-ray going through the object can be model using a line.
Along its path through the object the object interacts with the matter and once it passed through it
will hit some kind of detector \todo{Add figure for idea of line integral}. This can either measure
the attenuation of the X-ray, the refraction or the scattering, but the mathematical abstraction is
very similar.

This idea was already studied by \citeauthor{radon_uber_1917}. Hence, the resulting transformation
modeling the projection of an object to its projection is often referred to as the \textit{Radon
	Transform}. For \(2\)-dimensional reconstructions this holds true, but for higher dimensions,
this is not necessarily true anymore. Other transformations, such as the \textit{X-ray
	Transform}~\cite{solmon_x-ray_1976}, have been presented. A closer overview of both the Radon
Transformtion and the X-ray Transformation is given in \autoref{chap:radon_transform_and_related}.

Again, note how the mathematical description closes the gap between different concepts of
measuring X-ray attenuation, refraction and scattering.

The remainder of this chapter will introduce a mainly two different imaging modalities and their
applications: X-ray attenuation CT, and differential X-ray phase-contrast CT\@. This should
also help to gain an intuition for the mathematical introduction and definitions in the following
chapters. This part will cover briefly the acquisition of measurements and the physical process
behind it. \todo{what else will I talk about?}

Out of scope are physical details will not be discussed. Rather, they will be striped down and
simplified to what is necessary and useful for the scope of this thesis. However, as much as
possible, resources for the interested reader are cited in the corresponding sections.

\section{X-ray Attenuation CT}\label{sec:xray_attenuation_ct}

The discovery of X-rays by Wilhelm Conrad Röntgen \insertref{Röntgen Wilhelm Conrad Röntgen} and its
usages have already.
\todo{Cite (medical) applications for each method, write a little more}

The breakthrough for X-ray attenuation CT came with the a series of publications of by
\citeauthor{hounsfield_computerized_1973}\cite{hounsfield_computerized_1973},
\citeauthor{ambrose_computerized_1973}\cite{ambrose_computerized_1973} and
~\citeauthor{perry_computerized_1973}\cite{perry_computerized_1973}. The mathematical foundation was
laid but many, but importantly as already mentioned~\citeauthor{radon_uber_1917}, but also
\citeauthor{cormack_representation_1963}\cite{cormack_representation_1963}.

\inlinetodo{First talk about how CT scanners rotate around object and take measurements}
\inlinetodo{Figure showing attenuation in wave model (i.e. reduction of amplitude of wave)}

For a simple study of the physical process in X-ray attenuation CT, we'll start with the analysis of
a single X-ray going through an object with a homogeneous (i.e.\ single constant) attenuation
coefficient (see \autoref{fig:x-ray_homogeneous_attenuation}). The object one desires to investigae,
is between a X-ray source and a detector. The X-ray source generates X-rays with a certain intensity
\(I_0\). After the X-ray travers the object, the detector measures a reduced intensity denoted as
\(I_1\). The connection between \(I_0\) and \(I_1\) is determined by the attenuation coefficient
\(mu\) and the distance traveled through the matter \(s\). It is given by
\[ I_1 = I_0 e^{-\mu s} \]

\begin{figure}[t]
	\centering
	\def\svgwidth{0.75\textwidth}
	\import{./figures/homogeneous_attenuation}{homogeneous_attenuation.pdf_tex}
	\caption{Simplified model of a single X-ray (red line) going through a material with
		homogeneous attenuation coefficient \(\mu\) (blue rectangle). \(I_0\) is the initial
		intensity of the of the X-ray, \(I_1\) is the measured intensity, given by
		attuenuation coefficient \(mu\) and the distance \(s\) the ray travels through the
		object.}\label{fig:x-ray_homogeneous_attenuation}
\end{figure}

Extending this model to accommodate changing or varying attenuation coefficients, one needs to
replace the constant coefficient \(\mu\), by a function \(\mu: \mathbb{R}^2 \mapsto \mathbb{R}\)
(for now, this is keep in the two dimensional case). Then the measured intensity is given by the
integral along the line the ray travels along \(L\) of the function \(\mu\), see
\autoref{fig:x-ray_nonhomogeneous_attenuation} for an illustration.

\begin{figure}[t]
	\centering
	\def\svgwidth{0.75\textwidth}
	\import{./figures/nonhomogeneous_attenuation}{nonhomogeneous_attenuation.pdf_tex}
	\caption{Model of a single X-ray (red line) going through a material with
		homogeneous attenuation coefficient \(\mu(x)\) (abdominal section). Again \(I_0\) is
		the initial intensity of the of the X-ray, \(I_1\) is the measured intensity, given
		by the integral along the line \(L\) (i.e.\ the ray given in red) of the
		attuenuation coefficient function \(mu(x)\), where \(x\) is each point along the
		line \(L\).}\label{fig:x-ray_nonhomogeneous_attenuation}
\end{figure}

% Some help from https://www.fips.fi/slides/Bubba_SummerSchoolVFIP2019_1.pdf
Then the connection between the initial intensity, and measured intensity is connected by the
\textit{Beer-Lambert law}~\cite{buzug_computed_2008}:
\begin{equation}\label{eq:beer-Lambert-law}
	- \ln \frac{I}{I_0} = \int_L \mu (x) \, \mathrm{d}x
\end{equation}
The right-hand side of \autoref{eq:beer-Lambert-law} is the line integral of the attenuation
coefficient function \(\mu\) along the line \(L\). Such a transformation of a function is referred
to as the \textit{Radon Transform}~\cite{radon_uber_1917} (see~\cite{radon_determination_1986} for
the English translation). The challenge of X-ray attenuation CT, is to reconstruct the attenuation
coefficients \(\mu\) from a finite set of measurements.

So far, only single rays are discussed, but let alone from a practical standpoint, many X-rays
traverse the object. CT scanners usually have a couple of different geometric models that are or
were common. By now rather uncommon, but important for certain theoretical results is the parallel
beam scanner. There all rays traverse the object parallel to each other. Another, common setting is
the fan-bean geometry. It is a \(2\)-dimensional setting, where the X-rays are emitted from the a
point-source and are not parallel to each other, but rather (as the name suggest) have a fan like
pattern. Further, the detector is curved. The last discussed geometry is the so called cone beam
geometry. As it's name suggest, the rays travels in a cone like fashion from the point source. The
detector is typically flat.
\todo{find citation for geometry setups}

\begin{figure}[t]
	\centering
	\makebox[\textwidth]{ \makebox[1.3\textwidth]{%
			\begin{subfigure}{0.6\textwidth}
				\def\svgwidth{\textwidth}
				\import{./figures/parallel_beam_setup}{parallel_beam_setup.pdf_tex}
				\caption{Parallel Beam Setup}\label{fig:parallel_beam_geometry}
			\end{subfigure}%
			\begin{subfigure}{0.6\textwidth}
				\def\svgwidth{\textwidth}
				\import{./figures/fan_beam_setup}{fan_beam_setup.pdf_tex}
				\caption{Fan Beam Setup}\label{fig:fan_beam_geometry}
			\end{subfigure}%
		}}%

	\makebox[\textwidth]{ \makebox[1.3\textwidth]{%
			\begin{subfigure}{0.6\textwidth}
				\def\svgwidth{\textwidth}
				\import{./figures/cone_beam_setup}{cone_beam_setup.pdf_tex}
                                \caption{Cone Beam setup}\label{fig:cone_beam_geometry}
			\end{subfigure}%
			\begin{subfigure}{0.6\textwidth}
			\end{subfigure}%
		}}%
	\caption{Geometry setup of CT scanners}\label{fig:ct_geometry_setup}
\end{figure}

\inlinetodo{\autoref{fig:ct_geometry_setup} the last about cone is still ugly}

\section{Phase-Contrast CT}\label{sec:phasecontrast_ct}

In the X-ray attenuation CT setup, X-rays are only considered to exhibit attenuation. However, as
visible light, X-rays are also refracted and scattered. There exist a variety of different means to
measure a signal to reconstruct the phase-contrast \todo{add some references for the different
	signals}. However, here I'll focus on the grating based interferometer as described
in~\cite{pfeiffer_hard-x-ray_2008}. There a differential phase-contrast is acquierd. Thus this
method is often referred to as differential phase-contrast CT\@.

\inlinetodo{Figures showing grating setup}

Disregarding many of the physical details, \(\phi(x, z)\) is acquired using the grating
interferometer. The measurement can be described as
\[ \phi(x, z) = \frac{2\pi d}{p} \frac{\partial}{\partial x}\int \delta(x, y, z)\mathrm{d}y\]
where \(\delta (x, y, z)\) is the refractive index decrement, associated to the real part of the
complex index of refraction \(n = 1 − \delta + i\beta\), \(d\) the distance between the distance
between phase grating and analyzer grating and \(p\) is related to phase-stepping. Phase-stepping is
required to acquire the differential phase-shift signal, but is out of scope for the thesis, see
~\cite{weitkamp_x-ray_2005} for more information.

One key thing should be noted and is of interested here. The phase-shift also depends on the line
integral. Therefore, similar mathematical methods can be used, as with X-ray attenuation CT\@.

\inlinetodo{Show Mathias images of a tooth}

\subsection{X-ray Dark-field Tomography}\label{sec:directional_darkfield}

The same grating based measures one more thing. It is also possible to measure (ultra) small angle
scattering, which enables the reconstruction of a X-ray dark-field signal.

\inlinetodo{Figure of tooth. Some more on this, but not today}

\chapter{Radon Transform and its related transform}\label{chap:radon_transform_and_related}

In the previous chapter, the notion of the inverse problem was introduced. There, based on a given
measurement one tries to reconstruct the original quantity, based on a given model. This chapter is
mostly devoted to derive and analyze this model. Specifically, in the context of X-ray attenuation
CT, with a brief outlook into other areas needed for e.g.\ phase-contrast CT\@.

The physical model is the link between the unknown signal and it's measurements. Usually, the model
is referred to as forward model. Let us first state a couple of basic definitions and then we will
look into the derivation of the forward model for X-ray attenuation CT\@.

\begin{definition}[Image]\label{def:image}
	Let \(f\colon \mathbb{R}^n \to \mathbb{R}\) be a \(n\)-dimensional continuous function,
	whose support is bounded. It is referd to it as a \(n\)-dimensional image. And often it will
	be referred to as image, without the special mention of \(n\)-dimensional.
\end{definition}

\inlinetodo{define cartesion grid for images}

In the special case of \(n=2\), it is called an \textit{2-dimensional image} or just
\textit{2D image}. However, if from the context, the \(2\)-dimensional is obvious, it will still be
referred to as image. In case of \(n=3\), it is called a \textit{volume}.

\begin{definition}[Forward Model]\label{def:forward-model}
	To reconstruct an unknown image \(f\), a set of \(J\) scalar measurements is necessary.
	A single scalar valued measurement \(m_j\), with \(j \in \{1, \dots, J\}\) is defined in therms
	of the pshyical model:
	\[ m_j = \mathscr{M}_j(f)\]
	where
	\[ \mathscr{M}_j\colon (\Omega \to \mathbb{R}) \to \mathbb{R} \]
	and \(\Omega \subseteq \mathbb{R}^n\). The mapping is required to be linear, as this will play an
	important role later on.
\end{definition}

This is a very general definition. This is useful to mathematically model a wide variety of
different applications. In fact, all of the applications in the previous chapter, can be modeled
using this general definition.

Other important transformations, which are not considered in this thesis are the Cone-Beam Transform
(c.f.~\cite[chap 2]{carpio_inverse_2008}), or Abel transform \insertref{Ref to Abel Transform}.

\section{Radon Transform}\label{sec:radon_transform}

Looking at the previously stated Beer Lambert law for attenuation CT \autoref{eq:beer-Lambert-law}.
One can see, that the measured values, are somehow related to a line integral. The mathematical
model, one can use was first defined by Johann Radon in
1917~\cite{radon_uber_1917,radon_determination_1986}, and later rediscovered by Allan M. Cormack
~\cite{cormack_representation_1963}.

\begin{definition}[Radon Transform]
	Let \(\Omega \subset \mathbb{R}^n\) and \(f\colon \Omega \to \mathbb{R}\), which is assumed
	to sufficiently nice. Then the mapping \(\mathscr{R}f\colon (\mathbb{R}^n \to \mathbb{R})
	\to (\mathbb{R} \times \mathscr{S}^{n-1} \to \mathbb{R})\) of \(f\), maps \(f\) into the set
	of its integrals over the affine hyperplanes of \(\mathbb{R}^{n-1}\) is called the
	\textit{Radon Transform} (c.f.~\cite{natterer_mathematics_1986,buzug_computed_2008}).

	Specifically, given the unit direction \(\theta \in \mathscr{S}^{n-1}\), one can define the
	hyperplane \(\mathscr{H}^{n-1}(\theta)\) through the origin and perpendicular to \(\theta\).
	The Radon Transform \(\mathscr{R}\) of \(f\) is defined by the line integral over the
	hyperplane perpendicular to the direction \(\theta\) with signed distance \(s \in
	\mathbb{R}\) to the origin
	\[ \mathscr{R}f(\theta, s) = \int_{\mathbb{H}^{n-1}(\theta)} f(x + s\theta) \, \, \mathrm{d}x \]
\end{definition}

Note that for \(n=2\) the hyperplanes are lines, and hence match the forward model for X-ray
imaging. For the 3-dimensional case, this does not fit anymore. For this case, the so called X-ray
transform \(\mathscr{X}\) was developed~\cite{solmon_x-ray_1976}, which is very similar to the Radon
transform, but for all dimensions only considers integral along lines.

In the \(2\)-dimensional case, \(\theta\) is often described by its polar angles \(\phi\) and an
orthogonal vector \(\theta^\perp\) such that
\( \theta = (\cos \phi, \sin \phi)\) and \(\theta^\perp = (-\sin\theta, \cos\phi)\)
then the Radon Transform is described as \( \mathscr{R}_\phi f(s) =\mathscr{R}f(\theta^\perp, s)\).

However, in the setting of tomographic reconstruction, one wishes to reconstruct the original image
\(f\) from a set of measurements \(g\). Already, \citeauthor*{radon_uber_1917} showed a theoretical
way of the adjoint operations of the Radon Transform. However, these results are very theoretical.

\begin{definition}[Fourier Slice Theorem]\label{def:fourier_slice_theorem}
	Let \(f\colon \mathbb{R}^2 \to \mathbb{R}\) be sufficiently nice and \(\mathscr{F}_n\) the
	\(n\)-dimensional Fourier transform. Then
	\[ (\mathscr{F}_2f)(s) = (\mathscr{F}_1(\mathscr{R}f(\cdot, \phi)))(s) \]
	It is also often referred to as \textit{projection-slice theorem} or \textit{central slice theorem}
\end{definition}

This is a very powerful theorem. In a more natural language, the \(1\)-dimensional Fourier Transform
of the projected data measured at the angle \(\phi\), yields a line in the \(2\)-dimensional
representation of the image \(f\). Even more, the line is the line going through the origin with a
rotation angle \(theta\). Hence, with enough projections, the \(f\) can be described fully in the
Fourier domain and reconstructed using the inverse \(2\)d Fourier Transform.

However, this method has a couple of drawback. \textit{Enough} projections is quite a vague
statement and a strong limitation. Especially considering the trend to reduce X-ray dosage and hence
reducing the number of projections acquired. Further, problems arise as the Fourier domain is
typically sampled in polar coordinates, but for this representation, we'd like to access them using
Cartesian. This requires some for of interpolation. See the dissertation of
\citeauthor{vogel_tomographic_nodate}\cite[chap. 4.1.2]{vogel_tomographic_nodate} for a little more
detailed discussions and very nice illustrative figures. More means to compute the adjoint and
reconstruction \(f\) are presented in \autoref{chap:tomographic_reconstruction}.

Of further interest to us is the first derivative of the Radon transform. As a variety of image
methods build on top of the derivative.

\begin{definition}[Derivative of the Radon Transform]
	The \(n\)th derivative of the Radon Transform is denoted by (compare
	e.g.\ \cite{nilchian_differential_2012,nilchian_fast_2013})
	\[ \mathscr{R}^{(n)} = \frac{\partial^n}{\partial s^n} \mathscr{R}f(\theta, s)\]
\end{definition}

The derivatives are linear operators, which are scale invariant, pseudo-distributive with respect to
convolution and projected translational invariant. The adjoint of the first derivative of the Radon
Transform is shown in~\cite{nilchian_differential_2012}, and for the \(n\)th derivative
see~\cite{nilchian_fast_2013}.

\section{X-ray Transform}\label{sec:xray_transform}

The \(n\)-dimensional Radon Transform computed the integral over \(n-1\)-dimensional hyperplanes.
However, for the setting of attenuation X-ray CT, one is interested in the line integration over
lines in \(n\)-dimensional spaces.

\begin{definition}[X-ray Transform]
	Given \(\theta \in \mathscr{S}^{n-1}\) and \(x \in \mathbb{R}^n\), then
	\[ \mathscr{X}f(\theta, x) = \int_{-\infty}^{+\infty} f(x + t \theta) \, \mathrm{d}t\]
	is the X-ray Transform. It is the integral over the straight line through \(x\) with
	direction \(\theta\) (c.f.~\cite{natterer_mathematics_1986,solmon_x-ray_1976}).
\end{definition}

In the two dimensional case, the Radon Transform and X-ray Transform are equivalent.
Then, the relation between the Radon Transform and the X-ray Transform is
\[\mathscr{X}f(\theta, s\theta^\perp) = \mathscr{R}f(\theta^\perp, s)\]
For the case of attenuation X-ray CT, the X-ray Transform is the physical forward model used.

\chapter{Image Representation}\label{chap:image_representation}

Images as defined in \ref{def:image}, are continuous functions. However, one wishes to use computers
to solve the reconstruction tasks and computers are inherently discrete. Hence, one wishes to
represent an image in a discrete fashion.

\begin{definition}[Permissible representation]
	\label{def:permissible_representation}
	Let \(N \in \mathbb{N}\) be a positive integer and \(\varphi_n\) a set basis function for
	\(1 \leq n \leq N\), then the signal \(f\) can be approximated as a linear combinations
	of these basis functions and the coefficients \(c_n\):
	\[ \hat{f}(x) = \sum_{k=1}^{N} c_k \varphi_k(x) \]
\end{definition}

For our purposes, we assume the function lies on a regular spaced discrete grid. Then, let
\(\varphi\) be a zero centered symmetrical basis function, \(\symbfit{k} \in \mathbb{Z}^n\) be the
\(n\)-dimensional index of a grid cell, and \(x_{\symbfit{k}} \in \mathbb{R}^n\) the center coordinate
of the \(\symbfit{k}\)-th grid cell. Then, the previous equation can be reformulated:
\[ \hat{f}(x) = \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}} \varphi(x - x_{\symbfit{k}}) \]
This definition follows the notation given in~\cite{momey_new_2011}. This method to discretize an
image is called \textit{series expansion} and is described in detail in
e.g.\ \cite{herman_basis_2015}.

Now, if one applies the Radon transformation to the discretized image: \todo{generalize to all linear physical models}
\[ \mathscr{R}\hat{f}(x) = \mathscr{R}\left( \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}} \varphi(x - x_{\symbfit{k}}) \right) \]
Due to the linearity of the Radon Transform this is equivalent to \[ \mathscr{R}\hat{f}(x) =  \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}}\mathscr{R}\left( \varphi(x - x_{\symbfit{k}}) \right) \]
i.e.\ the Radon transformation of the image, only act upon the basis function. Hence, it is
sufficient to study, how the Radon transformation acts upon the individual basis function.

Note that this holds for any linear physical model. Notably, this holds for the X-ray transform and
the first derivative of the Radon transform. But it is also true for the physical models behind the
applications discussed in the previous chapter. Hence, it is sufficient to study, how a basis
function acts under the given transformation.

\section{Voxel Basis}\label{sec:voxel_basis}

The most likely most well known basis function in imaging is the pixel or voxel basis functions. The
voxel basis function is a piecewise linear function. The voxel basis function is most likely the
most widely used basis function. Most literature assumes the voxel basis function implicitly.

The centered voxel basis function of step width \(h\), is given by:
\begin{equation}\label{eq:voxel_basis_fn}
	\varphi^{\text{pixel}}(\symbfit{x}) =
	\begin{cases}
		1, \abs{\symbfit{x}} < \frac{h}{2} \\
		0, \text{otherwise}
	\end{cases}
\end{equation}
Here, the absolute value is coefficient wise, as soon as the absolute value of any coefficient of
the vector \(\symbfit{x} \in \mathbb{R}^n\) is larger half the step size, the function will return
\(0\).

An image approximated by the voxel basis function, in the series expansion method is equivalent to
the nearest neighbourhood interpolation.

As pointed out by \citeauthor*{lewitt_multidimensional_1990}
in~\cite{lewitt_multidimensional_1990,lewitt_alternatives_1992} the voxel-basis function isn't
necessarily a good choice for biomedical images. It is discontinuous at the boundaries.

The analytical formulation of the Radon transform of the pixel basis function
(compare~\cite{toft_radon_1996}) is given by:
\begin{equation}\label{eq:radon_voxel_basis}
	\mathscr{R}\varphi^{\text{pixel}}(\rho, \theta) =
	\begin{cases}
		0                                                  & x_1 > 0                         \\
		\sqrt{4 + (x_1 - x_{-1})^2} = \frac{2}{\cos\theta} & x_1 < 1\;\text{and}\;x_{-1} < 1 \\
		\sqrt{(1 - x_1)^2 + (1 - x_{-1})^2}                & x_1 < 1\;\text{and}\;x_{-1} > 1
	\end{cases}
\end{equation}

As the voxel basis function is discontinuous at the boundaries, there does not exist a way to
compute the derivative of the radon transform relying on the basis function. Steps such as numerical
derivation must be used.

\todo{understand this properly and explain this properly}

\section{Blob Basis}\label{sec:blob_basis}

First introduced by Lewitt in~\cite{lewitt_multidimensional_1990}, spherically symmetric volume
elements (often referred to as blobs) are an alternative to the pixel basis.
~\cite{lewitt_alternatives_1992} describes how blobs can be used in iterative reconstruction
algorithms as a basis instead of pixels.

Blob basis functions have been adopted in many different fields. Among others electron
microscopy~\cite{marabini_3d_1998, garduno_optimization_2001}, poistron emission tomography
(PET)~\cite{jacobs_comparative_1999, chlewicki_noise_2004}, single-photon emission tomography
(SPECT)~\cite{wang_3d_2004, yendiki_comparison_2004}, attenuation X-ray
CT~\cite{jacobs_iterative_1999, carvalho_helical_2003, isola_motion-compensated_2008},
phase-contrast CT~\cite{kohler_iterative_2011, xu_investigation_2012}, reconstruction of coronary
trees~\cite{zhou_blob-based_2008}, breast tomosynthesis~\cite{wu_breast_2010}, reduction of metal
artifacts~\cite{levakhina_two-step_2010} or computed laminography~\cite{trampert_spherically_2017}.

\inlinetodo{Read papers and assert what blobs brings to the table}

Generally, many fields report increased accuracy with a comparable performance. In other fields,
such as phase-contrast CT, blobs enable the usage of iterative reconstructions without an extra step
of numerical differentiations.

The generalized Kaiser-Bessel basis function as proposed by Lewitt, is defined as:
\begin{equation}\label{eq:blob_basis_fn}
	\varphi^{\text{blob}}_{m, \alpha, a}(r) =
	\begin{cases}
		\frac{I_m\left( \alpha \sqrt{1 - \left(\frac{r}{a}\right)^2} \right)} {I_m\left( \alpha \right)} \left( \sqrt{1 - \left(\frac{r}{a}\right)^2}\right)^m & 0 \le r \le a      \\
		0                                                                                                                                                      & \textit{otherwise}
	\end{cases}
\end{equation}
where \(I_m\) is the modified Kaiser-Bessel function of the first kind of order \(m\), \(r\) the
distance to the blob center, \(a\) the blob radius given in units of the grid, and \(\alpha\)
controlling the shape of the blob. \(m\) controls the continuity of the blob function.
\todo{Figures showing different parameters of blob}

The X-ray transform of the blob basis function is given by
(c.f.~\cite{lewitt_multidimensional_1990,lewitt_alternatives_1992})
\begin{align}\label{eq:radon_blob_basis}
	p(s) & = 2 \int_0^{(a^2-s^2)^{1 / 2}} \varphi^{\text{blob}}_{m, \alpha, a}\left(\left(s^2 - t^2\right)^{1/2}\right) \, \mathrm{d} t                                                                        \\
	     & = \frac{a}{I_m(\alpha)} \left( \frac{2\pi}{\alpha}\right)^{1/2} \left( \sqrt{1 - \left(\frac{s}{a}\right)^2} \right)^{m + 1/2} I_{m+1/2}\left( \alpha \sqrt{1 - \left(\frac{s}{a}\right)^2} \right)
\end{align}
\(s\) is the distance from the X-ray to the blob center, and \(\sqrt{a^2 - s^2}\) is one half of the
intersection length btween the blob and the ray. The projected value only depends on the distance
from the X-ray to the blob center. This is a very nice property. This makes implementations quite
efficient.

\inlinetodo{figure for parameters for projected basis, figure for parameters for basis}

From an implementational standpoint, the half integer order of the modified Kaiser-Bessel function
of the first kind, can be quite nasty. Implementations do exist as it can be seen
in~\cite{temme_numerical_1975}. However, the floating point implementations are non-trivial. Plus,
for the case of C++, since C++17 the standard library provides mathematical special functions
~\cite{noauthor_c_nodate, noauthor_stdcyl_bessel_i_nodate}. But sadly, it is not yet entirely
cross platform, as it is only supported by libstdc++~\cite{noauthor_libstdc_nodate-1}, and not
libc++. However, for our cases it is sufficient to assume \(m \in \mathbb{N}\). Then the above
equation can be further simplified.

The recurrence formulation for the modified Kaiser-Bessel function of the first kind is
(c.f.~\cite[chapter 9]{abramowitz_handbook_1972}):
\begin{equation}\label{eq:kaiser_bessel_recurrence}
	I_{m+1}(x) = I_{m-1}(x) - \frac{2 m}{x}I_m(x)
\end{equation}
Further, the Kaiser-Bessel functions have representations with elementary functions. For the
modified Kaiser-Bessel function of the first kind, there are defined as (c.f.~\cite[chapter 10]{abramowitz_handbook_1972}):
\begin{align}\label{eq:kaiser_bessel_half_integer}
	I_{0.5}(x) & = \sqrt{\frac{2}{\pi x}} \sinh(x)                                                                               \\
	I_{1.5}(x) & = \sqrt{\frac{2}{\pi x}} \left( \cosh(x) \frac{\sinh(x)}{x} \right)                                             \\
	I_{2.5}(x) & = \sqrt{\frac{2}{\pi x}} \left(\left(\frac{3}{x^2} + \frac{1}{x}\right)\sinh(x) - \frac{3}{x^2} \cosh(x)\right)
\end{align}
Then \autoref{eq:radon_blob_basis} can be simplified to not include any non-integer evaluations of
the modified Kaiser-Bessel function of the first kind. For example assuming, \(m = 0\), and to keep
everything a little more concise, let \(w = \sqrt{1 - \left(\frac{r}{a}\right)^2}\):
\begin{align}\label{eq:radon_blob_basis_order_0_simplified}
	p(s) & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi}{\alpha}\right)^{1/2} \left( w \right)^{1/2} I_{1/2}\left( \alpha w \right)                     \\
	     & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi w}{\alpha}\right)^{1/2} I_{1/2}\left( \alpha w \right)                                          \\
	     & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi w}{\alpha}\right)^{1/2} \left( \frac{2}{\pi \alpha w}\right)^{1/2} \sinh \left(\alpha w \right) \\
	     & = \frac{2 a}{\alpha I_0(\alpha)} \sinh \left(\alpha w \right)
\end{align}
In the last step, \(\pi\) and \(w\) cancel out, and both the \(2^2\) and \(\alpha^2\) ared moved out
of the square root, leaving it empty. Similar operations can be done for \(m = 1\) and \(m = 2\).

\section{B-Spline Basis}\label{sec:bspline_basis}

Splines are common in image and signal processing~\cite{unser_splines_1999}. Applications include
image interpolation, image transformations, image compressions or the calculation of the first and
second derivative. A common approach is the approximation of the function or image using Splines and
then working efficiently on the continuous representation of the splines. For B-Splines
specifically~\cite{unser_fast_1991} shows the continuous image representation using B-Splines.

This approach was adopted to tomographic reconstruction.~\cite{la_riviere_spline-based_1998}
proposed the calculation of the inverse 2D and 3D Radon transform based on B-Spline.
Similarly,~\cite{horbelt_discretization_2002} develops a B-Spline based filtered backprojection.
Apart from attenuation CT, other medical applications of B-Splines include electron
tomography~\cite{tran_robust_2013, tran_inverse_2014}, positron emission tomography
(PET)~\cite{nichols_spatiotemporal_2002, li_fast_2007, verhaeghe_investigation_2007} and
single-photon emission tomography (SPECT)~\cite{guedon_b-spline_1991, reutter_fully_2007}.

Using B-Splines as a basis function was first presented by~\cite{momey_new_2011,
	momey_b-spline_2012, momey_spline_2015}. And a similar image representation was adapted for
phase-contrast CT in~\cite{nilchian_fast_2013, nilchian_differential_2012, nilchian_spline_2015}.
They differ in the approximation of the evaluiation of the X-ray transform. The former use a
footprint of the B-Splines, where the later rely on the first derivative of the B-Spline basis
function.

\begin{definition}[B-Spline]
	The most basic definition of a B-Spline of degree \(0\) and unit width is the step function:
	\begin{equation}
		\beta^0(x) = \mu(x) =
		\begin{cases}
			1, & \text{if } x \in \mathopen[\minus \frac{1}{2}, \frac{1}{2}\mathclose] \\
			0, & \text{otherwise}
		\end{cases}
	\end{equation}
	Then the univariate B-Spline of degree \(d\) can be constructed by \(d + 1\) convolution of \(\beta^0\)
	(compare~\cite{momey_new_2011}):
	\begin{equation}
		\beta^d(x) = \beta^0 * \beta^{d-1}(x) =
		\underbrace{\beta^0 * \dots * \beta^0(x)}_{d+1 \text{concolution terms}}
	\end{equation}
\end{definition}
Note that B-Splines of degree \(0\) is just the voxel-basis function.

Another way to actually compute the B-Spline basis of order \(d\) is given
in~\cite{unser_fast_1991}:
\begin{equation}
	\beta^d(x) = \sum_{i=0}^{d+1} \frac{(-1)^i}{n!} \binom{d+1}{i}(x - i)^d\mu(x - i)
\end{equation}
where \(\binom{d+1}{i}\) is the bionomial coefficient.

The derivatives are again B-Spline of degree \(d-1\) (compare \cite{unser_splines_1999}):
\begin{equation}
	\frac{\partial \beta^d(x)}{\partial x} = \beta^{d-1}\left(x + \frac{1}{2}\right) -
	\beta^{d-1}\left(x - \frac{1}{2}\right)
\end{equation}
B-Splines are continuously differentiable up to order \(d-1\).

B-Splines are separable. Hence, \(n\)-dimensional B-Splines, often referred to as tensor product
B-Splines, can be constructed the following way:
\begin{equation}
	\beta^d(x) = \prod^n_{i=1} \beta^d(x_i)
\end{equation}
where \(x \in \mathbb{R}^n\)

B-Splines have a couple of really attractive properties. B-Splines are the shortest and smoothest
scaling functions for a given order of approximation~\cite{momey_b-spline_2012}. They are close to a
Gaussian function, with a sufficiently large $d$~\cite{momey_b-spline_2012}, all while preserving
compactness. Hence, they tend to spherically symmetric functions, while preserving local support.
Due to these properties, \citeauthor*{momey_new_2011}\cite{momey_new_2011} argue for B-Splines over
blobs. Further, they note the need to tune the parameters for of blobs for optimal results, which
adds complexity.

Importantly, blobs fail to satisfy the partition of unity~\cite{nilchian_fast_2013}. A basis
functions that satisfies the partition of unity, can approximate any input function arbitarly close.
~\cite{nilchian_fast_2013} show the importance of the property for tomographic reconstruction.

In~\cite{horbelt_discretization_2002}, it was shown that the Radon transform of B-Splines are spline
bikernel.~\cite{entezari_box_2012} shows how expliclity how (tensor product) B-Splines act under
the X-ray and Radon transform.~\cite{nilchian_differential_2012} shows how B-Splines act under
the first derivative of the Radon transform.

The Radon Transform of a \(2\)-dimensional B-Spline was shown by
\citeauthor*{horbelt_discretization_2002}\cite{horbelt_discretization_2002}. Given the projection
angle \(\theta\), then it is:
\begin{equation}
	\mathscr{R}\beta^d(s) = \beta^d_{\sin\theta} * \beta^d_{\cos\theta}(d)
\end{equation}
Hence it is the convolution of two splines of different width (denoted by the subscript). These are
referred to as spline bikernels. \citeauthor*{horbelt_discretization_2002} presents an explicit
formulation to compute it.

To extend this to the \(3\)-dimensional setting, one can look into \textit{box splines}. Box splines
can be seen as a generalization of B-Splines.

\begin{definition}[Box Spline]
	Box splines are the shadow of a hypercube in \(\mathbb{R}^n\), when projected down to a
	lower dimension \(\mathbb{R}^d\). Similarly to B-Splines, box splines can be defined via
	convolution:
	\begin{equation}
		M_\Xi(x) = M_{\xi_1} * \dots * M_{\xi_n}(x)
	\end{equation}
	where \(\Xi \coloneq \mathopen[ \xi_1 \xi_2 \dots \xi_n \mathclose] \in \mathbb{R}^{s \times n}\)
	is the matrix of directions. Each \(\xi\) defines a direction of the hypercube.
	\(\Xi\) completely defines the box spline (compare~\cite{de_boor_box_1993})
\end{definition}

\citeauthor*{entezari_box_2012}~\cite{entezari_box_2012} proofed that the X-ray transform of a
\(d\)-variate box spline is a \(d - 1\) variate box spline. Further, it is the box spline defined
the by projection of the direction matrix \(\Xi\). This means, going back to B-Splines, that for any
dimension, the X-ray transform of B-Splines are box Splines of lower dimension.

\chapter{Tomographic Reconstruction}\label{chap:tomographic_reconstruction}

Till this point, the physical models involved in tomographic reconstruction have been presented and
discritzation has been discussed. The last missing piece is the solution to the inverse problem.
As tomographic reconstruction problems are inverse problems, many methods depend on common solutions
to this space.

Generally, one needs to find solution to the system \(A(f) = m\). There \(A\) is the forward model
(i.e.\ the physical model, for attenuation X-ray CT it's the X-ray Transform), \(f\) is the
\(n\)-dimensional image one seeks to reconstruct, and \(m\) is the measured data, i.e.\ the
projected data.

\section{Analytical Reconstruction}\label{sec:analytical_reconstruction}

As already eluded to in the section about the Radon Transform in \autoref{sec:radon_transform}.
There do exist closed form analytical inversion methods for the Radon Transform. In the
aforementioned section, the Fourier Slice Theorem was introduced. There exist method that use this
approach as a means to reconstruct the desired image. However, they are not often used in practice
in tomographic reconstruction.

\begin{definition}[Back-Projection]\label{def:back_projection}
	Let \(f\colon \Omega \to \mathbb{R}\), where \(\Omega \in \mathbb{R}^2\) sufficiently nice.
	Further, let \(g \coloneq \mathscr{R}f\) be the Radon Transform of \(f\). Then
	\[ (R^\ast g)(x, y) \coloneq \int_0^\pi g(x\cos \phi + y \sin\phi, \phi) \mathrm{d}\phi \]
	is the unfiltered back-projection of \(g\).
\end{definition}

\inlinetodo{Add example images for 1, 2, 4, and so on projection angles}

The result of back-projecting the measured projections is a blurry version of the original function.
\citeauthor{buzug_computed_2008} describes post-processing as a possible solution. However, there
exists another option. If the projections are filtered in the Fourier domain and then the filtered
values are back-projected just as before, a sharper image can be obtained. This is referred to as
the filtered back-projection (FBP)~\cite{ramachandran_three-dimensional_1971}.

\begin{definition}[Filtered Back-Projection]\label{def:filtered_back_projection}
	Still, let \(f\colon \Omega \to \mathbb{R}\), where \(\Omega \in \mathbb{R}^2\) sufficiently
	nice, and \(g \coloneq \mathscr{R}f\) be the Radon Transform of \(f\). Then
	\[ g^\delta(t, \phi) \coloneq (f \ast g(\cdot, \phi))(t) \]
	is the filtered projection. \(\delta(x) \approx \abs{x}\) is a filter, which is convoled
	with the projection data. Using \(\mathscr{R}^\ast\) is the adjoint of the Radon Transform
	as in \autoref{def:back_projection}, then \(\mathscr{R}^\ast g^\delta\) is the
	\textit{filtered back-projection}.
\end{definition}

The quality of the reconstruction depends on the filter and data acquisition. Further the FBP as
presented here, is limited to parallel beam geometry setups. There do exists other methods for fan
beam settings that require rebinning (i.e.\ sorting the projections, such that all are parallel).
However, overall the FBP leads to sharp images and it is wiedly used in medical CT
scanners~\cite{pan_why_2009}.

\inlinetodo{Read Deans (2007) chapter 6, Natterer (1986) chapter 5.1, Herman (2009) chapter 7, Kak (1987) chapter 3.3, Buzug (2008) chapter 5.6)}

\section{Towards the Matrix form}\label{sec:matrix_formulation}

The goal of this chapter is a short run through the steps that are necessary to go form the
continuous definition of the inverse problem as given in \autoref{def:inverse_problem} to a discrete
version, with which we can work on computers.

In the \autoref{def:forward-model}, the definition of forward model is given. This forward model
takes the place of the bounded linear operator \(A\) from \autoref{def:inverse_problem}. However,
this is all still in the continuous space. Using \autoref{def:permissible_representation}, \(f\) can
decomposed into the sum of coefficients and basis functions:
\[ f \approx \hat{f}(x) = \sum_{k=1}^{N} c_k \varphi_k(x) \]
as shown for the Radon Transform, the forward model can be applied and rearranging a little using
the linearity of the operator:
\[ m_j \approx \mathscr{M}_j(\hat{f}) = \sum_{k=1}^{N} c_k \mathscr{M}_j(\varphi_k) \]
\(a_{ji} \coloneq \mathscr{M}_j(\varphi_k)\) is the contribution of a single \(k\)th basis function
to the \(j\)th measurement.

\inlinetodo{Have an image like Vogel Beyond tomographic Figure 4.7}

Now, the measurements can be stacked to a vector \(m = (m_j) \in \mathbb{R}^J\), the same for
the coefficients \(c = (c_i) \in \mathbb{R}^I\), and the contributions \(a_{j} = (a_{ji}) \in
\mathbb{R}^I\). Then a single measurement can be written as a scalar product of the coefficient
vector and the contribution vector. But also importantly, the linear system can be defined:
\begin{equation}\label{eq:system_lin_equation}
	m \approx
	\begin{bmatrix}
		\rule[.5ex]{2em}{0.4pt} & a_1^T & \rule[.5ex]{2em}{0.4pt} \\
		\rule[.5ex]{2em}{0.4pt} & a_2^T & \rule[.5ex]{2em}{0.4pt} \\
		\vdots                                                    \\
		\rule[.5ex]{2em}{0.4pt} & a_J^T & \rule[.5ex]{2em}{0.4pt}
	\end{bmatrix} c \eqcolon A c
\end{equation}

This is a regular system of linear equation, it partitions the problem in the measurements, the
\textit{system matrix} \(A \in \mathbb{R}^{J \times I}\) and the coefficient vector \(c\). Also
note, that the choice of basis function is integrated into the system matrix.

\section{Iterative Reconstruction}\label{sec:iterative_reconstruction}

As explained in \autoref{chap:image_representation}, the problems considered in this thesis are
ill-posed. Hence, care has to be taken when solving the linear system of equations in
\autoref{eq:system_lin_equation}. A common approach is to considered least squares problem instead.

\begin{definition}[Least Squared Problem]\label{def:least_squares_problem}
	The least squares problem is defined as
	\[ \argmin_c \frac{1}{2} \norm{Ac - m}^2_2 \]
	The solution to the least squares problem is given by the normal equation
	\[ A^T A c = A^T m \]
\end{definition}

Note here, that \(Ac\) is considered the forward projection and \(A^T m\) is the backward
projection.

However, the system matrix is usually to large to store in system memory. Therefore, algorithms are
necessary, which do not require the knowledge of the complete system matrix. The software computing
the system matrix on the fly, is often referred to as projectors. A deep dive into the
implementation will be conduced in \autoref{chap:projector}.

\subsection{Landweber Iteration}\label{subsec:landweber_iteration}

A well studied class of iterative algorithms is the \textit{landweber
	iteration}~\cite{landweber_iteration_1951}. It has been discovered in many different ways in
the past. The algorithm was introduced in the tomographic space by
\citeauthor{gilbert_iterative_1972}~\cite{gilbert_iterative_1972} under the name of
Simultaneous Iterative Reconstruction Technique (SIRT).

\begin{definition}[Landweber Iteration]\label{def:landweber_iteration}
	Given a linear system of equations as defined in \autoref{def:inverse_problem}, the
	landweber iteration finds a solution to the corresponding least squares problem. The update
	step for \(k = 0, 1, \dots\) is given by
	\[
		c^{(k+1)} = c^{(k)} + \lambda^{(k)} A^T(m - Ac^{(k)})
	\]
	\(\lambda^{(k)} \in \mathbb{R}\) is a sequence of relaxation parameter that must satisfy
	\(0 < \lambda^{(k)} < 2 \norm{A^T A}_2^{-1}\quad \forall k \in \mathbb{N}\).
\end{definition}

Generally, Landweber iterations (in the basic case) only rely on the forward \(Ac^{(k)}\) and backward
\(A^T m\) projections. The innermost part of the of the update function, is a forward projection of the
current guess. Next, the residual to the measurement is taken and finally the error is back
projected and used as an update to the current guess.

The basic Landweber iteration is a special case of gradient descent. If \(f(c) = \frac{1}{2}
\norm{Ac - m}_2^2\), the update can be written in terms of the gradient
\[
	c^{(k+1)} = c^{(k)} - \lambda^{(k)} \nabla f(c^{(k)})
\]
A generalisation of the Landweber iteration can be given by
\[
	c^{(k+1)} = c^{(k)} + \lambda^{(k)} DA^TM(m - Ac^{(k)})
\]
The expat algorithm and it's convergence behaviour, depend on the exact choice of the matrices \(D\)
and \(M\). The basic Landweber iteration as presented above has \(D = M = I\). If \(D = \frac{1}{J}
\text{diag}(\norm{a_j}^2_2)^{-1}\), with \(\norm{a_j}^2_2\) being the squared \(L_2\) norm of the
\(j\)th row of the system matrix~\cite[chap 6.2]{hansen_discrete_2010}.

\subsection{Algebraic Reconstruction Technique}\label{subsec:algebraic_reconstruction_technique}

The Algebraic Reconstruction Technique (ART) was proposed by
\citeauthor{gordon_algebraic_1970}\cite{gordon_algebraic_1970}. However, outside of tomography the
method is often know as Kaczmarz method~\cite{kaczmarz_approximate_1993}. Though ART is a slight
modification of the original Kaczmarz method.

The basic idea of the algorithm, is to view each row of the system matrix as a hyperplane and update
the solution by iteratively project it onto the hyperplane. If the system matrix is square \(I = J\)
and of full rank, all hyperplanes intersect at one point and ART will converge to it. However, if
the system is overderminted (\(J > I\)) and noisy, which it usually is, the hyperplanes will not
intersect at a single point, but rather at in close proximity to each other.

\begin{definition}[Algebraic Reconstruction Technique]\label{def:art}
	Given the system of linear equations \(Ac = m\), and an initial solution guess \(c^{(0)} \in
	\mathbb{R}^I\) (often the zero vector). Then the solution can be iterativly updated for
	\(k = 0, 1, \dots\)
	\[
		c^{(k+1)} = c^{(k)} + \lambda^{j(k)} \frac{m_{j(k)} - \langle a_{j(k)}, c^{j(k)} \rangle}{\norm{a_{j(k)}}}a_{j(k)}
	\]
	where \(\lambda^{(k)} \in \left(0, 1\right]\) is a sequence of relaxation parameters, and
	\(j(k)\) is a mapping to select the an appropriate row for each iterations. In the simple
	case \(j(k) = (k \mod J) + 1\), but it can also be
	randomized~\cite{strohmer_randomized_2007}.
\end{definition}

The original Kaczmarz method had \(\lambda(k) = 1\, \forall k \in \mathbb{N}\), lowering the
relaxation parameter can improve the reconstruction in noisy settings. Further, the method can be
written as a Landweber-type method~\cite{hansen_discrete_2010}, but it's rather uncommon.

Compared to the Landweber iterations given in the previous section, the Kaczmarz methods accesses
the rows of the system matrix \(A\) sequentially (but maybe not in ascending order). On the other
hand Landweber type methods access all rows of the system matrix simultaneously. Hence, the
`simultaneous' as part of SIRT\@.

\inlinetodo{add figure for this}


\subsection{CG}\label{subsec:conjuage_gradient}

CG and such

\subsection{First-order methods}\label{subsec:first_order_methods}

First orther methods such as Gradient Descent and it's derivatives

\section{Regularization}\label{sec:regularization}

So far all solvers presented here have only looked at the least squares problem. I.e.\ they only
concern themself with the forward model and do not incorporate any further constrains. However,
usually we have some information about the images we wish to reconstruct. For example, one would
expect them to be smooth. One hopes that regularization stabilises the solution. In the sense that
small perturbations by noise in the measurements, still yields solutions close to the exact
solutions.

\begin{definition}[Regularized Problem]\label{def:regularized_problem}
	Let \(R(c)\) be a \textit{penalty function} or \textit{regularizer}, then the least square
	problem can be expanded to
	\[
		\argmin_c \frac{1}{2} \norm{A c - m}_2^2 + \lambda R(c)
	\]
	this is referred to as \textit{regularized problem}. \(lambda\) is a regularization
	parameter. It denotes the weight of of the penalty term.
\end{definition}

This can also be generalized to other problems. Let \(T(c)\) be a data fidelity term (e.g.\
the least squares one, or the negative log-likelihood). Then the regularized problem can be
described as
\[ \argmin_c T(c) + \lambda R(c) \]
Usually, \(R\) is chosen to be non-linear and often is expected to be continuously differentiable.
This equation has three parts. The first \(T(c)\) is the data term. It measures how well a
prediction models the noisy data. However, we do not want to fit the noise in the data. This is the
second, the regularization term. \(\lambda\) controls the importance or the balance of each the
previous terms.

\subsection{Tikonov Regularization}\label{subsec:tikhonov_regularization}

A well studied and often used regularization is named after Andrey Nikolayevich
Tikhonov~\cite{tihonov_solution_1963}. The penalty restricts the solution based on the Euclidean
norm.

\begin{definition}[Tikhonov Regularization]\label{def:tikhonov_regularization}
	The penalty term for Tikhonov regularization is given by
	\[
		R(c)_{\text{Tikhonov}} = \norm{\Gamma c}_2^2
	\]
\end{definition}
A common case is a simple scaling function i.e.\ \(\Gamma = \alpha I\), thus the Tikhonov
regularizer penalises \(c\) with large \(L_2\) norm. Therefore, it is often referred to as
\(L_2\)-regularization. But also the first and second derivative operator is commonly
used~\cite{golub_tikhonov_1999} The hope is, that Tikhonov regularization suppresses high-frequency
noise.

On a fruther note, the Tikhonov regularization can be reformulated to a least squares problem again
\[
	\argmin_c \frac{1}{2}
	\left\lVert
	\begin{pmatrix}
		A \\
		\lambda \Gamma
	\end{pmatrix}
	c -
	\begin{pmatrix}
		m \\
		0
	\end{pmatrix}
	\right\rVert_2^2
\]
Here \(\begin{pmatrix}
	A \\
	\lambda \Gamma
\end{pmatrix}\) is a stacked matrix, with the system matrix on top, and the Tikhonov matrix in the
bottom. A different commonly used notation is
\[
	(A^T A + \lambda \Gamma^T \Gamma)x = A^T m
\]
The problem remains linear and thus can be solved using the previously discussed iterative
reconstruction algorithms. However, in general, for non Tikhonov regularization this does not hold
true. Thus, the problem is rendered non-linear and different optimization techniques have to be
used.

\subsection{\(L_1\)-Regularization}\label{subsec:l1_regularization}

Another common regularization method is based on the \(L_1\) norm, i.e.\ the sum of absolute
values.
\begin{definition}[\(L_1\)-Regularization]\label{def:l1_regularization}
	The penalty term for Tikhonov regularization is given by
	\[
		R(c)_{L_1} = \norm{c}_1
	\]
	See~\cite{tibshirani_regression_1996,tibshirani_lasso_2013,beck_fast_2009}
\end{definition}
Compared to the \(L_2\) regularization, the \(L_1\) regulraization enforces sparsity. I.e.\ the
assumption is that the representation is in some way sparse, and should be enfored. Also it is
more robust to outliers~\cite{beck_fast_2009}. For information on how these problems can be solved
see \citeauthor{beck_fast_2009}~\cite{beck_fast_2009}. As it will be used in the experimental
sections, specifically iterative shrinkage-thresholding algorithms (ISTA) and  Fast Iterative
Shrinkage-Thresholding Algorithm (FISTA).

\begin{definition}[ISTA]\label{def:ista}
	The update step for ISTA is given by
	\[
		c^{(k+1)} = \mathscr{T_\alpha} (c^{(k)} - 2 \lambda A^T (A c^{(k)} - m))
	\]
	where \(t\) is an appropriate stepsize and \(\mathscr{T_\alpha}\) is the shrinkage operator
	defined by
	\[
		\mathscr{T_\alpha}(c)_j = \max(\abs{c_i} - \alpha, 0) \sign(c_j)
	\]
\end{definition}
Similar to Landweber like methods, the residual of the forward projection current prediction and the
measurement vector is backprojected. Next, the projected error is subtracted from the current
estimate and then the shirankage operator is applied.

However, the convergence of ISTA is rather slow (compare~\cite{beck_fast_2009} and its references).
FISTA improves on the convergence of ISTA, but it is out of scope for this thesis. Please refer to
\citeauthor{beck_fast_2009}~\cite{beck_fast_2009} for further reading. \todo{but is it?}

\subsection{TV Regularization}\label{subsec:tv_regularization}

% \begin{listing}
% 	\begin{minted}{cpp}
% int main() {
%     fmt::print("hello, world\n");
%     return 0;
% }
%     \end{minted}
% 	\caption{"Some sampe C code"}
% \end{listing}
% \begin{listing}
% 	\begin{minted}{python}
% import numpy as np
%
% np.linspace(0, 1)
%     \end{minted}
% 	\caption{"Some sampe python code"}
% \end{listing}
