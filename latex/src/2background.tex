\chapter{Notation and Terminology}\label{chap:notation}

This is my notation used thourgh the thesis
\chapter{Imaging Modalities}\label{chap:imaging_modalities}

A large number of different fields boil down to a very similar problem statement. Based on a given
measurement, how can one retrieve the original measured object, assuming the projection process is
known \cite{herman_basis_2015}. This is presicly, the definition of reconstruction used throughout
this thesis.

These fields include but are not limited to variation methods in imaging (e.g. image denoising,
impainting, super resolution and more \insertref{find reference for each}), X-ray attenuation
computed tomography, Phase-Contrast computed tomography, ligth-field tomography, electron
microscopic \insertref{find references}, and many more.

All of these problems are part of the class of so called \textit{inverse problems}. Inverse problems
in turn are again part of the so called \textit{ill-posed problems}. Following the definition of
Hadamard, \insertref{properly cite Hadamard, J. (1902) Sur les Problèmes aux Dérivées Partielles et
	Leur Signification Physique. Princeton University Bulletin, 13, 49-52.} a problem is well posed if
all of the following properties are fulfilled:

\begin{itemize}
	\item \textbf{Existence}: The problem has a solution.
	\item \textbf{Uniqueness}: The solution is the only solution.
	\item \textbf{Stability}: The solution depends continuously on the data.
\end{itemize}

A problem, which fulfills all of the above is referred to as \textit{well-posed}. If any doesn't
hold it's called \textit{ill-posed}. The classes of problems referred to above, are basically all
ill-posed.

\paragraph{Existence}

If a problem does not have any solution, a reformulation can often yields a desired result. Instead
of the original problem statement, the equivalent least squares problem can be solved to find a
unique solution. \todo{find a nice example, see~\cite{hansen_discrete_2010}}

\paragraph{Uniqueness}

If a problem has multiple - or infinitely many solutions - more requirements or preconditions can be
added, to narrow down the number of possible solutions. \todo{find a nice example, see~\cite{hansen_discrete_2010}}

\paragraph{Stability}

This condition is quite critical. As a violation of stability can basically mean, a small
perturbation or change in the data will result in arbitrarily large changes in the result. Again a
reformulation can help out. Usually, one adds some for of regularisation to stabilize the results.
\todo{find a nice example, see~\cite{hansen_discrete_2010}}

The specific field of tomographic reconstruction deals with the challenge to reconstruction an
image, from a finite set of projections. Specifically, most of the applications considered are
projections from X-ray sources. However, we will also look at Light field microscopy.

The common ground for all tomographic reconstruction is the Radon transform~\cite{radon_uber_1917}
(see~\cite{radon_determination_1986} for the english translation), or closely related
transformations such as the X-ray Transform~\cite{solmon_x-ray_1976}. The Radon Transform will be
properly introduced in \autoref{chap:radon_transform_and_related}.

The remainder of this chapter will introduce a variety of different imaging modalities and their
applications. This will serve as a motivation for developments in the different fields. This should
also help to gain an intuition for the mathematical introduction and definitions in the following
chapters.

Physical details will not be discussed, as they are beyond the scope of this thesis. Rather, they
will be striped down and simplified to what is necessary and useful for the scope of this thesis.
However, as much as possible, resources for the interested reader are cited in the corresponding
sections.


\section{X-ray Attenuation CT}\label{sec:xray_attenuation_ct}

The discovery of X-rays by Wilhelm Conrad Röntgen \insertref{Röntgen Wilhelm Conrad Röntgen} and its
usages have already.
\todo{Cite (medical) applications for each method, write a little more}

The breakthrough for X-ray attenuation CT came with the a series of publications of by
\citeauthor{hounsfield_computerized_1973}\cite{hounsfield_computerized_1973},
\citeauthor{ambrose_computerized_1973}\cite{ambrose_computerized_1973} and
~\citeauthor{perry_computerized_1973}\cite{perry_computerized_1973}. The mathematical foundation was
laid but many, but importantly as already mentioned~\citeauthor{radon_uber_1917}, but also
\citeauthor{cormack_representation_1963}\cite{cormack_representation_1963}.

Narrowing our observations to a single X-ray going through a 2 dimensional object, which we wish to
examinate from the inside. For now, consider the X-ray going in a straight line \(L\). The
attenuation of the X-ray along this path is based on the Beer-Lambert law~\cite{buzug_computed_2008}:

\begin{equation}
	\label{eq:beer-Lambert-law}
	- \ln \frac{I}{I_0} = \int_L \mu (x) \, \mathrm{d}x
\end{equation}

where the integral along the path \(L\) is referred to as the line integral.
\(\mu \colon \mathbb{R}^2 \to \mathbb{R}\) denotes the linear attenuation cofficients, \(I_0\)
the initial intensity of the beam (i.e.\ without any object between the source and the detector).

The challenge for X-ray attenuation CT is to reconstruct the function \(\mu\).

\section{Phase-Contrast CT}\label{sec:phasecontrast_ct}

Though, attenuation is the predominant X-ray based imaging method, other methods take other physical
properties of X-rays into account, such as diffraction, refraction and scattering.

As the name suggest, Phase-contrast X-ray CT considers the phase shift of the X-rays going through
an object.

Read and incorporate:
\begin{itemize}
	\item Differential phase-contrast X-ray computed tomography: From model discretization to image
	      reconstruction, Nilchan
	\item Tomographic reconstruction of three-dimensional objects from hard X-ray differential phase
	      contrast projection images
\end{itemize}


\section{Anisotropic X-ray Dark-field Tomography}\label{sec:axdt}

Once one takes also

\section{Light Field Microscopy}\label{sec:lightfield_microscopy}

Lastly, a field not based on X-rays.


\chapter{Radon Transform and its related transform}\label{chap:radon_transform_and_related}

In the previous chapter, the notion of the inverse problem was introduced. There, based on a given
measurement one tries to reconstruct the original quantity, based on a given model. This chapter is
mostly devoted to derive and analyze this model. Specifically, in the context of X-ray attenuation
CT, with a brief outlook into other areas needed for e.g.\ phase-contrast CT\@.

The physical model is the link between the unknown signal and it's measurements. Usually, the model
is referred to as forward model. Let us first state a couple of basic definitions and then we will
look into the derivation of the forward model for X-ray attenuation CT\@.

\begin{definition}[Image]\label{def:image}
	Let \(f\colon \mathbb{R}^n \to \mathbb{R}\) be a \(n\)-dimensional continuous function,
	whose support is bounded. It is referd to it as a \(n\)-dimensional image. And often it will
	be referred to as image, without the special mention of \(n\)-dimensional.
\end{definition}

\inlinetodo{define cartesion grid for images}

In the special case of \(n=2\), it is called an \textit{2-dimensional image} or just
\textit{2D image}. However, if from the context, the \(2\)-dimensional is obvious, it will still be
referred to as image. In case of \(n=3\), it is called a \textit{volume}.

\begin{definition}[Forward Model]\label{def:forward-model}
	To reconstruct an unknown image \(f\), a set of \(J\) scalar measurements is necessary.
	A single scalar valued measurement \(m_j\), with \(j \in \{1, \dots, J\}\) is defined in therms
	of the pshyical model:
	\[ m_j = \mathscr{M}_j(f)\]
	where
	\[ \mathscr{M}_j\colon (\Omega \to \mathbb{R}) \to \mathbb{R} \]
	and \(\Omega \subseteq \mathbb{R}^n\). The mapping is required to be linear, as this will play an
	important role later on.
\end{definition}

This is a very general definition. This is useful to mathematically model a wide variety of
different applications. In fact, all of the applications in the previous chapter, can be modeled
using this general definition.

\section{Radon Transform}\label{sec:radon_transform}

Looking at the previously stated Beer Lambert law for attenuation CT \autoref{eq:beer-Lambert-law}.
One can see, that the measured values, are somehow related to a line integral. The mathematical
model, one can use was first defined by Johann Radon in
1917~\cite{radon_uber_1917,radon_determination_1986}, and later rediscovered by Allan M. Cormack
~\cite{cormack_representation_1963}.

\begin{definition}[Radon Transform]
	Let \(\Omega \subset \mathbb{R}^n\) and \(f\colon \Omega \to \mathbb{R}\), which is assumed
	to sufficiently nice. Then the mapping \(\mathscr{R}f\colon (\mathbb{R}^n \to \mathbb{R})
	\to (\mathbb{R} \times \mathscr{S}^{n-1} \to \mathbb{R})\) of \(f\), maps \(f\) into the set
	of its integrals over the affine hyperplanes of \(\mathbb{R}^{n-1}\) is called the
	\textit{Radon Transform} (c.f.~\cite{natterer_mathematics_1986,buzug_computed_2008}).

	Specifically, given the unit direction \(\theta \in \mathscr{S}^{n-1}\), one can define the
	hyperplane \(\mathscr{H}^{n-1}(\theta)\) through the origin and perpendicular to \(\theta\).
	The Radon Transform \(\mathscr{R}\) of \(f\) is defined by the line integral over the
	hyperplane perpendicular to the direction \(\theta\) with signed distance \(s \in
	\mathbb{R}\) to the origin
	\[ \mathscr{R}f(\theta, s) = \int_{\mathbb{H}^{n-1}(\theta)} f(x + s\theta) \, \, \mathrm{d}x \]
\end{definition}

Note that for \(n=2\) the hyperplanes are lines, and hence match the forward model for X-ray
imaging. For the 3-dimensional case, this does not fit anymore. For this case, the so called X-ray
transform \(\mathscr{X}\) was developed~\cite{solmon_x-ray_1976}, which is very similar to the Radon
transform, but for all dimensions only considers integral along lines.

In the \(2\)-dimensional case, \(\theta\) is often described by its polar angles \(\phi\) and an
orthogonal vector \(\theta^\perp\) such that
\( \theta = (\cos \phi, \sin \phi)\) and \(\theta^\perp = (-\sin\theta, \cos\phi)\)
then the Radon Transform is described as \( \mathscr{R}_\phi f(s) =\mathscr{R}f(\theta^\perp, s)\).

However, in the setting of tomographic reconstruction, one wishes to reconstruct the original image
\(f\) from a set of measurements \(g\). Already, \citeauthor*{radon_uber_1917} showed a theoretical
way of the adjoint operations of the Radon Transform. However, these results are very theoretical.

\begin{definition}[Fourier Slice Theorem]\label{def:fourier_slice_theorem}
	Let \(f\colon \mathbb{R}^2 \to \mathbb{R}\) be sufficiently nice and \(\mathscr{F}_n\) the
	\(n\)-dimensional Fourier transform. Then
	\[ (\mathscr{F}_2f)(s) = (\mathscr{F}_1(\mathscr{R}f(\cdot, \phi)))(s) \]
	It is also often referred to as \textit{projection-slice theorem} or \textit{central slice theorem}
\end{definition}

This is a very powerful theorem. In a more natural language, the \(1\)-dimensional Fourier Transform
of the projected data measured at the angle \(\phi\), yields a line in the \(2\)-dimensional
representation of the image \(f\). Even more, the line is the line going through the origin with a
rotation angle \(theta\). Hence, with enough projections, the \(f\) can be described fully in the
Fourier domain and reconstructed using the inverse \(2\)d Fourier Transform.

However, this method has a couple of drawback. \textit{Enough} projections is quite a vague
statement and a strong limitation. Especially considering the trend to reduce X-ray dosage and hence
reducing the number of projections acquired. Further, problems arise as the Fourier domain is
typically sampled in polar coordinates, but for this representation, we'd like to access them using
Cartesian. This requires some for of interpolation. See the dissertation of
\citeauthor{vogel_tomographic_nodate}\cite[chap. 4.1.2]{vogel_tomographic_nodate} for a little more
detailed discussions and very nice illustrative figures. More means to compute the adjoint and
reconstruction \(f\) are presented in \autoref{chap:tomographic_reconstruction}.

Of further interest to us is the first derivative of the Radon transform. As a variety of image
methods build on top of the derivative.

\begin{definition}[Derivative of the Radon Transform]
	The \(n\)th derivative of the Radon Transform is denoted by (compare
	e.g.~\cite{nilchian_differential_2012,nilchian_fast_2013})
	\[ \mathscr{R}^{(n)} = \frac{\partial^n}{\partial s^n} \mathscr{R}f(\theta, s)\]
\end{definition}

The derivatives are linear operators, which are scale invariant, pseudo-distributive with respect to
convolution and projected translational invariant. The adjoint of the first derivative of the Radon
Transform is shown in~\cite{nilchian_differential_2012}, and for the \(n\)th derivative
see~\cite{nilchian_fast_2013}.

\section{X-ray Transform}\label{sec:xray_transform}

The \(n\)-dimensional Radon Transform computed the integral over \(n-1\)-dimensional hyperplanes.
However, for the setting of attenuation X-ray CT, one is interested in the line integration over
lines in \(n\)-dimensional spaces.

\begin{definition}[X-ray Transform]
	Given \(\theta \in \mathscr{S}^{n-1}\) and \(x \in \mathbb{R}^n\), then
	\[ \mathscr{X}f(\theta, x) = \int_{-\infty}^{+\infty} f(x + t \theta) \, \mathrm{d}t\]
	is the X-ray Transform. It is the integral over the straight line through \(x\) with
	direction \(\theta\) (c.f.~\cite{natterer_mathematics_1986,solmon_x-ray_1976}).
\end{definition}

In the two dimensional case, the Radon Transform and X-ray Transform are equivalent.
Then, the relation between the Radon Transform and the X-ray Transform is
\[\mathscr{X}f(\theta, s\theta^\perp) = \mathscr{R}f(\theta^\perp, s)\]
For the case of attenuation X-ray CT, the X-ray Transform is the physical forward model used.

\section{Abel Transform}\label{sec:abel_transform}

\chapter{Image Representation}\label{chap:image_representation}

Images as defined in \ref{def:image}, are continuous functions. However, one wishes to use computers
to solve the reconstruction tasks and computers are inherently discrete. Hence, one wishes to
represent an image in a discrete fashion.

\begin{definition}[Permissible representation]
	\label{def:permissible_representation}
	Let \(N \in \mathbb{N}\) be a positive integer and \(\varphi_n\) a set basis function for
	\(1 \leq n \leq N\), then the signal \(f\) can be approximated as a linear combinations
	of these basis functions and the coefficients \(c_n\):
	\[ \hat{f}(x) = \sum_{k=1}^{N} c_k \varphi_k(x) \]
\end{definition}

For our purposes, we assume the function lies on a regular spaced discrete grid. Then, let
\(\varphi\) be a zero centered symmetrical basis function, \(\symbfit{k} \in \mathbb{Z}^n\) be the
\(n\)-dimensional index of a grid cell, and \(x_{\symbfit{k}} \in \mathbb{R}^n\) the center coordinate
of the \(\symbfit{k}\)-th grid cell. Then, the previous equation can be reformulated:
\[ \hat{f}(x) = \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}} \varphi(x - x_{\symbfit{k}}) \]
This definition follows the notation given in \cite{momey_new_2011}. This method to discretize an
image is called \textit{series expansion} and is described in detail in
e.g.~\cite{herman_basis_2015}.

Now, if one applies the Radon transformation to the discretized image: \todo{generalize to all linear physical models}
\[ \mathscr{R}\hat{f}(x) = \mathscr{R}\left( \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}} \varphi(x - x_{\symbfit{k}}) \right) \]
Due to the linearity of the Radon Transform this is equivalent to \[ \mathscr{R}\hat{f}(x) =  \sum_{\symbfit{k} \in \mathbb{Z}^n} c_{\symbfit{k}}\mathscr{R}\left( \varphi(x - x_{\symbfit{k}}) \right) \]
i.e. the Radon transformation of the image, only act upon the basis function. Hence, it is
sufficient to study, how the Radon transformation acts upon the individual basis function.

Note that this holds for any linear physical model. Notably, this holds for the X-ray transform and
the first derivative of the Radon transform. But it is also true for the physical models behind the
applications discussed in the previous chapter. Hence, it is sufficient to study, how a basis
function acts under the given transformation.

\section{Voxel Basis}\label{sec:voxel_basis}

The most likely most well known basis function in imaging is the pixel or voxel basis functions. The
voxel basis function is a piecewise linear function. The voxel basis function is most likely the
most widely used basis function. Most literature assumes the voxel basis function implicitly.

The centered voxel basis function of step width \(h\), is given by:
\begin{equation}\label{eq:voxel_basis_fn}
	\varphi^{\text{pixel}}(\symbfit{x}) =
	\begin{cases}
		1, \abs{\symbfit{x}} < \frac{h}{2} \\
		0, \text{otherwise}
	\end{cases}
\end{equation}
Here, the absolute value is coefficient wise, as soon as the absolute value of any coefficient of
the vector \(\symbfit{x} \in \mathbb{R}^n\) is larger half the step size, the function will return
\(0\).

An image approximated by the voxel basis function, in the series expansion method is equivalent to
the nearest neighbourhood interpolation.

As pointed out by \citeauthor*{lewitt_multidimensional_1990}
in~\cite{lewitt_multidimensional_1990,lewitt_alternatives_1992} the voxel-basis function isn't
necessarily a good choice for biomedical images. It is discontinuous at the boundaries.

The analytical formulation of the Radon transform of the pixel basis function
(compare~\cite{toft_radon_1996}) is given by:
\begin{equation}\label{eq:radon_voxel_basis}
	\mathscr{R}\varphi^{\text{pixel}}(\rho, \theta) =
	\begin{cases}
		0                                                  & x_1 > 0                         \\
		\sqrt{4 + (x_1 - x_{-1})^2} = \frac{2}{\cos\theta} & x_1 < 1\;\text{and}\;x_{-1} < 1 \\
		\sqrt{(1 - x_1)^2 + (1 - x_{-1})^2}                & x_1 < 1\;\text{and}\;x_{-1} > 1
	\end{cases}
\end{equation}

As the voxel basis function is discontinuous at the boundaries, there does not exist a way to
compute the derivative of the radon transform relying on the basis function. Steps such as numerical
derivation must be used.

\todo{understand this properly and explain this properly}

\section{Blob Basis}\label{sec:blob_basis}

First introduced by Lewitt in~\cite{lewitt_multidimensional_1990}, spherically symmetric volume
elements (often referred to as blobs) are an alternative to the pixel basis.
~\cite{lewitt_alternatives_1992} describes how blobs can be used in iterative reconstruction
algorithms as a basis instead of pixels.

Blob basis functions have been adopted in many different fields. Among others electron
microscopy~\cite{marabini_3d_1998, garduno_optimization_2001}, poistron emission tomography
(PET)~\cite{jacobs_comparative_1999, chlewicki_noise_2004}, single-photon emission tomography
(SPECT)~\cite{wang_3d_2004, yendiki_comparison_2004}, attenuation X-ray
CT~\cite{jacobs_iterative_1999, carvalho_helical_2003, isola_motion-compensated_2008},
phase-contrast CT~\cite{kohler_iterative_2011, xu_investigation_2012}, reconstruction of coronary
trees~\cite{zhou_blob-based_2008}, breast tomosynthesis~\cite{wu_breast_2010}, reduction of metal
artifacts~\cite{levakhina_two-step_2010} or computed laminography~\cite{trampert_spherically_2017}.

\inlinetodo{Read papers and assert what blobs brings to the table}

Generally, many fields report increased accuracy with a comparable performance. In other fields,
such as phase-contrast CT, blobs enable the usage of iterative reconstructions without an extra step
of numerical differentiations.

The generalized Kaiser-Bessel basis function as proposed by Lewitt, is defined as:
\begin{equation}\label{eq:blob_basis_fn}
	\varphi^{\text{blob}}_{m, \alpha, a}(r) =
	\begin{cases}
		\frac{I_m\left( \alpha \sqrt{1 - \left(\frac{r}{a}\right)^2} \right)} {I_m\left( \alpha \right)} \left( \sqrt{1 - \left(\frac{r}{a}\right)^2}\right)^m & 0 \le r \le a      \\
		0                                                                                                                                                      & \textit{otherwise}
	\end{cases}
\end{equation}
where \(I_m\) is the modified Kaiser-Bessel function of the first kind of order \(m\), \(r\) the
distance to the blob center, \(a\) the blob radius given in units of the grid, and \(\alpha\)
controlling the shape of the blob. \(m\) controls the continuity of the blob function.
\todo{Figures showing different parameters of blob}

The X-ray transform of the blob basis function is given by
(c.f.~\cite{lewitt_multidimensional_1990,lewitt_alternatives_1992})
\begin{align}\label{eq:radon_blob_basis}
	p(s) & = 2 \int_0^{(a^2-s^2)^{1 / 2}} \varphi^{\text{blob}}_{m, \alpha, a}\left(\left(s^2 - t^2\right)^{1/2}\right) \, \mathrm{d} t                                                                        \\
	     & = \frac{a}{I_m(\alpha)} \left( \frac{2\pi}{\alpha}\right)^{1/2} \left( \sqrt{1 - \left(\frac{s}{a}\right)^2} \right)^{m + 1/2} I_{m+1/2}\left( \alpha \sqrt{1 - \left(\frac{s}{a}\right)^2} \right)
\end{align}
\(s\) is the distance from the X-ray to the blob center, and \(\sqrt{a^2 - s^2}\) is one half of the
intersection length btween the blob and the ray. The projected value only depends on the distance
from the X-ray to the blob center. This is a very nice property. This makes implementations quite
efficient.

\inlinetodo{figure for parameters for projected basis, figure for parameters for basis}

From an implementational standpoint, the half integer order of the modified Kaiser-Bessel function
of the first kind, can be quite nasty. Implementations do exist as it can be seen
in~\cite{temme_numerical_1975}. However, the floating point implementations are non-trivial. Plus,
for the case of C++, since C++17 the standard library provides mathematical special functions
~\cite{noauthor_c_nodate, noauthor_stdcyl_bessel_i_nodate}. But sadly, it is not yet entirely
cross platform, as it is only supported by libstdc++~\cite{noauthor_libstdc_nodate-1}, and not
libc++. However, for our cases it is sufficient to assume \(m \in \mathbb{N}\). Then the above
equation can be further simplified.

The recurrence formulation for the modified Kaiser-Bessel function of the first kind is
(c.f.~\cite[chapter 9]{abramowitz_handbook_1972}):
\begin{equation}\label{eq:kaiser_bessel_recurrence}
	I_{m+1}(x) = I_{m-1}(x) - \frac{2 m}{x}I_m(x)
\end{equation}
Further, the Kaiser-Bessel functions have representations with elementary functions. For the
modified Kaiser-Bessel function of the first kind, there are defined as (c.f.~\cite[chapter 10]{abramowitz_handbook_1972}):
\begin{align}\label{eq:kaiser_bessel_half_integer}
	I_{0.5}(x) & = \sqrt{\frac{2}{\pi x}} \sinh(x)                                                                               \\
	I_{1.5}(x) & = \sqrt{\frac{2}{\pi x}} \left( \cosh(x) \frac{\sinh(x)}{x} \right)                                             \\
	I_{2.5}(x) & = \sqrt{\frac{2}{\pi x}} \left(\left(\frac{3}{x^2} + \frac{1}{x}\right)\sinh(x) - \frac{3}{x^2} \cosh(x)\right)
\end{align}
Then \autoref{eq:radon_blob_basis} can be simplified to not include any non-integer evaluations of
the modified Kaiser-Bessel function of the first kind. For example assuming, \(m = 0\), and to keep
everything a little more concise, let \(w = \sqrt{1 - \left(\frac{r}{a}\right)^2}\):
\begin{align}\label{eq:radon_blob_basis_order_0_simplified}
	p(s) & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi}{\alpha}\right)^{1/2} \left( w \right)^{1/2} I_{1/2}\left( \alpha w \right)                     \\
	     & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi w}{\alpha}\right)^{1/2} I_{1/2}\left( \alpha w \right)                                          \\
	     & = \frac{a}{I_0(\alpha)} \left(\frac{2\pi w}{\alpha}\right)^{1/2} \left( \frac{2}{\pi \alpha w}\right)^{1/2} \sinh \left(\alpha w \right) \\
	     & = \frac{2 a}{\alpha I_0(\alpha)} \sinh \left(\alpha w \right)
\end{align}
In the last step, \(\pi\) and \(w\) cancel out, and both the \(2^2\) and \(\alpha^2\) ared moved out
of the square root, leaving it empty. Similar operations can be done for \(m = 1\) and \(m = 2\).

\section{B-Spline Basis}\label{sec:bspline_basis}

Splines are common in image and signal processing~\cite{unser_splines_1999}. Applications include
image interpolation, image transformations, image compressions or the calculation of the first and
second derivative. A common approach is the approximation of the function or image using Splines and
then working efficiently on the continuous representation of the splines. For B-Splines
specifically~\cite{unser_fast_1991} shows the continuous image representation using B-Splines.

This approach was adopted to tomographic reconstruction.~\cite{la_riviere_spline-based_1998}
proposed the calculation of the inverse 2D and 3D Radon transform based on B-Spline.
Similarly,~\cite{horbelt_discretization_2002} develops a B-Spline based filtered backprojection.
Apart from attenuation CT, other medical applications of B-Splines include electron
tomography~\cite{tran_robust_2013, tran_inverse_2014}, positron emission tomography
(PET)~\cite{nichols_spatiotemporal_2002, li_fast_2007, verhaeghe_investigation_2007} and
single-photon emission tomography (SPECT)~\cite{guedon_b-spline_1991, reutter_fully_2007}.

Using B-Splines as a basis function was first presented by~\cite{momey_new_2011,
	momey_b-spline_2012, momey_spline_2015}. And a similar image representation was adapted for
phase-contrast CT in~\cite{nilchian_fast_2013, nilchian_differential_2012, nilchian_spline_2015}.
They differ in the approximation of the evaluiation of the X-ray transform. The former use a
footprint of the B-Splines, where the later rely on the first derivative of the B-Spline basis
function.

\begin{definition}[B-Spline]
	The most basic definition of a B-Spline of degree \(0\) and unit width is the step function:
	\begin{equation}
		\beta^0(x) = \mu(x) =
		\begin{cases}
			1, & \text{if } x \in \mathopen[\minus \frac{1}{2}, \frac{1}{2}\mathclose] \\
			0, & \text{otherwise}
		\end{cases}
	\end{equation}
	Then the univariate B-Spline of degree \(d\) can be constructed by \(d + 1\) convolution of \(\beta^0\)
	(compare~\cite{momey_new_2011}):
	\begin{equation}
		\beta^d(x) = \beta^0 * \beta^{d-1}(x) =
		\underbrace{\beta^0 * \dots * \beta^0(x)}_{d+1 \text{concolution terms}}
	\end{equation}
\end{definition}
Note that B-Splines of degree \(0\) is just the voxel-basis function.

Another way to actually compute the B-Spline basis of order \(d\) is given
in~\cite{unser_fast_1991}:
\begin{equation}
	\beta^d(x) = \sum_{i=0}^{d+1} \frac{(-1)^i}{n!} \binom{d+1}{i}(x - i)^d\mu(x - i)
\end{equation}
where \(\binom{d+1}{i}\) is the bionomial coefficient.

The derivatives are again B-Spline of degree \(d-1\) (compare \cite{unser_splines_1999}):
\begin{equation}
	\frac{\partial \beta^d(x)}{\partial x} = \beta^{d-1}\left(x + \frac{1}{2}\right) -
	\beta^{d-1}\left(x - \frac{1}{2}\right)
\end{equation}
B-Splines are continuously differentiable up to order \(d-1\).

B-Splines are separable. Hence, \(n\)-dimensional B-Splines, often referred to as tensor product
B-Splines, can be constructed the following way:
\begin{equation}
	\beta^d(x) = \prod^n_{i=1} \beta^d(x_i)
\end{equation}
where \(x \in \mathbb{R}^n\)

B-Splines have a couple of really attractive properties. B-Splines are the shortest and smoothest
scaling functions for a given order of approximation~\cite{momey_b-spline_2012}. They are close to a
Gaussian function, with a sufficiently large $d$~\cite{momey_b-spline_2012}, all while preserving
compactness. Hence, they tend to spherically symmetric functions, while preserving local support.
Due to these properties, \citeauthor*{momey_new_2011}\cite{momey_new_2011} argue for B-Splines over
blobs. Further, they note the need to tune the parameters for of blobs for optimal results, which
adds complexity.

Importantly, blobs fail to satisfy the partition of unity~\cite{nilchian_fast_2013}. A basis
functions that satisfies the partition of unity, can approximate any input function arbitarly close.
~\cite{nilchian_fast_2013} show the importance of the property for tomographic reconstruction.

In~\cite{horbelt_discretization_2002}, it was shown that the Radon transform of B-Splines are spline
bikernel.~\cite{entezari_box_2012} shows how expliclity how (tensor product) B-Splines act under
the X-ray and Radon transform.~\cite{nilchian_differential_2012} shows how B-Splines act under
the first derivative of the Radon transform.

The Radon Transform of a \(2\)-dimensional B-Spline was shown by
\citeauthor*{horbelt_discretization_2002}\cite{horbelt_discretization_2002}. Given the projection
angle \(\theta\), then it is:
\begin{equation}
	\mathscr{R}\beta^d(s) = \beta^d_{\sin\theta} * \beta^d_{\cos\theta}(d)
\end{equation}
Hence it is the convolution of two splines of different width (denoted by the subscript). These are
referred to as spline bikernels. \citeauthor*{horbelt_discretization_2002} presents an explicit
formulation to compute it.

To extend this to the \(3\)-dimensional setting, one can look into \textit{box splines}. Box splines
can be seen as a generalization of B-Splines.

\begin{definition}[Box Spline]
	Box splines are the shadow of a hypercube in \(\mathbb{R}^n\), when projected down to a
	lower dimension \(\mathbb{R}^d\). Similarly to B-Splines, box splines can be defined via
	convolution:
	\begin{equation}
		M_\Xi(x) = M_{\xi_1} * \dots * M_{\xi_n}(x)
	\end{equation}
	where \(\Xi \coloneq \mathopen[ \xi_1 \xi_2 \dots \xi_n \mathclose] \in \mathbb{R}^{s \times n}\)
	is the matrix of directions. Each \(\xi\) defines a direction of the hypercube.
	\(\Xi\) completely defines the box spline (compare~\cite{de_boor_box_1993})
\end{definition}

\citeauthor*{entezari_box_2012}~\cite{entezari_box_2012} proofed that the X-ray transform of a
\(d\)-variate box spline is a \(d - 1\) variate box spline. Further, it is the box spline defined
the by projection of the direction matrix \(\Xi\). This means, going back to B-Splines, that for any
dimension, the X-ray transform of B-Splines are box Splines of lower dimension.

\chapter{Tomographic Reconstruction}\label{chap:tomographic_reconstruction}

Till this point, the physical models involved in tomographic reconstruction have been presented and
discritzation has been discussed. The last missing piece is the solution to the inverse problem.
As tomographic reconstruction problems are inverse problems, many methods depend on common solutions
to this space.

Generally, one needs to find solution to the system \(A(f) = m\). There \(A\) is the forward model
(i.e.\ the physical model, for attenuation X-ray CT it's the X-ray Transform), \(f\) is the
\(n\)-dimensional image one seeks to reconstruct, and \(m\) is the measured data, i.e.\ the
projected data.

\section{Analytical Reconstruction}\label{sec:analytical_reconstruction}

As already eluded to in the section about the Radon Transform in \autoref{sec:radon_transform}.
There do exist closed form analytical inversion methods for the Radon Transform. In the
aforementioned section, the Fourier Slice Theorem was introduced. There exist method that use this
approach as a means to reconstruct the desired image. However, they are not often used in practice
in tomographic reconstruction.

\begin{definition}[Back-Projection]\label{def:back_projection}
	Let \(f\colon \Omega \to \mathbb{R}\), where \(\Omega \in \mathbb{R}^2\) sufficiently nice.
	Further, let \(g \coloneq \mathscr{R}f\) be the Radon Transform of \(f\). Then
	\[ (R^\ast g)(x, y) \coloneq \int_0^\pi g(x\cos \phi + y \sin\phi, \phi) \mathrm{d}\phi \]
	is the unfiltered back-projection of \(g\).
\end{definition}

\inlinetodo{Add example images for 1, 2, 4, and so on projection angles}

The result of back-projecting the measured projections is a blurry version of the original function.
\citeauthor{buzug_computed_2008} describes post-processing as a possible solution. However, there
exists another option. If the projections are filtered in the Fourier domain and then the filtered
values are back-projected just as before, a sharper image can be obtained. This is referred to as
the filtered back-projection (FBP)~\cite{ramachandran_three-dimensional_1971}.

\begin{definition}[Filtered Back-Projection]\label{def:filtered_back_projection}
	Still, let \(f\colon \Omega \to \mathbb{R}\), where \(\Omega \in \mathbb{R}^2\) sufficiently
	nice, and \(g \coloneq \mathscr{R}f\) be the Radon Transform of \(f\). Then
	\[ g^\delta(t, \phi) \coloneq (f \ast g(\cdot, \phi))(t) \]
	is the filtered projection. \(\delta(x) \approx \abs{x}\) is a filter, which is convoled
	with the projection data. Using \(\mathscr{R}^\ast\) is the adjoint of the Radon Transform
	as in \autoref{def:back_projection}, then \(\mathscr{R}^\ast g^\delta\) is the
	\textit{filtered back-projection}.
\end{definition}

The quality of the reconstruction depends on the filter and data acquisition. Further the FBP as
presented here, is limited to parallel beam geometry setups. There do exists other methods for fan
beam settings that require rebinning (i.e.\ sorting the projections, such that all are parallel).
However, overall the FBP leads to sharp images and it is wiedly used in medical CT
scanners\cite{pan_why_2009}.

\inlinetodo{Read Deans (2007) chapter 6, Natterer (1986) chapter 5.1, Herman (2009) chapter 7, Kak (1987) chapter 3.3, Buzug (2008) chapter 5.6)}

\section{Towards the Matrix form}\label{sec:matrix_formulation}

Before we concentrate on iterative reconstruction techniques, a deeper dive into the linear system
of equation is needed. Recall the \autoref{def:forward-model}, the definition of the forward model.
There, \(\mathscr{M}_j\) describes the acquisition of a single measurement, i.e.
\[ m_j = \mathscr{M}_j(f) \]
This is done for the complete set of projections \(J\). Then further recall
\autoref{def:permissible_representation}
\[ f \approx \hat{f}(x) = \sum_{k=1}^{N} c_k \varphi_k(x) \]
as shown for the Radon Transform, the forward model can be applied and rearranging a little:
\[ m_j \approx \mathscr{M}_j(\hat{f}) = \sum_{k=1}^{N} c_k \mathscr{M}_j(\varphi_k) \]
\(a_{ji} \coloneq \mathscr{M}_j(\varphi_k)\) is the contribution of a single \(k\)th basis function
to the \(j\)th measurement.

Now, the measurements can be stacked to a vector \(m = (m_j) \in  \mathbb{R}^J\), the same for
the coefficients \(c = (c_i) \mathbb{R}^I\), and the contributions \(a_{j} = (a_{ji}) \in
\mathbb{R}^I\). Then a single measurement can be written as a scalar product of the coefficient
vector and the contribution vector. But also importantly, the linear system can be defined:
\begin{equation}\label{eq:system_lin_equation}
	m \approx
	\begin{bmatrix}
		\rule[.5ex]{2em}{0.4pt} & a_1^T & \rule[.5ex]{2em}{0.4pt} \\
		\rule[.5ex]{2em}{0.4pt} & a_2^T & \rule[.5ex]{2em}{0.4pt} \\
		\vdots                                                    \\
		\rule[.5ex]{2em}{0.4pt} & a_J^T & \rule[.5ex]{2em}{0.4pt}
	\end{bmatrix} c \eqcolon A c
\end{equation}

This is a regular system of linear equation, it partitions the problem in the measurements, the
\textit{system matrix} \(A\) and the coefficient vector \(c\). Also note, that the choice of basis
function is integrated into the system matrix.

\section{Iterative Reconstruction}\label{sec:iterative_reconstruction}

The system of linear equations given in \autoref{eq:system_lin_equation} is usually ill-posed due to
noise and measurement errors. Also it is not usually square. Therefore, the problem is not solved
directly, but rather the \textit{least squared problem} can be solved instead.

\begin{definition}[Least Squared Problem]\label{def:least_squares_problem}
	The least squares problem is defined as
	\[ \argmin_c \frac{1}{2} \norm{Ac - m}^2_2 \]
	The solution to the least squares problem is given by the normal equation
	\[ A^T A c = A^T m \]
\end{definition}

Note here, that \(Ac\) is considered the forward projection and \(A^T m\) is the backward
projection.

However, the system matrix is usually to large to store in system memory. Therefore, algorithms are
necessary, which do not require the knowledge of the complete system matrix. The software computing
the system matrix on the fly, is often referred to as projectors. A deep dive into the
implementation will be conduced in \autoref{chap:projector}.


\subsection{ART}\label{subsec:algebraic_reconstruction_technique}

\begin{listing}
	\begin{minted}{cpp}
int main() {
    fmt::print("hello, world\n");
    return 0;
}
    \end{minted}
	\caption{"Some sampe C code"}
\end{listing}
\begin{listing}
	\begin{minted}{python}
import numpy as np

np.linspace(0, 1)
    \end{minted}
	\caption{"Some sampe python code"}
\end{listing}

ART and derivatives

\subsection{CG}\label{subsec:conjuage_gradient}

CG and such

\subsection{First-order methods}\label{subsec:first_order_methods}

First orther methods such as Gradient Descent and it's derivatives

\section{Regularization}\label{sec:regularization}

\subsection{Tikonov}\label{subsec:tikhonov_regularization}

\subsection{LASSO L1}\label{subsec:l1_regularization}

\subsection{TV Regularization}\label{subsec:tv_regularization}
