
@article{entezari_box_2012,
	title = {A {Box} {Spline} {Calculus} for the {Discretization} of {Computed} {Tomography} {Reconstruction} {Problems}},
	volume = {31},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/6172241/},
	doi = {10.1109/TMI.2012.2191417},
	abstract = {B-splines are attractive basis functions for the continuous-domain representation of biomedical images and volumes. In this paper, we prove that the extended family of box splines are closed under the Radon transform and derive explicit formulae for their transforms. Our results are general; they cover all known brands of compactly-supported box splines (tensorproduct B-splines, separable or not) in any number of dimensions. The proposed box spline approach extends to non-Cartesian lattices used for discretizing the image space. In particular, we prove that the 2-D Radon transform of an N -direction box spline is generally a (non-uniform) polynomial spline of degree N − 1. The proposed framework allows for a proper discretization of a variety of tomographic reconstruction problems in a box spline basis. It is of relevance for imaging modalities such as X-ray computed tomography and cryo-electron microscopy.},
	language = {en},
	number = {8},
	urldate = {2021-02-17},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Entezari, A. and Nilchian, M. and Unser, M.},
	month = aug,
	year = {2012},
	pages = {1532--1541},
	file = {Entezari et al. - 2012 - A Box Spline Calculus for the Discretization of Co.pdf:/home/david/Zotero/storage/7W67K34M/Entezari et al. - 2012 - A Box Spline Calculus for the Discretization of Co.pdf:application/pdf},
}

@phdthesis{wieczorek_anisotropic_2017,
	title = {Anisotropic {X}-ray {Dark}-field {Tomography}},
	abstract = {Modern X-ray based imaging enables recording of phase-contrast (refraction) and dark-field
(Small Angle X-ray Scattering) information using Talbot-Lau interferometry. These X-ray
imaging modalities provide improved contrast where standard absorption based imaging
only provides poor to none. The task of Computed Tomography (CT) amounts to recon-
struction of the physical quantities within the imaged object which caused a specific obser-
vation/measurement. A major prerequisite for tomographic reconstruction is first a model
of the physical properties, e.g. using scalars, vectors or tensors. Second, a forward model is
required which enables simulation of measurements from a given 3D representation of the
physical properties. For X-ray based absorption CT, this describes the task of computing the
accumulative effect on the X-ray beam traversing through the object. The combination of a
forward model and corresponding measurements form an inverse problem. Mathematically,
the task of CT corresponds to the inversion of the forward model which can be computed
using according numerical methods.
While tomographic reconstruction for modalities different than X-ray CT often employs very
similar mathematical concepts, software frameworks are often strictly focused on a specific
modality. The first contribution presented in this thesis is the development of an abstract
software framework for tomographic reconstruction. Within this framework the numerical
methods are implemented independently from the specific forward model which enables
adaptation and application of methods for multiple modalities. Additionally, the framework
supports the composition of various common approaches such as regularization methods which
allows for intensive comparison and evaluation of specific methods for multiple modalities.
Within the scope of this work, this framework will be applied to tomographic reconstruction
of the dark-field signal.
Reconstruction of the dark-field signal poses a particularly challenging problem, as the
scattering within an object depends on the X-ray beam’s direction as well as the grating
orientation in contrast to absorption and phase-contrast imaging. Thus, the physical quantity
at each position cannot be modeled by a scalar entity, but requires a more complex model
instead. A first method has been presented previously in form of X-ray Tensor Tomography
(XTT) where a rank-2 tensor is used to describe the scattering happening in each location of
the measured object. This tensor combines information on the scattering strength as well as
its directional distribution which provides an insight into orientation of microstructures within
the object.
A major limitation of the XTT approach is that a tensor is restricted to a single microstructure
direction. In order to cope with this problem within this thesis a general closed-form, continu-
ous forward model of the Anisotropic X-ray Dark-field Tomography will be presented. This
vii
model contains the XTT model under specific assumptions and in addition enables the tomo-
graphic reconstruction of a spherical function representing the whole scattering profile in each
location of the object. This novel approach provides strongly improved reconstructions using
spherical harmonics. All this is achieved at a computational complexity comparable to that
required by XTT. Additionally, an approach to extract the orientation of the microstructures
causing the scattering will be presented. Experiments show that the method of AXDT is capa-
ble of reconstructing multiple scattering orientations and the corresponding microstructure
orientations.
Finally, a first biomedical experiment on a sample of a human cerebellum indicates that AXDT
could provide a complementary imaging modality for imaging nerve fibers within the Central
Nervous System (CNS).},
	language = {en},
	school = {Technische Universität München},
	author = {Wieczorek, Matthias},
	month = oct,
	year = {2017},
	file = {Wieczorek - 2017 - Anisotropic X-ray Dark-field Tomography.pdf:/home/david/Zotero/storage/VINZ2W2A/Wieczorek - 2017 - Anisotropic X-ray Dark-field Tomography.pdf:application/pdf},
}

@article{nilchian_fast_2013,
	title = {Fast iterative reconstruction of differential phase contrast {X}-ray tomograms},
	volume = {21},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-21-5-5511},
	doi = {10.1364/OE.21.005511},
	abstract = {Differential phase-contrast is a recent technique in the context of X-ray imaging. In order to reduce the specimen’s exposure time, we propose a new iterative algorithm that can achieve the same quality as FBP-type methods, while using substantially fewer angular views. Our approach is based on 1) a novel spline-based discretization of the forward model and 2) an iterative reconstruction algorithm using the alternating direction method of multipliers. Our experimental results on real data suggest that the method allows to reduce the number of required views by at least a factor of four.},
	language = {en},
	number = {5},
	urldate = {2021-02-17},
	journal = {Optics Express},
	author = {Nilchian, Masih and Vonesch, Cédric and Modregger, Peter and Stampanoni, Marco and Unser, Michael},
	month = mar,
	year = {2013},
	pages = {5511},
	file = {Nilchian et al. - 2013 - Fast iterative reconstruction of differential phas.pdf:/home/david/Zotero/storage/YPH6XGY2/Nilchian et al. - 2013 - Fast iterative reconstruction of differential phas.pdf:application/pdf},
}

@book{de_boor_box_1993,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Box {Splines}},
	volume = {98},
	isbn = {978-1-4419-2834-4 978-1-4757-2244-4},
	url = {http://link.springer.com/10.1007/978-1-4757-2244-4},
	language = {en},
	urldate = {2021-02-17},
	publisher = {Springer New York},
	author = {de Boor, Carl and Höllig, Klaus and Riemenschneider, Sherman},
	editor = {John, F. and Marsden, J. E. and Sirovich, L.},
	year = {1993},
	doi = {10.1007/978-1-4757-2244-4},
	file = {de Boor et al. - 1993 - Box Splines.pdf:/home/david/Zotero/storage/XZACJRNH/de Boor et al. - 1993 - Box Splines.pdf:application/pdf},
}

@phdthesis{von_teuffenbach_statistical_nodate,
	title = {Statistical signal processing and reconstruction algorithms for grating-based {X}-ray imaging and computed tomography},
	abstract = {Grating-based X-ray interferometry is a novel imaging technique that offers great potential for
the visualization of materials and tissues that are not easily depicted using conventional X-ray
imaging methods. Tomographic reconstruction based on interferometric data provides not only
access to the distribution of an object’s attenuation but also to its refraction and ultra-small-
angle scattering power. These images provide valuable additional information that could well
expand the diagnostic capabilities of a clinical computer tomography (CT) scanner.
One of the main reasons why this technique has not yet been implemented in a modern CT
scanner is that the improved functionality comes at the cost of longer measurement times. Ex-
isting projection-based processing algorithms require not a single measurement per projection
angle but several measurements with precise grating movements in between. A further reason
is that these signal estimation algorithms are also very sensitive to changes in the system align-
ment due to mechanical vibrations or thermal drifts, which are abound in a clinical high-power
CT using a continuously rotating gantry.
Several solutions for these problems have been proposed but all suffer from major drawbacks.
In this thesis first two simple improvements to existing signal estimation methods are pre-
sented and then a novel direct reconstruction method is introduced. A fast algorithm for
reconstructions using this method is developed, the technique is tested at scans using just a
single measurement per angular position and further enhancements that make the reconstruc-
tions robust to vibrations and drifts are implemented and tested.
The results in this thesis demonstrate that it is possible to successfully reconstruct the attenu-
ation, refraction, and ultra-small-angle scattering of an object using only a single measurement
per projection angle on a system influenced by significant vibrations and drifts.
This is a milestone for the future implementation of a grating interferometer onto a continuously
rotating clinical CT scanner.},
	language = {English},
	school = {Technische Universität München},
	author = {von Teuffenbach, Maximilian},
	file = {von Teuffenbach - Statistical signal processing and reconstruction a.pdf:/home/david/Zotero/storage/XDQRZTVK/von Teuffenbach - Statistical signal processing and reconstruction a.pdf:application/pdf},
}

@article{kohler_iterative_2011,
	title = {Iterative reconstruction for differential phase contrast imaging using spherically symmetric basis functions: {Iterative} reconstruction for differential phase constrast imaging},
	volume = {38},
	issn = {00942405},
	shorttitle = {Iterative reconstruction for differential phase contrast imaging using spherically symmetric basis functions},
	url = {http://doi.wiley.com/10.1118/1.3608906},
	doi = {10.1118/1.3608906},
	abstract = {Purpose: The purpose of this work is to combine two areas of active research in tomographic x-ray imaging. The ﬁrst one is the use of iterative reconstruction (IR) techniques. The second one is differential phase contrast imaging (DPCI).
Methods: The authors derive a maximum likelihood (ML) reconstruction algorithm with regularization for DPCI. Forward and back-projection are implemented using spherically symmetric basis functions (blobs) and differential footprints, thus completely avoiding the need for numerical differentiation throughout the reconstruction process. The method is applied to the problem of reconstruction of an object from sparsely sampled projections.
Results: The results show that the proposed method can handle the sparsely sampled data efﬁciently. In particular no streak artifacts are visible which are present in images obtained by ﬁltered back-projection (FBP).
Conclusions: IR algorithms have a wide spectrum of proven advantages in the area of conventional computed tomography. The present work describes for the ﬁrst time, how a matched forward and back-projection can be implemented for DPCI, which is furthermore free of any heuristics. The newly developed ML reconstruction algorithm for DPCI shows that for the case of sparsely sampled projection data, an improvement in image quality is obtained that is qualitatively comparable to a corresponding situation in conventional x-ray imaging. Based on the proposed operators for forward and back-projection, a large variety of IR algorithms is thus made available for DPCI.},
	language = {en},
	number = {8},
	urldate = {2021-02-24},
	journal = {Medical Physics},
	author = {Köhler, Thomas and Brendel, Bernhard and Roessl, Ewald},
	month = jul,
	year = {2011},
	note = {Number: 8},
	pages = {4542--4545},
	file = {Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:/home/david/Zotero/storage/JC48J3TV/Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:application/pdf;Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:/home/david/Zotero/storage/DSPA27N2/Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:application/pdf},
}

@book{bebis_advances_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Visual} {Computing}: 10th {International} {Symposium}, {ISVC} 2014, {Las} {Vegas}, {NV}, {USA}, {December} 8-10, 2014, {Proceedings}, {Part} {I}},
	volume = {8887},
	isbn = {978-3-319-14248-7 978-3-319-14249-4},
	shorttitle = {Advances in {Visual} {Computing}},
	url = {http://link.springer.com/10.1007/978-3-319-14249-4},
	language = {en},
	urldate = {2021-02-24},
	publisher = {Springer International Publishing},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and McMahan, Ryan and Jerald, Jason and Zhang, Hui and Drucker, Steven M. and Kambhamettu, Chandra and El Choubassi, Maha and Deng, Zhigang and Carlson, Mark},
	year = {2014},
	doi = {10.1007/978-3-319-14249-4},
	file = {Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:/home/david/Zotero/storage/BGRSUDL4/Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:application/pdf},
}

@inproceedings{mirzargar_spline_2013,
	address = {San Francisco, CA, USA},
	title = {A spline framework for sparse tomographic reconstruction},
	isbn = {978-1-4673-6455-3 978-1-4673-6456-0 978-1-4673-6454-6},
	url = {http://ieeexplore.ieee.org/document/6556763/},
	doi = {10.1109/ISBI.2013.6556763},
	abstract = {We present a spline-based sparse tomographic reconstruction framework. The proposed method utilizes the closed-form analytical Radon transform of B-splines and box splines of any order and integrates the (transform-domain) sparsity of the image into the reconstruction algorithm. Our experiments show that the synergy of sparse reconstruction together with higher order basis functions (e.g., cubic B-splines) improves the accuracy of the reconstruction. This gain can also be exploited for reducing the number of projection angles in the data acquisition.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {2013 {IEEE} 10th {International} {Symposium} on {Biomedical} {Imaging}},
	publisher = {IEEE},
	author = {Mirzargar, Mahsa and Sakhaee, Elham and Entezari, Alireza},
	month = apr,
	year = {2013},
	pages = {1272--1275},
	file = {Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:/home/david/Zotero/storage/GHZPRVVJ/Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:application/pdf},
}

@article{momey_spline_2015,
	title = {Spline {Driven}: {High} {Accuracy} {Projectors} for {Tomographic} {Reconstruction} {From} {Few} {Projections}},
	volume = {24},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Spline {Driven}},
	url = {http://ieeexplore.ieee.org/document/7182340/},
	doi = {10.1109/TIP.2015.2466083},
	language = {en},
	number = {12},
	urldate = {2021-02-24},
	journal = {IEEE Transactions on Image Processing},
	author = {Momey, Fabien and Denis, Loic and Burnier, Catherine and Thiebaut, Eric and Becker, Jean-Marie and Desbat, Laurent},
	month = dec,
	year = {2015},
	note = {Number: 12},
	pages = {4715--4725},
	file = {Momey et al. - 2015 - Spline Driven High Accuracy Projectors for Tomogr.pdf:/home/david/Zotero/storage/BVR98LW9/Momey et al. - 2015 - Spline Driven High Accuracy Projectors for Tomogr.pdf:application/pdf},
}

@article{momey_b-spline_2012,
	title = {A {B}-spline based and computationally performant projector for iterative reconstruction in tomography},
	language = {en},
	author = {Momey, Fabien and Denis, Loıc and Mennessier, Catherine},
	year = {2012},
	pages = {5},
	file = {Momey et al. - 2012 - A B-spline based and computationally performant pr.pdf:/home/david/Zotero/storage/KKG94HAZ/Momey et al. - 2012 - A B-spline based and computationally performant pr.pdf:application/pdf},
}

@article{kobbelt_stable_1997,
	title = {Stable {Evaluation} of {Box} {Splines}},
	abstract = {The most elegant way to evaluate box-splines is by using their recursive de nition. However, a straightforward implementation reveals numerical di culties. A careful analysis of the algorithm allows a reformulation which overcomes these problems without losing e ciency. A concise vectorized MATLAB-implementation is given.},
	language = {en},
	author = {Kobbelt, Leif},
	month = may,
	year = {1997},
	pages = {4},
	file = {Kobbelt - 1997 - Stable Evaluation of Box Splines.pdf:/home/david/Zotero/storage/4A3VXJDQ/Kobbelt - 1997 - Stable Evaluation of Box Splines.pdf:application/pdf},
}

@article{richter_use_1998,
	title = {Use of box splines in computer tomography},
	volume = {61},
	issn = {0010-485X, 1436-5057},
	url = {http://link.springer.com/10.1007/BF02684410},
	doi = {10.1007/BF02684410},
	abstract = {Box splines are attractive for practical multivariate approximation, since they possess good approximation power and can he evaluated very efficiently.We want to give an idea of how their qualities can be made to come into play in the field of image reconstruction in computerized tomography (CT). To keep the exposition simple, we will concentrate on a special situation: our tomograph will be characterized by the bivariate standard scanning geometry and our reconstructions will alwayslie in scales of the linear space spanned by the integer translates of a fixed piecewise quadratic box spline. On the other hand we give details of an algorithm based on Fourier reconstruction, which produces approximationsof optimal order for the box splines used, whilst the amount of computational work required is of no higher order than for classical Fourier reconstruction. We present another reconstruction procedure based on quasi-interpolation, which compares to filtered backprojection in computational complexity.Along with our exposition,we give a generalization of a certain Theorem due to Nievergelt which may be of interest for practical applications.},
	language = {en},
	number = {2},
	urldate = {2021-02-24},
	journal = {Computing},
	author = {Richter, M.},
	month = jun,
	year = {1998},
	note = {Number: 2},
	pages = {133--150},
	file = {Richter - 1998 - Use of box splines in computer tomography.pdf:/home/david/Zotero/storage/HYN3R3C2/Richter - 1998 - Use of box splines in computer tomography.pdf:application/pdf},
}

@incollection{prautzsch_box_2002,
	title = {Box {Splines}},
	isbn = {0-444-51104-0},
	booktitle = {Handbook of {Computer} {Aided} {Geometric} {Design}},
	publisher = {North Holland; Illustrated Edition},
	author = {Prautzsch, Hartmut and Boehm, Wolfgang},
	month = mar,
	year = {2002},
	keywords = {Box Splines},
	file = {Prautzsch and Boehm - 2002 - Box Splines.pdf:/home/david/Zotero/storage/FJILGBCN/Prautzsch and Boehm - 2002 - Box Splines.pdf:application/pdf},
}

@article{ruijters_gpu_2012,
	title = {{GPU} {Prefilter} for {Accurate} {Cubic} {B}-spline {Interpolation}},
	volume = {55},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxq086},
	doi = {10.1093/comjnl/bxq086},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {The Computer Journal},
	author = {Ruijters, D. and Thevenaz, P.},
	month = jan,
	year = {2012},
	pages = {15--20},
	file = {Ruijters and Thevenaz - 2012 - GPU Prefilter for Accurate Cubic B-spline Interpol.pdf:/home/david/Zotero/storage/7V32IDUX/Ruijters and Thevenaz - 2012 - GPU Prefilter for Accurate Cubic B-spline Interpol.pdf:application/pdf},
}

@inproceedings{entezari_linear_2004,
	address = {Austin, TX, USA},
	title = {Linear and cubic box splines for the body centered cubic lattice},
	isbn = {978-0-7803-8788-1},
	url = {http://ieeexplore.ieee.org/document/1372174/},
	doi = {10.1109/VISUAL.2004.65},
	abstract = {In this paper we derive piecewise linear and piecewise cubic box spline reconstruction ﬁlters for data sampled on the body centered cubic (BCC) lattice. We analytically derive a time domain representation of these reconstruction ﬁlters and using the Fourier slice-projection theorem we derive their frequency responses. The quality of these ﬁlters, when used in reconstructing BCC sampled volumetric data, is discussed and is demonstrated with a raycaster. Moreover, to demonstrate the superiority of the BCC sampling, the resulting reconstructions are compared with those produced from similar ﬁlters applied to data sampled on the Cartesian lattice.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {{IEEE} {Visualization} 2004},
	publisher = {IEEE Comput. Soc},
	author = {Entezari, A. and Dyer, R. and Moller, T.},
	year = {2004},
	pages = {11--18},
	file = {Entezari et al. - 2004 - Linear and cubic box splines for the body centered.pdf:/home/david/Zotero/storage/Q4MD9SQB/Entezari et al. - 2004 - Linear and cubic box splines for the body centered.pdf:application/pdf},
}

@article{horacsek_fast_2016,
	title = {Fast and exact evaluation of box splines via the {PP}-form},
	url = {http://arxiv.org/abs/1606.08910},
	abstract = {For the class of non-degenerate box splines, we prove that these box splines are piecewise polynomial. This is not a new result, it is in fact a well known and useful property of box splines. However, our proof is constructive, and the main result of this work is a corollary that follows from this proof, namely one that gives an explicit construction scheme for the polynomial pieces in the interior regions of any non-degenerate box spline.},
	language = {en},
	urldate = {2021-02-24},
	journal = {arXiv:1606.08910 [math]},
	author = {Horacsek, Joshua and Alim, Usman},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.08910},
	keywords = {Mathematics - Functional Analysis, Mathematics - Numerical Analysis},
	file = {Horacsek and Alim - 2016 - Fast and exact evaluation of box splines via the P.pdf:/home/david/Zotero/storage/FDPCAMML/Horacsek and Alim - 2016 - Fast and exact evaluation of box splines via the P.pdf:application/pdf},
}

@article{condat_three-directional_2006,
	title = {Three-directional box-splines: characterization and efficient evaluation},
	volume = {13},
	issn = {1070-9908},
	shorttitle = {Three-directional box-splines},
	url = {http://ieeexplore.ieee.org/document/1642713/},
	doi = {10.1109/LSP.2006.871852},
	abstract = {We propose a new characterization of three-directional box-splines, which are well adapted for interpolation and approximation on hexagonal lattices. Inspired by a construction already applied with success for exponential splines [1] and hex-splines [2], we characterize a box-spline as a convolution of a generating function, which is a Green function of the spline’s associated differential operator, and a discrete ﬁlter that plays the role of a localization operator. This process leads to an elegant analytical expression of three-directional box-splines. It also brings along a particularly efﬁcient implementation.},
	language = {en},
	number = {7},
	urldate = {2021-02-24},
	journal = {IEEE Signal Processing Letters},
	author = {Condat, L. and Van De Ville, D.},
	month = jul,
	year = {2006},
	pages = {417--420},
	file = {Condat and Van De Ville - 2006 - Three-directional box-splines characterization an.pdf:/home/david/Zotero/storage/JSMJQQMN/Condat and Van De Ville - 2006 - Three-directional box-splines characterization an.pdf:application/pdf},
}

@article{de_boor_evaluation_2000,
	title = {On the evaluation of box splines},
	language = {en},
	author = {de Boor, Carl},
	month = feb,
	year = {2000},
	pages = {19},
	file = {de Boor - 2000 - On the evaluation of box splines.pdf:/home/david/Zotero/storage/ZZ7I7GFD/de Boor - 2000 - On the evaluation of box splines.pdf:application/pdf},
}

@article{speleers_inner_2015,
	title = {Inner products of box splines and their derivatives},
	volume = {55},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/s10543-014-0513-1},
	doi = {10.1007/s10543-014-0513-1},
	abstract = {A simple and explicit expression is given for the inner product of (higher order) derivatives of multivariate box splines and their translates. We also show that the energy inner product related to a linear partial differential equation discretized with a set of shifted box splines can be interpreted as an evaluation of the differential operator applied to a higher order box spline.},
	language = {en},
	number = {2},
	urldate = {2021-02-24},
	journal = {BIT Numerical Mathematics},
	author = {Speleers, Hendrik},
	month = jun,
	year = {2015},
	pages = {559--567},
	file = {Speleers - 2015 - Inner products of box splines and their derivative.pdf:/home/david/Zotero/storage/KQNDLECX/Speleers - 2015 - Inner products of box splines and their derivative.pdf:application/pdf},
}

@book{hansen_discrete_2010,
	address = {Philadelphia},
	series = {Fundamentals of algorithms},
	title = {Discrete inverse problems: insight and algorithms},
	isbn = {978-0-89871-696-2},
	shorttitle = {Discrete inverse problems},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Hansen, Per Christian},
	year = {2010},
	note = {OCLC: ocn469915410},
	keywords = {Inverse problems (Differential equations)},
	file = {Hansen - 2010 - Discrete inverse problems insight and algorithms.pdf:/home/david/Zotero/storage/6D4E4743/Hansen - 2010 - Discrete inverse problems insight and algorithms.pdf:application/pdf},
}

@book{rieder_keine_2003,
	address = {Wiesbaden},
	title = {Keine {Probleme} mit {Inversen} {Problemen}},
	isbn = {978-3-528-03198-5 978-3-322-80234-7},
	url = {http://link.springer.com/10.1007/978-3-322-80234-7},
	language = {de},
	urldate = {2021-03-04},
	publisher = {Vieweg+Teubner Verlag},
	author = {Rieder, Andreas},
	year = {2003},
	doi = {10.1007/978-3-322-80234-7},
	file = {Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:/home/david/Zotero/storage/C2Y88G2D/Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:text/html;Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:/home/david/Zotero/storage/U4EAHJF9/Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:application/pdf},
}

@article{entezari_practical_2008,
	title = {Practical {Box} {Splines} for {Reconstruction} on the {Body} {Centered} {Cubic} {Lattice}},
	volume = {14},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/4359498/},
	doi = {10.1109/TVCG.2007.70429},
	abstract = {We introduce a family of box splines for efficient, accurate, and smooth reconstruction of volumetric data sampled on the body-centered cubic (BCC) lattice, which is the favorable volumetric sampling pattern due to its optimal spectral sphere packing property. First, we construct a box spline based on the four principal directions of the BCC lattice that allows for a linear C0 reconstruction. Then, the design is extended for higher degrees of continuity. We derive the explicit piecewise polynomial representations of the C0 and C2 box splines that are useful for practical reconstruction applications. We further demonstrate that approximation in the shift-invariant space—generated by BCC-lattice shifts of these box splines—is twice as efficient as using the tensor-product B-spline solutions on the Cartesian lattice (with comparable smoothness and approximation order and with the same sampling density). Practical evidence is provided demonstrating that the BCC lattice not only is generally a more accurate sampling pattern, but also allows for extremely efficient reconstructions that outperform tensor-product Cartesian reconstructions.},
	language = {en},
	number = {2},
	urldate = {2021-03-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Entezari, Alireza and Van De Ville, Dimitri and Moller, Torsten},
	month = mar,
	year = {2008},
	pages = {313--328},
	file = {Entezari et al. - 2008 - Practical Box Splines for Reconstruction on the Bo.pdf:/home/david/Zotero/storage/FMXDXXTX/Entezari et al. - 2008 - Practical Box Splines for Reconstruction on the Bo.pdf:application/pdf},
}

@article{paleo_practical_2016,
	title = {A practical local tomography reconstruction algorithm based on known subregion},
	url = {http://arxiv.org/abs/1606.04940},
	abstract = {We propose a new method to reconstruct data acquired in a local tomography setup. This method uses an initial reconstruction and reﬁnes it by correcting the low frequency artifacts known as the cupping eﬀect. A basis of Gaussian functions is used to correct the initial reconstruction. The coeﬃcients of this basis are iteratively optimized under the constraint of a known subregion. Using a coarse basis reduces the degrees of freedom of the problem while actually correcting the cupping eﬀect. Simulations show that the known region constraint yields an unbiased reconstruction, in accordance to uniqueness theorems stated in local tomography.},
	language = {en},
	urldate = {2021-03-10},
	journal = {arXiv:1606.04940 [physics]},
	author = {Paleo, Pierre and Desvignes, Michel and Mirone, Alessandro},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04940},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Physics - Medical Physics},
	file = {Paleo et al. - 2016 - A practical local tomography reconstruction algori.pdf:/home/david/Zotero/storage/X5MLXQN7/Paleo et al. - 2016 - A practical local tomography reconstruction algori.pdf:application/pdf},
}

@inproceedings{zhou_blob-based_2008,
	address = {San Diego, CA},
	title = {A blob-based tomographic reconstruction of {3D} coronary trees from rotational x-ray angiography},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.769478},
	doi = {10.1117/12.769478},
	abstract = {A method is proposed for a 3D reconstruction of coronary networks from rotational projections that departs from motion-compensated approaches. It deals with multiple views extracted from a time-stamped image sequence through ECG gating. This statistics-based vessel reconstruction method relies on a new imaging model by considering both the eﬀect of background tissues and the image representation using spherically-symmetric basis functions, also called ’blobs’ . These blobs have a closed analytical expression for the X-ray transform, which makes easier to compute a cone-beam projection than a voxel-based description. A Bayesian maximum a posteriori (MAP) estimation is used with a Poisson distributed projection data instead of the Gaussian approximation often used in tomography reconstruction. A heavy-tailed distribution is proposed as image prior to take into account the sparse nature of the object of interest. The optimization is performed by an expectationmaximization like (EM) block iterative algorithm which oﬀers a fast convergence and a sound introduction of the non-negativity constraint for vessel attenuation coeﬃcients. Simulations are performed using a model of coronary tree extracted from multidetector CT scanner and a performance study is conducted. They point out that, even with severe angular undersampling (6 projections over 110 degrees for instance) and without introducing a prior model of the object, signiﬁcant results can be achieved.},
	language = {en},
	urldate = {2021-03-10},
	author = {Zhou, Jian and Bousse, Alexandre and Yang, Guanyu and Bellanger, Jean-Jacques and Luo, Limin and Toumoulin, Christine and Coatrieux, Jean-Louis},
	editor = {Hsieh, Jiang and Samei, Ehsan},
	month = mar,
	year = {2008},
	keywords = {coroncary trees},
	pages = {69132N},
	file = {Zhou et al. - 2008 - A blob-based tomographic reconstruction of 3D coro.pdf:/home/david/Zotero/storage/C7TUBHKX/Zhou et al. - 2008 - A blob-based tomographic reconstruction of 3D coro.pdf:application/pdf},
}

@article{castrillo_blob-enhanced_2016,
	title = {Blob-enhanced reconstruction technique},
	volume = {27},
	issn = {0957-0233, 1361-6501},
	url = {https://iopscience.iop.org/article/10.1088/0957-0233/27/9/094011},
	doi = {10.1088/0957-0233/27/9/094011},
	abstract = {A method to enhance the quality of the tomographic reconstruction and, consequently, the 3D velocity measurement accuracy, is presented. The technique is based on integrating information on the objects to be reconstructed within the algebraic reconstruction process. A first guess intensity distribution is produced with a standard algebraic method, then the distribution is rebuilt as a sum of Gaussian blobs, based on location, intensity and size of agglomerates of light intensity surrounding local maxima. The blobs substitution regularizes the particle shape allowing a reduction of the particles discretization errors and of their elongation in the depth direction. The performances of the blob-enhanced reconstruction technique (BERT) are assessed with a 3D synthetic experiment. The results have been compared with those obtained by applying the standard camera simultaneous multiplicative reconstruction technique (CSMART) to the same volume. Several blob-enhanced reconstruction processes, both substituting the blobs at the end of the CSMART algorithm and during the iterations (i.e. using the blob-enhanced reconstruction as predictor for the following iterations), have been tested. The results confirm the enhancement in the velocity measurements accuracy, demonstrating a reduction of the bias error due to the ghost particles. The improvement is more remarkable at the largest tested seeding densities. Additionally, using the blobs distributions as a predictor enables further improvement of the convergence of the reconstruction algorithm, with the improvement being more considerable when substituting the blobs more than once during the process. The BERT process is also applied to multi resolution (MR) CSMART reconstructions, permitting simultaneously to achieve remarkable improvements in the flow field measurements and to benefit from the reduction in computational time due to the MR approach. Finally, BERT is also tested on experimental data, obtaining an increase of the signal-to-noise ratio in the reconstructed flow field and a higher value of the correlation factor in the velocity measurements with respect to the volume to which the particles are not replaced.},
	language = {en},
	number = {9},
	urldate = {2021-03-10},
	journal = {Measurement Science and Technology},
	author = {Castrillo, Giusy and Cafiero, Gioacchino and Discetti, Stefano and Astarita, Tommaso},
	month = sep,
	year = {2016},
	pages = {094011},
	file = {Castrillo et al. - 2016 - Blob-enhanced reconstruction technique.pdf:/home/david/Zotero/storage/ZC3AVLSH/Castrillo et al. - 2016 - Blob-enhanced reconstruction technique.pdf:application/pdf},
}

@article{herman_basis_2015,
	title = {Basis {Functions} in {Image} {Reconstruction} {From} {Projections}: {A} {Tutorial} {Introduction}},
	volume = {16},
	issn = {1557-2064, 1557-2072},
	shorttitle = {Basis {Functions} in {Image} {Reconstruction} {From} {Projections}},
	url = {http://link.springer.com/10.1007/s11220-015-0107-2},
	doi = {10.1007/s11220-015-0107-2},
	abstract = {The series expansion approaches to image reconstruction from projections assume that the object to be reconstructed can be represented as a linear combination of ﬁxed basis functions and the task of the reconstruction algorithm is to estimate the coefﬁcients in such a linear combination based on the measured projection data. It is demonstrated that using spherically symmetric basis functions (blobs), instead of ones based on the more traditional pixels, yields superior reconstructions of medically relevant objects. The demonstration uses simulated computerized tomography projection data of head cross-sections and the series expansion method ART for the reconstruction. In addition to showing the results of one anecdotal example, the relative efﬁcacy of using pixel and blob basis functions in image reconstruction from projections is also evaluated using a statistical hypothesis testing based task oriented comparison methodology. The superiority of the efﬁcacy of blob basis functions over that of pixel basis function is found to be statistically signiﬁcant.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Sensing and Imaging},
	author = {Herman, Gabor T.},
	month = dec,
	year = {2015},
	keywords = {blobs},
	pages = {6},
	file = {Herman - 2015 - Basis Functions in Image Reconstruction From Proje.pdf:/home/david/Zotero/storage/CRBR5QWI/Herman - 2015 - Basis Functions in Image Reconstruction From Proje.pdf:application/pdf},
}

@article{jacobs_iterative_nodate,
	title = {Iterative {Image} {Reconstruction} {From} {Projections} {Based} {On} {Generalised} {Kaiser}-{Bessel} {Window} {Functions}},
	abstract = {Tomographic images are calculated from data provided by a scanner. The reconstruction algorithms used for this purpose are either analytical or iterative in nature. The paper focuses on an iterative algorithm called the row-action maximum-likelihood algorithm, and on transmission tomography. Iterative algorithms approximate the image as a linear combination of a limited set of basis functions. The paper introduces a new set of basis functions, called blobs, into the field of process tomography. It also presents preliminary results of a study which evaluates the advantage of using blobs, instead of pixels, for different amounts of available data and different noise levels. The results clearly show that the use of blobs is also beneficial for process tomography.},
	language = {en},
	author = {Jacobs, Filip and Lemahieu, Ignace},
	pages = {7},
	file = {Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:/home/david/Zotero/storage/2P5QZHG3/Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:application/pdf},
}

@article{lewitt_alternatives_1992,
	title = {Alternatives to voxels for image representation in iterative reconstruction algorithms},
	volume = {37},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/37/3/015},
	doi = {10.1088/0031-9155/37/3/015},
	abstract = {Spherically symmetric volume elements are alternatives to the more conventional voxels far the Construction o f volume images in the computer. The image representation, and the calculation of projections of it, are essential components of iterative algorithms for image reconstmction from projenion data. A two-parameter family of spherical volume elements is described that allows control of the smoothness properties of the represented image, whereas conventional voxek are discontinuous. The rotational symmetry of the spherical elements leads to efficient calculation of projections of the represented image, as required in iterative reconstruction algorithms. Far volume elements whose shape is ellipsoidal (rather than spherical) it is shown that efficient calculation of the projections is also possible by means of an image space transformation.},
	language = {en},
	number = {3},
	urldate = {2021-03-10},
	journal = {Physics in Medicine and Biology},
	author = {Lewitt, R M},
	month = mar,
	year = {1992},
	pages = {705--716},
	file = {Lewitt - 1992 - Alternatives to voxels for image representation in.pdf:/home/david/Zotero/storage/YXG4DKA2/Lewitt - 1992 - Alternatives to voxels for image representation in.pdf:application/pdf},
}

@article{matej_efficient_1995,
	title = {Efficient {3D} grids for image reconstruction using spherically-symmetric volume elements},
	volume = {42},
	issn = {00189499},
	url = {http://ieeexplore.ieee.org/document/467854/},
	doi = {10.1109/23.467854},
	language = {en},
	number = {4},
	urldate = {2021-03-10},
	journal = {IEEE Transactions on Nuclear Science},
	author = {Matej, S. and Lewitt, R.M.},
	month = aug,
	year = {1995},
	pages = {1361--1370},
	file = {Matej and Lewitt - 1995 - Efficient 3D grids for image reconstruction using .pdf:/home/david/Zotero/storage/YUQSMM4P/Matej and Lewitt - 1995 - Efficient 3D grids for image reconstruction using .pdf:application/pdf},
}

@article{matej_practical_1996,
	title = {Practical considerations for 3-{D} image reconstruction using spherically symmetric volume elements},
	volume = {15},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/481442/},
	doi = {10.1109/42.481442},
	abstract = {Spherically symmetric volume elements with smooth tapering of the values near their boundaries are alternatives to the more conventional voxels for the construction of volume images in the computer. Their use, instead of voxels, introduces additional parameters which enable the user to control the shape of the volume element (blob) and consequently to control the characteristics of the images produced by iterative methods for reconstruction from projection data. For images composed of blobs, efficient algorithms have been designed €or the projection and discrete back-projection operations, which are the crucial parts of iterative reconstruction methods. We have investigated the relationship between the values of the blob parameters and the properties of images represented by the blobs. Experiments show that using blobs in iterative reconstruction methods leads to substantial improvement in the reconstruction performance, based on visual quality and on quantitative measures, in comparison with the voxel case. The images reconstructed using appropriately chosen blobs are characterized by less image noise for both noiseless data and noisy data, without loss of image resolution.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Matej, S. and Lewitt, R.M.},
	month = feb,
	year = {1996},
	pages = {68--78},
	file = {Matej and Lewitt - 1996 - Practical considerations for 3-D image reconstruct.pdf:/home/david/Zotero/storage/497ZYSHG/Matej and Lewitt - 1996 - Practical considerations for 3-D image reconstruct.pdf:application/pdf},
}

@article{bippus_projector_2011,
	title = {Projector and {Backprojector} for {Iterative} {CT} {Reconstruction} with {Blobs} using {CUDA}},
	abstract = {Using blobs allows modeling the CT system’s geometry more correctly within an iterative reconstruction framework. However their application comes with an increased computational demand. This led us to use blobs for image representation and a dedicated GPU hardware implementation to counteract their computational demand. Making extensive use of the texture interpolation capabilities of CUDA and implementing an asymmetric projector/backprojector pair we achieve reasonable processing times and good system modeling at the same time.},
	language = {en},
	author = {Bippus, Rolf-Dieter and Köhler, Thomas and Bergner, Frank and Brendel, Bernhard and Hansis, Eberhard and Proksa, Roland},
	year = {2011},
	pages = {4},
	file = {Bippus et al. - 2011 - Projector and Backprojector for Iterative CT Recon.pdf:/home/david/Zotero/storage/KEA2CVT8/Bippus et al. - 2011 - Projector and Backprojector for Iterative CT Recon.pdf:application/pdf},
}

@inproceedings{popescu_ray_2004,
	address = {Rome, Italy},
	title = {Ray tracing through a grid of blobs},
	volume = {6},
	isbn = {978-0-7803-8700-3},
	url = {http://ieeexplore.ieee.org/document/1466750/},
	doi = {10.1109/NSSMIC.2004.1466750},
	abstract = {In this paper we describe two ray tracing algorithms for images represented using spherically symmetric basis functions (blobs) on regular grids. The method presented here allows more realistic modeling of the forward projection by considering tube shaped kernels, rather than simple lines. Each kernel is a function of the radial distance r from its center and can vary with the position l along the projection line. The forward projections are computed by convolutions of the kernel with the blob line integrals. Both ray tracing procedures presented incrementally compute the square distance r2 for each visited blob enabling the appropriate resolution kernel to be used. The second variant also computes the l coordinate along the line of response axis allowing for longitudinal variations of the resolution kernel to be considered as well as time-of-ﬂight (TOF) modeling.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {{IEEE} {Symposium} {Conference} {Record} {Nuclear} {Science} 2004.},
	publisher = {IEEE},
	author = {Popescu, L.M. and Lewitt, R.M.},
	year = {2004},
	pages = {3983--3986},
	file = {Popescu and Lewitt - 2004 - Ray tracing through a grid of blobs.pdf:/home/david/Zotero/storage/SZY9DRKS/Popescu and Lewitt - 2004 - Ray tracing through a grid of blobs.pdf:application/pdf},
}

@inproceedings{momey_new_2011,
	address = {Valencia, Spain},
	title = {A new representation and projection model for tomography, based on separable {B}-splines},
	isbn = {978-1-4673-0120-6 978-1-4673-0118-3 978-1-4673-0119-0},
	url = {http://ieeexplore.ieee.org/document/6152700/},
	doi = {10.1109/NSSMIC.2011.6152700},
	abstract = {Data modelization in tomography is a key point for iterative reconstruction. The design of the projector, i.e. the numerical model of projection, is mostly inﬂuenced by the representation of the object of interest, decomposed on a discrete basis of functions.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {2011 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	publisher = {IEEE},
	author = {Momey, Fabien and Denis, Loic and Mennessier, Catherine and Thiebaut, Eric and Becker, Jean-Marie and Desbat, Laurent},
	month = oct,
	year = {2011},
	pages = {2602--2609},
	file = {Momey et al. - 2011 - A new representation and projection model for tomo.pdf:/home/david/Zotero/storage/GFS8TT3D/Momey et al. - 2011 - A new representation and projection model for tomo.pdf:application/pdf},
}

@inproceedings{zhang_box_2019,
	address = {Venice, Italy},
	title = {Box {Spline} {Projection} in {Non}-{Parallel} {Geometry}},
	isbn = {978-1-5386-3641-1},
	url = {https://ieeexplore.ieee.org/document/8759327/},
	doi = {10.1109/ISBI.2019.8759327},
	abstract = {The pixel- and voxel-basis are common choices for image discretization in the context of computed tomography (CT). They can also be viewed as ﬁrst-order box splines – a class of functions with closed-form X-ray and Radon transforms that can be computed efﬁciently. In this paper we derive a method for exact projection of box splines in a non-parallel geometry that can be used in fan-beam and cone-beam tomographic image reconstruction algorithms. We also provide efﬁcient computational procedures for evaluation of the basis function in the projection domain.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	publisher = {IEEE},
	author = {Zhang, Kai and Entezari, Alireza},
	month = apr,
	year = {2019},
	pages = {1844--1847},
	file = {Zhang and Entezari - 2019 - Box Spline Projection in Non-Parallel Geometry.pdf:/home/david/Zotero/storage/L2VHGIHT/Zhang and Entezari - 2019 - Box Spline Projection in Non-Parallel Geometry.pdf:application/pdf},
}

@inproceedings{lasser_elsa_2019,
	address = {Philadelphia, United States},
	title = {elsa - an elegant framework for tomographic reconstruction},
	isbn = {978-1-5106-2837-3 978-1-5106-2838-0},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11072/2534833/elsa---an-elegant-framework-for-tomographic-reconstruction/10.1117/12.2534833.full},
	doi = {10.1117/12.2534833},
	abstract = {Software for tomographic reconstruction has been around for decades now. So why yet another software framework for tomographic reconstruction? Because we needed a ﬂexible, operator- and optimization-based framework in C++ for our own target applications, we developed our own some years ago. As our framework has been applied to many tomographic problems by now, ranging from optical tomography, lightﬁeld tomography, SPECT, to various X-ray based imaging modalities (absorption contrast, differential phase contrast and anisotropic dark-ﬁeld contrast), we decided to open source a modernized version of it. The framework elsa is written in platform-independent modern C++17 using the CMake build system, with high unit-test coverage and continuous integration to ascertain reliability and correctness, as well as a Python interface for easy and rapid prototyping. Our intent in open sourcing the framework and presenting it here is three-fold, ﬁrst for easier reproducibility of our own research, second for use in teaching, and last but not least, in the hopes that some of you also ﬁnd some usefulness in it for your own tasks.},
	language = {en},
	urldate = {2021-04-13},
	booktitle = {15th {International} {Meeting} on {Fully} {Three}-{Dimensional} {Image} {Reconstruction} in {Radiology} and {Nuclear} {Medicine}},
	publisher = {SPIE},
	author = {Lasser, Tobias and Hornung, Maximilian and Frank, David},
	editor = {Matej, Samuel and Metzler, Scott D.},
	month = may,
	year = {2019},
	pages = {69},
	file = {Lasser et al. - 2019 - elsa - an elegant framework for tomographic recons.pdf:/home/david/Zotero/storage/KBNCTUTY/Lasser et al. - 2019 - elsa - an elegant framework for tomographic recons.pdf:application/pdf},
}

@article{kim_optimized_2016,
	title = {Optimized first-order methods for smooth convex minimization},
	volume = {159},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-015-0949-3},
	doi = {10.1007/s10107-015-0949-3},
	abstract = {We introduce new optimized ﬁrst-order methods for smooth unconstrained convex minimization. Drori and Teboulle (Math Program 145(1–2):451–482, 2014. doi:10.1007/s10107-013-0653-0) recently described a numerical method for computing the N -iteration optimal step coefﬁcients in a class of ﬁrst-order algorithms that includes gradient methods, heavy-ball methods (Polyak in USSR Comput Math Math Phys 4(5):1–17, 1964. doi:10.1016/0041-5553(64)90137-5), and Nesterov’s fast gradient methods (Nesterov in Sov Math Dokl 27(2):372–376, 1983; Math Program 103(1):127–152, 2005. doi:10.1007/s10107-004-0552-5). However, the numerical method in Drori and Teboulle (2014) is computationally expensive for large N , and the corresponding numerically optimized ﬁrst-order algorithm in Drori and Teboulle (2014) requires impractical memory and computation for large-scale optimization problems. In this paper, we propose optimized ﬁrst-order algorithms that achieve a convergence bound that is two times smaller than for Nesterov’s fast gradient methods; our bound is found analytically and reﬁnes the numerical bound in Drori and Teboulle (2014). Furthermore, the proposed optimized ﬁrst-order methods have efﬁcient forms that are remarkably similar to Nesterov’s fast gradient methods.},
	language = {en},
	number = {1-2},
	urldate = {2021-04-15},
	journal = {Mathematical Programming},
	author = {Kim, Donghwan and Fessler, Jeffrey A.},
	month = sep,
	year = {2016},
	pages = {81--107},
	file = {Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:/home/david/Zotero/storage/REIHELDD/Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:application/pdf;Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:/home/david/Zotero/storage/WJXYZPC5/Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:application/pdf},
}

@article{kim_combining_2015,
	title = {Combining {Ordered} {Subsets} and {Momentum} for {Accelerated} {X}-{Ray} {CT} {Image} {Reconstruction}},
	volume = {34},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/6882248},
	doi = {10.1109/TMI.2014.2350962},
	abstract = {Statistical X-ray computed tomography (CT) reconstruction can improve image quality from reduced dose scans, but requires very long computation time. Ordered subsets (OS) methods have been widely used for research in X-ray CT statistical image reconstruction (and are used in clinical PET and SPECT reconstruction). In particular, OS methods based on separable quadratic surrogates (OS-SQS) are massively parallelizable and are well suited to modern computing architectures, but the number of iterations required for convergence should be reduced for better practical use. This paper introduces OS-SQS-momentum algorithms that combine Nesterov’s momentum techniques with OS-SQS methods, greatly improving convergence speed in early iterations. If the number of subsets is too large, the OS-SQS-momentum methods can be unstable, so we propose diminishing step sizes that stabilize the method while preserving the very fast convergence behavior. Experiments with simulated and real 3D CT scan data illustrate the performance of the proposed algorithms.},
	language = {en},
	number = {1},
	urldate = {2021-04-15},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Kim, Donghwan and Ramani, Sathish and Fessler, Jeffrey A.},
	month = jan,
	year = {2015},
	pages = {167--178},
	file = {Kim et al. - 2015 - Combining Ordered Subsets and Momentum for Acceler.pdf:/home/david/Zotero/storage/8IHVXLGK/Kim et al. - 2015 - Combining Ordered Subsets and Momentum for Acceler.pdf:application/pdf},
}

@article{sangtae_ahn_convergent_2006,
	title = {Convergent incremental optimization transfer algorithms: application to tomography},
	volume = {25},
	issn = {0278-0062},
	shorttitle = {Convergent incremental optimization transfer algorithms},
	url = {http://ieeexplore.ieee.org/document/1599443/},
	doi = {10.1109/TMI.2005.862740},
	abstract = {No convergent ordered subsets (OS) type image reconstruction algorithms for transmission tomography have been proposed to date. In contrast, in emission tomography, there are two known families of convergent OS algorithms: methods that use relaxation parameters [1], and methods based on the incremental expectation-maximization (EM) approach [2]. This paper generalizes the incremental EM approach [3] by introducing a general framework, “incremental optimization transfer.” The proposed algorithms accelerate convergence speeds and ensure global convergence without requiring relaxation parameters. The general optimization transfer framework allows the use of a very broad family of surrogate functions, enabling the development of new algorithms [4]. This paper provides the ﬁrst convergent OS-type algorithm for (nonconcave) penalized-likelihood (PL) transmission image reconstruction by using separable paraboloidal surrogates (SPS) [5] which yield closed-form maximization steps. We found it is very effective to achieve fast convergence rates by starting with an OS algorithm with a large number of subsets and switching to the new “transmission incremental optimization transfer (TRIOT)” algorithm. Results show that TRIOT is faster in increasing the PL objective than nonincremental ordinary SPS and even OS-SPS yet is convergent.},
	language = {en},
	number = {3},
	urldate = {2021-04-15},
	journal = {IEEE Transactions on Medical Imaging},
	author = {{Sangtae Ahn} and Fessler, J.A. and Blatt, D. and Hero, A.O.},
	month = mar,
	year = {2006},
	pages = {283--296},
	file = {Sangtae Ahn et al. - 2006 - Convergent incremental optimization transfer algor.pdf:/home/david/Zotero/storage/MZCD9FUJ/Sangtae Ahn et al. - 2006 - Convergent incremental optimization transfer algor.pdf:application/pdf},
}

@article{gregor_comparison_2015,
	title = {Comparison of {SIRT} and {SQS} for {Regularized} {Weighted} {Least} {Squares} {Image} {Reconstruction}},
	volume = {1},
	issn = {2333-9403, 2334-0118},
	url = {http://ieeexplore.ieee.org/document/7118685/},
	doi = {10.1109/TCI.2015.2442511},
	abstract = {Tomographic image reconstruction is often formulated as a regularized weighted least squares (RWLS) problem optimized by iterative algorithms that are either inherently algebraic or derived from a statistical point of view. This paper compares a modiﬁed version of simultaneous iterative reconstruction technique (SIRT), which is of the former type, with a version of separable quadratic surrogates (SQS), which is of the latter type. We show that the two algorithms minimize the same criterion function using similar forms of preconditioned gradient descent. We present near-optimal relaxation for both based on eigenvalue bounds and include a heuristic extension for use with ordered subsets. We provide empirical evidence that SIRT and SQS converge at the same rate for all intents and purposes. For context, we compare their performance with an implementation of preconditioned conjugate gradient. The illustrative application is X-ray CT of luggage for aviation security.},
	language = {en},
	number = {1},
	urldate = {2021-04-15},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gregor, Jens and Fessler, Jeffrey A.},
	month = mar,
	year = {2015},
	pages = {44--55},
	file = {Gregor and Fessler - 2015 - Comparison of SIRT and SQS for Regularized Weighte.pdf:/home/david/Zotero/storage/PYU7WQGM/Gregor and Fessler - 2015 - Comparison of SIRT and SQS for Regularized Weighte.pdf:application/pdf},
}

@article{briand_theory_2018,
	title = {Theory and {Practice} of {Image} {B}-{Spline} {Interpolation}},
	volume = {8},
	url = {https://doi.org/10.5201%2Fipol.2018.221},
	doi = {10.5201/ipol.2018.221},
	journal = {Image Processing On Line},
	author = {Briand, Thibaud and Monasse, Pascal},
	month = jul,
	year = {2018},
	note = {Publisher: Image Processing On Line},
	pages = {99--141},
	file = {Briand and Monasse - 2018 - Theory and Practice of Image B-Spline Interpolatio.pdf:/home/david/Zotero/storage/R3AH248T/Briand and Monasse - 2018 - Theory and Practice of Image B-Spline Interpolatio.pdf:application/pdf},
}

@article{li_optimization_2018,
	title = {Optimization for {Blob}-{Based} {Image} {Reconstruction} {With} {Generalized} {Kaiser}–{Bessel} {Basis} {Functions}},
	volume = {4},
	url = {https://doi.org/10.1109%2Ftci.2018.2796302},
	doi = {10.1109/tci.2018.2796302},
	number = {2},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Li, Yusheng},
	month = jun,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	keywords = {Image reconstruction, blob, body-centered cubic (BCC) lattice, face-centered cubic (FCC) lattice, FCC, Image quality, Kaiser–Bessel radial basis function, lattice, Lattices, optimization, Optimization, simple cubic (SC) lattice, tomographic reconstruction, Tomography},
	pages = {257--270},
	file = {Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:/home/david/Zotero/storage/LK6X4YEC/Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:application/pdf;IEEE Xplore Full Text PDF:/home/david/Zotero/storage/GFPKWTXR/Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:application/pdf},
}

@article{nilchian_optimized_2015,
	title = {Optimized {Kaiser}–{Bessel} {Window} {Functions} for {Computed} {Tomography}},
	volume = {24},
	url = {https://doi.org/10.1109%2Ftip.2015.2451955},
	doi = {10.1109/tip.2015.2451955},
	number = {11},
	journal = {IEEE Transactions on Image Processing},
	author = {Nilchian, Masih and Ward, John Paul and Vonesch, Cedric and Unser, Michael},
	month = nov,
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {3826--3833},
	file = {Nilchian et al. - 2015 - Optimized Kaiser–Bessel Window Functions for Compu.pdf:/home/david/Zotero/storage/AGAU67MZ/Nilchian et al. - 2015 - Optimized Kaiser–Bessel Window Functions for Compu.pdf:application/pdf},
}

@article{pendrill_full_2021,
	title = {Full {Issue} {Download} {Vol}. 13 {No}. 1 2021 {The} {Importance} of the {Measurement} {Infrastructure} in {Economic} {Recovery} from the {COVID}-19 {Pandemic} {Richard} {J}. {C}. {Brown} , {Fiona} {Auty}, {Eugenio} {Renedo}, {Mike} {King} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.1 {Publisher} {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 18-21 {Abstract}: {This} paper describes the many, evidenced-based benefits to the economy of a well-developed measurement infrastructure. {In} particular, it explains how assuring confidence in measurement may be used to accelerate economic recovery from the {COVID}-19 pandemic including in emerging sectors such as the digital economy. {Recommendations} are made for providing near term support for national economic recovery whilst also demonstrating the advantages of sustained development of the measurement infrastructure in the medium-term to maximize the potential of future innovative and disruptive technologies. {These} recommendations, whilst focused on consideration of the {UK}, should apply globally. {References}: [1] {G}. {Tassey}, "{Underinvestment} in public good technologies," {J} {Technol}. {Transfer}, {Vol}. 30, pp. 89-113, 2004. https://doi.org/10.1007/s10961-004-4360-0 [2] {M}. {King}, and {E}. {Renedo}, "{Achieving} the 2.4\% {GDP} target: {The} role of measurement in increasing investment in {R}\&{D} and innovation," {NPL} {Report} {IEA} 3, {NPL}, {Teddington}, {UK}, {March} 2020. [3] {M}. {King} and {G}. {Tellett}, "{The} {National} {Measurement} {System}: {A} {Customer} {Survey} for {Three} of the {Core} {Labs} in the {National} {Measurement} {System}," {NMS} {Customer} {Survey} {Report} 2018, {NPL} {Teddington}, {UK}, {April} 2020 [4] {H}. {Kunzmann}, {T}. {Pfeifer}, {R}. {Schmitt}, {H}. {Schwenke}, and {A}.{Weckenmann}, "{Productive} metrology-adding value to manufacture," {CIRP} {Annals}, vol. 54, pp. 155-168, 2005. https://doi.org/10.1016/{S0007}-8506(07)60024-9 [5] {N}. {G}. {Orji}, {R}. {G}. {Dixson}, {A}. {Cordes}, {B}. {D}. {Bunday}, and {J}. {A}. {Allgair}, "{Measurement} traceability and quality assurance in a nanomanufacturing environment," {Instrumentation}, {Metrology}, and {Standards} for {Nanomanufacturing} {III}, {Proceedings} {Vol}. 7405, 740505, {August} 2009. https://doi.org/10.1117/12.826606 [6] {Belmana}, {Analysis} for {Policy} "{Public} {Support} for {Innovation} and {Business} {Outcomes}," {Belmana}: {London}, {UK}, 2020. [7] {R}. {Hawkins}, {Standards}, systems of innovation and policy in {Handbook} of {Innovation} and {Standards}. {Cheltenham}, {UK}: {Edward} {Elgar}, 2019. [8] {N}. {Nwaigbo}, and {M}. {King}, "{Evaluating} the {Impact} of the {NMS} {Consultancy} {Projects} on {Supported} {Firms} ({Working} {Paper})" {NPL}, {Teddington}, {UK}, 2020. [9] {M}. {King}, {R}. {Lambert}, and {P}. {Temple}, {Measurement}, standards and productivity spillovers in {Handbook} of {Innovation} and {Standards}. {Cheltenham}, {UK}: {Edward} {Elgar}, 2017, p. 162. https://doi.org/10.4337/9781783470082.00016 [10] {A}. {Font}, {K}. de {Hoogh}, {M}. {Leal}-{Sanchez}, {D}. {C}. {Ashworth}, {R}. {J}. {C}. {Brown}, {A}. {L}. {Hansell}, and {G}. {W}. {Fuller}, "{Using} metal ratios to detect emissions from municipal waste incinerators in ambient air pollution data," {Atmos}. {Environ}., vol. 113, pp. 177-186, {July} 2015. https://doi.org/10.1016/j.atmosenv.2015.05.002 [11] {S}. {Giannis}, {M}. {R}. {L}. {Gower}, {G}. {D}. {Sims}, {G}. {Pask}, and {G}. {Edwards}, "{Increasing} {UK} competitiveness by enhancing the composite materials regulatory infrastructure," {NPL} {Report} {MAT} 90, {NPL}, {Teddington}, {UK}, {October} 2019. [12] {HM} {Government}, {UK} {Research} and {Development} {Roadmap}, {BEIS}, {London}, {July} 2020. [13] {M}. {R}. {Mehra}, {S}. {S}. {Desai}, {F}. {Ruschitzka}, and {A}. {N}. {Patel}, "{Hydroxychloroquine} or chloroquine with or without a macrolide for treatment of {COVID}-19: a multinational registry analysis," {Lancet}, 2020, https://doi.org/10.1016/{S0140}-6736(20)31180-6 ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) ©2021 {NCSL} {International} {Smart} {Power} {Supply} {Calibration} {System} {Iraj} {Vasaeli} , {Brandon} {Umansky} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.2 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 22-27 {Abstract}: {This} paper details the development of an automated procedure to conduct calibrations of power supplies at {Jet} {Propulsion} {Laboratory}, {California} {Institute} of {Technology} ({JPL}). {The} fundamentals of power supply calibrations are given, and discussion on the method by which this custom software handles that calibration. {Additionally}, this technique provides real time uncertainty quantification of the calibrations. {This} automated system has demonstrated a time savings over existing automated techniques in use today. {References}: [1] {Keysight}, "{Low}-{Profile} {Modular} {Power} {System} {Series} {N6700} {Service} {Guide}", {Part} {Number}: 5969 2938, {Edition} 7, {January} 2015. [2] {B}. {N}. {Taylor} and {C}. {E}. {Kuyatt}, "{Guidelines} for {Evaluating} and {Expressing} the {Uncertainty} of {NIST} {Measurement} {Results}", {NIST} {Technical} {Note} 1297, 1994. https://doi.org/10.6028/{NIST}.{TN}.1297 [3] {JCGM}, "{Evaluation} of measurement data - {Guide} to the expression of uncertainty in measurement," first edition ({GUM} 1995 with minor corrections)," {JCGM} 100, 2008. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Computer} {Aided} {Verification} of {Voltage} {Dips} and {Short} {Interruption} {Generators} for {Electromagnetic} {Compatibility} {Immunity} {Test} in {Accordance} with {IEC} 61000-4-11: 2004 + {AMD}: 2017 {Hau} {Wah} {Lai} , {Cho} {Man} {Tsui} , {Hing} {Wah} {Li} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.3 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 28-39 {Abstract}: {This} paper describes a procedure and a computer-aided system developed by the {Standards} and {Calibration} {Laboratory} ({SCL}) for verification of voltage dip and short interruption generators in accordance with the international standard {IEC} 61000-4-11:2004+{AMD1}:2017. {The} verification is done by calibrating the specified parameters and comparing with the requirements stated in the standard. {The} parameters that should be calibrated are the ratios of the residual voltages to the rated voltage, the accuracy of the phase angle at switching, and the rise time, fall time, overshoot and undershoot of the switching waveform. {A} specially built adapter is used to convert the high voltage output waveforms of the generators to lower level signals to be acquired by a digital oscilloscope. {The} other circuits required for the testing are also provided. {In} addition, the paper discusses the uncertainty evaluations for the measured parameters. {References}: [1] {T}. {Williams}, and {K}. {Armstrong}, "{EMC} for {Systems} and {Installations} {Part} 6 - {Low}-{Frequency} {Magnetics} {Fields} ({Emissions} and {Immunity}) {Mains} {Dips}, {Dropouts}, {Interruptions}, {Sags}, {Brownouts} and {Swells}," {EMC} {Compliance} {Journal}, {August} 2000. [2] {M}.{I}. {Montrose}, and {E}. {M}. {Nakauchi}, {Testing} for {EMC} {Compliance}: {Approaches} and {Techniques}, {Wiley} {Interscience}, 2004. https://doi.org/10.1002/{047164465X} [3] {International} {Standard} {IEC} 61000-4-11:2004+{AMD1}:2017:{Electromagnetic} {Compatibility} ({EMC}) {Part} 4-11: {Testing} and measurement techniques - {Voltage} dips, short interruptions and voltage variations immunity tests. [4] {Evaluation} of measurement data - {Guide} to the expression of uncertainty in measurement, {First} {Edition} {JCGM} 100:2008. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Validation} of the {Photometric} {Method} {Used} for {Micropipette} {Calibration} {Elsa} {Batista} , {Isabel} {Godinho}, {George} {Rodrigues}, {Doreen} {Rumery} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.4 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 40-45 {Abstract}: {There} are two methods generally used for calibration of micropipettes: the gravimetric method described in {ISO} 8655-6:2002 and the photometric method described in {ISO} 8655-7:2005. {In} order to validate the photometric method, several micropipettes of different capacities from 0.1 µ{L} to 1000 µ{L} were calibrated using both methods (gravimetric and photometric) in two different laboratories, {IPQ} ({Portuguese} {Institute} for {Quality}) and {Artel}. {These} tests were performed by six different operators. {The} uncertainty for both methods was determined and it was verified that the uncertainty component that has a higher contribution to the final uncertainty budget depends on the volume delivered. {In} the photometric method for small volumes, the repeatability of the pipette is the largest uncertainty component, but for volumes, larger than 100 µ{L}, the photometric instrument is the most significant source of uncertainty. {Based} on all the results obtained with this study, one may consider the photometric method validated. {References}: [1] {ISO} 8655-1/2/6/7, {Piston}-operated volumetric apparatus, 2002. [2] {BIPM}, {International} {Vocabulary} of {Metrology}, 3rd edition, {JCGM} 200:2012. [3] {George} {Rodrigues}, {Bias} and transferability in standards methods of pipette calibration, {Artel}, {June} 2003. [4] {Taylor}, et.al. {The} definition of primary method of measurement ({PMM}) of the 'highest metrological quality': a challenge in understanding and communication, {Accred}. {Qual}.{Assur} (2001) 6:103-106. https://doi.org/10.1007/{PL00010444} [5] {EURAMET} project 1353, {Volume} comparison on {Calibration} of micropipettes - {Gravimetric} and photometric methods. [6] {ASTM} {E542}: {Standard} {Practice} for {Calibration} of laboratory {Volumetric} {Apparatus}, 2000. [7] {ISO} 4787; {Laboratory} glassware - {Volumetric} glassware - {Methods} for use and testing of capacity, 2010 . [8] {ISO} 13528:2005 - {Statistical} methods used in proficiency testing by interlaboratory comparisons. [9] {BIPM} et al, {Guide} to the {Expression} of {Uncertainty} in {Measurement} ({GUM}), 2nd ed., {International} {Organization} for {Standardization}, {Genève}, 1995. [10] {EURAMET} guide, cg 19, - {Guidelines} on the determination of uncertainty in gravimetric volume calibration, version 3.0, 2012. [11] {E}. {Batista} et all, {A} {Study} of {Factors} that {Influence} {Micropipette} {Calibrations}, {Measure} {Vol}. 10 {No}. 1, 2015 https://doi.org/10.1080/19315775.2015.11721717 [12] www.{BIPM}.org. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Material} {Flow} {Rate} {Estimation} in {Material} {Extrusion} {Additive} {Manufacturing} {G}. {P}. {Greeff} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.5 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 46-56 {Abstract}: {The} additive manufacturing of products promises exciting possibilities. {Measurement} methodologies, which measure an in-process dataset of these products and interpret the results, are essential. {However}, before developing such a level of quality assurance several in-process measurands must be realized. {One} of these is the material flow rate, or rate of adding material during the additive manufacturing process. {Yet}, measuring this rate directly in material extrusion additive manufacturing presents challenges. {This} work presents two indirect methods to estimate the volumetric flow rate at the liquefier exit in material extrusion, specifically in {Fused} {Deposition} {Modeling} or {Fused} {Filament} {Fabrication}. {The} methods are cost effective and may be applied in future sensor integration. {The} first method is an optical filament feed rate and width measurement and the second is based on the liquefier pressure. {Both} are used to indirectly estimate the volumetric flow rate. {The} work also includes a description of linking the {G}-code command to the final print result, which may be used to create a per extrusion command model of the part. {References}: [1] {T}. {Wohlers}, {I}. {Campbell}, {O}. {Diegel}, {J}. {Kowen}, {I}. {Fidan}, and {D}.{L}. {Bourell}, "{Wohlers} {Report} 2017: {3D} {Printing} and {Additive} {Manufacturing} {State} of the {Industry} {Annual} {Worldwide} {Progress} {Report}," 2017. [2] {Additive} manufacturing – {General} principles – {Terminology}. {Geneva}, {CH}: {International} {Organization} for {Standardization}, 2015. [3] {R}. {Jones} et al., "{Reprap} - {The} replicating rapid prototyper," {Robotica}, vol. 29, no. 1 {SPEC}. {ISSUE}, pp. 177-191, 2011, https://doi.org/10.1017/{S026357471000069X} [4] {T}. {Wohlers} and {T}. {Gornet}, "{History} of {Additive} {Manufacturing} 2017," 2017. [5] {S}. {A}. {M}. {Tofail}, {E}. {P}. {Koumoulos}, {A}. {Bandyopadhyay}, {S}. {Bose}, {L}. {O}'{Donoghue}, and {C}. {Charitidis}, "{Additive} manufacturing: scientific and technological challenges, market uptake and opportunities, "{Materials} {Today}, vol. 21, no. 1, pp. 22-37, {Jan}. 2018, https://doi.org/10.1016/j.mattod.2017.07.001 [6] {G}. {Moroni} and {S}. {Petrò}, "{Managing} uncertainty in the new manufacturing era," {Procedia} {CIRP}, vol. 75, pp. 1-2, 2018, https://doi.org/10.1016/j.procir.2018.07.001 [7] {R}. {Leach} et al., "{Information}-rich manufacturing metrology,"in {Eighth} {International} {Precision} {Assembly} {Seminar} ({IPAS}), 2018, no. {January}. https://doi.org/10.1007/978-3-030-05931-6\_14 [8] {S}. {Moylan}, {J}. {Slotwinski}, {A}. {Cooke}, {K}. {Jurrens}, {M}. {A}. {Donmez}, and {A}. {Donmez}, "{Proposal} for a {Standardized} {Test} {Artifact} for {Additive} {Manufacturing} {Machines} and {Processes}," {Solid} {Freeform} {Fabrication} {Symposium} {Proceedings}, pp. 902-920, 2012. https://doi.org/10.6028/{NIST}.{IR}.7858 [9] {ASME} {Y14}.46-2017 {Product} {Definition} for {Additive} {Manufacturing}. {New} {York}:{The} {American} {Society} of {Mechanical} {Engineers}, 2017. [10] {H}. {Li}, {T}. {Wang}, {J}. {Sun}, and {Z}. {Yu}, "{The} effect of process parameters in fused deposition modelling on bonding degree and mechanical properties," {Rapid} {Prototyping} {Journal}, vol. 24, no. 1, pp. 80-92, {Jan}. 2018, https://doi.org/10.1108/{RPJ}-06-2016-0090 [11] {A}. {W}. {Gebisa} and {H}. {G}. {Lemu}, "{Investigating} effects of {Fused}-deposition modeling ({FDM}) processing parameters on flexural properties of {ULTEM} 9085 using designed experiment, "{Materials}, vol.11, no. 4, pp. 1-23, 2018, https://doi.org/10.3390/ma11040500 {PMid}:29584674 {PMCid}:{PMC5951346} [12] {B}. {Wittbrodt} and {J}. {M}. {Pearce}, "{The} effects of {PLA} color on material properties of 3-{D} printed components," {Additive} {Manufacturing}, vol. 8, pp. 110-116, 2015, https://doi.org/10.1016/j.addma.2015.09.006 [13] {O}. {A}. {Mohamed}, {S}. {H}. {Masood}, and {J}. {L}. {Bhowmik}, "{Optimization} of fused deposition modeling process parameters: a review of current research and future prospects," {Advances} in {Manufacturing}, vol. 3, no. 1, pp. 42-53, {Mar}. 2015, https://doi.org/10.1007/s40436-014-0097-7 [14] {S}. {K}. {Everton}, {M}. {Hirsch}, {P}. {Stravroulakis}, {R}. {K}. {Leach} and {A}. {T}. {Clare}, "{Review} of in-situ process monitoring and in-situ metrology for metal additive manufacturing," {Materials} and {Design}, vol. 95, pp. 431-445, 2016, https://doi.org/10.1016/j.matdes.2016.01.099 [15] {P}. {K}. {Rao}, {J}. {P}. {Liu}, {D}. {Roberson}, {Z}. {J}. {Kong}, and {C}. {Williams},"{Online} {Real}-{Time} {Quality} {Monitoring} in {Additive} {Manufacturing} {Processes} {Using} {Heterogeneous} {Sensors}," {Journal} of {Manufacturing} {Science} and {Engineering}, vol. 137, no. 6, p.061007, {Sep}. 2015, https://doi.org/10.1115/1.4029823 [16] {J}. {Pellegrino}, {T}. {Makila}, {S}. {McQueen}, and {E}. {Taylor}, "{Measurement} science roadmap for polymer-based additive manufacturing," {Gaithersburg}, {MD}, {Dec}. 2016. https://doi.org/10.6028/{NIST}.{AMS}.100-5 [17] {T}. {R}. {Kramer}, {F}. {M}. {Proctor}, and {E}. {Messina}, "{The} {NIST} {RS274NGC} {Interpreter} -{Version} 3," {Gaithersburg}, {Maryland}, 2000. https://doi.org/10.6028/{NIST}.{IR}.6556 [18] {B}. {N}. {Turner}, {R}. {Strong}, and {S}. {A}. {Gold}, "{A} review of melt extrusion additive manufacturing processes: {I}. {Process} design and modeling," {Rapid} {Prototyping} {Journal}, vol. 20, no. 3, pp.192-204, {Apr}. 2014, https://doi.org/10.1108/{RPJ}-01-2013-0012 [19] {Conrad} {Electronic}, "{Renkforce} {RF1000} {3D} {Drucker}," 2016. https://www.conrad.de/de/renkforce-rf1000-3d-drucker-single-extruder-inkl-software-franzis-designcad-v24-3d-printrenkforce-edition-1007508.html (accessed {Sep}. 20, 2016). [20] {G}. {Hodgson}, {A}. {Ranellucci}, and {J}. {Moe}, "{Slic3r} {Manual} - {Flow} {Math}," 2016. http://manual.slic3r.org/advanced/flow-math (accessed {Jun}. 21, 2016). [21] {Repetier}, "{Repetier}-{Firmware} {Documentation}." https://www.repetier.com/documentation/repetier firmware/repetier-firmware-introduction/ (accessed {Apr}. 17, 2018). [22] {B}. {Weiss}, {D}. {W}. {Storti}, and {M}. {A}. {Ganter}, "{Low}-cost closedloop control of a {3D} printer gantry," {Rapid} {Prototyping} {Journal}, vol. 21, no. 5, pp. 482-490, {Aug}. 2015, https://doi.org/10.1108/{RPJ}-09-2014-0108 [23] {R}. {L}. {Zinniel} and {J}. {S}. {Batchelder}, "{Volumetric} {Feed} {Control} for {Flexible} {Filament}," {US} 6085957, 2000. [24] {W}. {J}. {Heij}, {Applied} {Metrology} in {Additive} {Manufacturing}. {Delft}: {Delft} {University} of {Technology}, 2016. [25] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Closed} loop control of slippage during filament transport in molten material extrusion," {Additive} {Manufacturing}, vol. 14, pp. 31-38, 2017, https://doi.org/10.1016/j.addma.2016.12.005 [26] {G}. {P}. {Greeff}, {Applied} {Metrology} in {Additive} {Manufacturing}, vol. 60. {Berlin}: {Mensch} und {Buch}, 2019. [27] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Comparing} {Retraction} {Methods} with {Volumetric} {Exit} {Flow} {Measurement} in {Molten} {Material} {Extrusion}," in {Special} {Interest} {Group} meeting on {Dimensional} {Accuracy} and {Surface} {Finish} in {Additive} {Manufacturing}, 2017, no. {October}, pp. 70-74. [28] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Single} print optimisation of fused filament fabrication parameters," {The} {International} {Journal} of {Advanced} {Manufacturing} {Technology}, {Aug}. 2018, https://doi.org/10.1007/s00170-018-2518-4 [29] {A}. {Bellini}, {S}. {Güçeri}, and {M}. {Bertoldi}, "{Liquefier} {Dynamics} in {Fused} {Deposition}," {Journal} of {Manufacturing} {Science} and {Engineering}, vol. 126, no. 2, p. 237, 2004, https://doi.org/10.1115/1.1688377 [30] {P}. {Virtanen} et al., "{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}," {Nature} {Methods}, vol. 17, no. 3, pp. 261-272, {Mar}. 2020, https://doi.org/10.1038/s41592-019-0686-2 {PMid}:32015543 {PMCid}:{PMC7056644} ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Software} to {Maximize} {End}-{User} {Uptake} of {Conformity} {Assessment} with {Measurement} {Uncertainty}, {Including} {Bivariate} {Cases}. {The} {European} {EMPIR} {CASoft} {Project}},
	volume = {13},
	url = {http://dx.doi.org/10.51843/measure.13.1.6},
	doi = {10.51843/measure.13.1.6},
	number = {1},
	journal = {NCSL International measure},
	author = {Pendrill, L.R. and Allard, A. and Fischer, N. and Harris, P.M. and Nguyen, J. and Smith, I.M.},
	month = jan,
	year = {2021},
	note = {Publisher: NCSL International},
	pages = {58--69},
	file = {Pendrill et al. - 2021 - Full Issue Download Vol. 13 No. 1 2021 The Importa.pdf:/home/david/Zotero/storage/NP73N643/Pendrill et al. - 2021 - Full Issue Download Vol. 13 No. 1 2021 The Importa.pdf:application/pdf},
}

@article{unser_fast_1991,
	title = {Fast {B}-spline transforms for continuous image representation and interpolation},
	volume = {13},
	url = {https://doi.org/10.1109%2F34.75515},
	doi = {10.1109/34.75515},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	month = mar,
	year = {1991},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {277--285},
	file = {Unser et al. - 1991 - Fast B-spline transforms for continuous image repr.pdf:/home/david/Zotero/storage/23CG7G43/Unser et al. - 1991 - Fast B-spline transforms for continuous image repr.pdf:application/pdf},
}

@article{unser_b-spline_1993,
	title = {B-spline signal processing. {II}. {Efficiency} design and applications},
	volume = {41},
	url = {http://dx.doi.org/10.1109/78.193221},
	doi = {10.1109/78.193221},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	year = {1993},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {834--848},
	file = {Unser et al. - 1993 - B-spline signal processing. II. Efficiency design .pdf:/home/david/Zotero/storage/J3RL864C/Unser et al. - 1993 - B-spline signal processing. II. Efficiency design .pdf:application/pdf},
}

@article{unser_b-spline_1993-1,
	title = {B-spline signal processing. {I}. {Theory}},
	volume = {41},
	url = {http://dx.doi.org/10.1109/78.193220},
	doi = {10.1109/78.193220},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	year = {1993},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {821--833},
	file = {Unser et al. - 1993 - B-spline signal processing. I. Theory.pdf:/home/david/Zotero/storage/YJN9BZ62/Unser et al. - 1993 - B-spline signal processing. I. Theory.pdf:application/pdf},
}

@book{stepanov_elements_2019,
	address = {Palo Alto Mountain View},
	title = {Elements of programming},
	isbn = {978-0-578-22214-1},
	language = {en},
	publisher = {Semigroups Press},
	author = {Stepanov, Alexander A. and McJones, Paul},
	year = {2019},
	file = {Stepanov and McJones - 2019 - Elements of programming.pdf:/home/david/Zotero/storage/GKI82GL3/Stepanov and McJones - 2019 - Elements of programming.pdf:application/pdf},
}

@article{kogel_fast_2011,
	title = {A {Fast} {Gradient} method for embedded linear predictive control},
	volume = {44},
	issn = {14746670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474667016437997},
	doi = {10.3182/20110828-6-IT-1002.03322},
	abstract = {This work considers the fast solution of model predictive control problems for linear systems with input constraints and a quadratic cost criterion. If the resulting optimization problem arising from the model predictive control is solved online using the Fast Gradient method one needs to determine the gradient of the cost function. We propose a method, tailored for embedded control purposes, that eﬃciently calculates the gradient taking the underlying structure of the system into account. Moreover, we discuss how the stability of the plant inﬂuences the required number of iterations to obtain a solution within a prescribed accuracy.},
	language = {en},
	number = {1},
	urldate = {2021-12-12},
	journal = {IFAC Proceedings Volumes},
	author = {Kögel, Markus and Findeisen, Rolf},
	month = jan,
	year = {2011},
	pages = {1362--1367},
	file = {Kögel and Findeisen - 2011 - A Fast Gradient method for embedded linear predict.pdf:/home/david/Zotero/storage/3QNSJV36/Kögel and Findeisen - 2011 - A Fast Gradient method for embedded linear predict.pdf:application/pdf},
}

@book{nesterov_introductory_2004,
	address = {Boston, MA},
	series = {Applied {Optimization}},
	title = {Introductory {Lectures} on {Convex} {Optimization}},
	volume = {87},
	isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
	url = {http://link.springer.com/10.1007/978-1-4419-8853-9},
	language = {en},
	urldate = {2021-12-12},
	publisher = {Springer US},
	author = {Nesterov, Yurii},
	editor = {Pardalos, Panos M. and Hearn, Donald W.},
	year = {2004},
	doi = {10.1007/978-1-4419-8853-9},
	file = {Nesterov - 2004 - Introductory Lectures on Convex Optimization.pdf:/home/david/Zotero/storage/AA9LNYKW/Nesterov - 2004 - Introductory Lectures on Convex Optimization.pdf:application/pdf},
}

@article{xu_investigation_2012,
	title = {Investigation of discrete imaging models and iterative image reconstruction in differential {X}-ray phase-contrast tomography},
	volume = {20},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-20-10-10724},
	doi = {10.1364/OE.20.010724},
	abstract = {Differential X-ray phase-contrast tomography (DPCT) refers to a class of promising methods for reconstructing the X-ray refractive index distribution of materials that present weak X-ray absorption contrast. The tomographic projection data in DPCT, from which an estimate of the refractive index distribution is reconstructed, correspond to onedimensional (1D) derivatives of the two-dimensional (2D) Radon transform of the refractive index distribution. There is an important need for the development of iterative image reconstruction methods for DPCT that can yield useful images from few-view projection data, thereby mitigating the long data-acquisition times and large radiation doses associated with use of analytic reconstruction methods. In this work, we analyze the numerical and statistical properties of two classes of discrete imaging models that form the basis for iterative image reconstruction in DPCT. We also investigate the use of one of the models with a modern image reconstruction algorithm for performing few-view image reconstruction of a tissue specimen.},
	language = {en},
	number = {10},
	urldate = {2021-12-13},
	journal = {Optics Express},
	author = {Xu, Qiaofeng and Sidky, Emil Y. and Pan, Xiaochuan and Stampanoni, Marco and Modregger, Peter and Anastasio, Mark A.},
	month = may,
	year = {2012},
	pages = {10724},
	file = {Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:/home/david/Zotero/storage/E4F9TVK6/Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:application/pdf},
}

@misc{noauthor_why_nodate,
	title = {why is {BSpline}() only working for regular grids? · {Issue} \#131 · {JuliaMath}/{Interpolations}.jl},
	shorttitle = {why is {BSpline}() only working for regular grids?},
	url = {https://github.com/JuliaMath/Interpolations.jl/issues/131},
	abstract = {I was wondering how hard it would be to support irregular grids for BSpline interpolation. I am aware that you have Gridded for this case, however, I often end up missing features of Gridded which ...},
	language = {en},
	urldate = {2021-12-13},
	journal = {GitHub},
	file = {Snapshot:/home/david/Zotero/storage/M6KNQ676/Interpolations.html:text/html},
}

@article{thevenaz_interpolation_2000,
	title = {Interpolation revisited [medical images application]},
	volume = {19},
	issn = {1558-254X},
	doi = {10.1109/42.875199},
	abstract = {Based on the theory of approximation, this paper presents a unified analysis of interpolation and resampling techniques. An important issue is the choice of adequate basis functions. The authors show that, contrary to the common belief, those that perform best are not interpolating. By opposition to traditional interpolation, the authors call their use generalized interpolation; they involve a prefiltering step when correctly applied. The authors explain why the approximation order inherent in any basis function is important to limit interpolation artifacts. The decomposition theorem states that any basis function endowed with approximation order ran be expressed as the convolution of a B spline of the same order with another function that has none. This motivates the use of splines and spline-based functions as a tunable way to keep artifacts in check without any significant cost penalty. The authors discuss implementation and performance issues, and they provide experimental evidence to support their claims.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Thevenaz, P. and Blu, T. and Unser, M.},
	month = jul,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Biomedical imaging, Convolution, Cost function, Heart, Interpolation, Kernel, Performance analysis, Sampling methods, Spline},
	pages = {739--758},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/9VM4R4YE/Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/9VWGRF2K/875199.html:text/html;Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:/home/david/Zotero/storage/5I9E2DQJ/Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:application/pdf},
}

@misc{noauthor_spline_nodate,
	title = {Spline {Interpolation}},
	url = {http://bigwww.epfl.ch/thevenaz/interpolation/},
	urldate = {2021-12-13},
	file = {Spline Interpolation:/home/david/Zotero/storage/PWU9L84S/interpolation.html:text/html},
}

@article{ratnani_b-splines_nodate,
	title = {B-{Splines} and {IsoGeometric} {Analysis}},
	language = {en},
	author = {Ratnani, A and Sonnendrücker, E},
	pages = {15},
	file = {Ratnani and Sonnendrücker - B-Splines and IsoGeometric Analysis.pdf:/home/david/Zotero/storage/JQVDYASZ/Ratnani and Sonnendrücker - B-Splines and IsoGeometric Analysis.pdf:application/pdf},
}

@article{mccann_fast_2016,
	title = {Fast {3D} reconstruction method for differential phase contrast {X}-ray {CT}},
	volume = {24},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/abstract.cfm?URI=oe-24-13-14564},
	doi = {10.1364/OE.24.014564},
	abstract = {We present a fast algorithm for fully 3D regularized X-ray tomography reconstruction for both traditional and differential phase contrast measurements. In many applications, it is critical to reduce the X-ray dose while producing high-quality reconstructions. Regularization is an excellent way to do this, but in the differential phase contrast case it is usually applied in a slice-by-slice manner. We propose using fully 3D regularization to improve the dose/quality trade-off beyond what is possible using slice-by-slice regularization. To make this computationally feasible, we show that the two computational bottlenecks of our iterative optimization process can be expressed as discrete convolutions; the resulting algorithms for computation of the X-ray adjoint and normal operator are fast and simple alternatives to regridding. We validate this algorithm on an analytical phantom as well as conventional CT and differential phase contrast measurements from two real objects. Compared to the slice-by-slice approach, our algorithm provides a more accurate reconstruction of the analytical phantom and better qualitative appearance on one of the two real datasets.},
	language = {en},
	number = {13},
	urldate = {2021-12-13},
	journal = {Optics Express},
	author = {McCann, Michael T. and Nilchian, Masih and Stampanoni, Marco and Unser, Michael},
	month = jun,
	year = {2016},
	pages = {14564},
	file = {McCann et al. - 2016 - Fast 3D reconstruction method for differential pha.pdf:/home/david/Zotero/storage/BMGS9ZWI/McCann et al. - 2016 - Fast 3D reconstruction method for differential pha.pdf:application/pdf},
}

@article{sorzano_xmipp_2004,
	title = {{XMIPP}: a new generation of an open-source image processing package for electron microscopy},
	volume = {148},
	issn = {10478477},
	shorttitle = {{XMIPP}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047847704001261},
	doi = {10.1016/j.jsb.2004.06.006},
	abstract = {X-windows based microscopy image processing package (Xmipp) is a specialized suit of image processing programs, primarily aimed at obtaining the 3D reconstruction of biological specimens from large sets of projection images acquired by transmission electron microscopy. This public-domain software package was introduced to the electron microscopy ﬁeld eight years ago, and since then it has changed drastically. New methodologies for the analysis of single-particle projection images have been added to classiﬁcation, contrast transfer function correction, angular assignment, 3D reconstruction, reconstruction of crystals, etc. In addition, the package has been extended with functionalities for 2D crystal and electron tomography data. Furthermore, its current implementation in C++, with a highly modular design of well-documented data structures and functions, oﬀers a convenient environment for the development of novel algorithms. In this paper, we present a general overview of a new generation of Xmipp that has been re-engineered to maximize ﬂexibility and modularity, potentially facilitating its integration in future standardization eﬀorts in the ﬁeld. Moreover, by focusing on those developments that distinguish Xmipp from other packages available, we illustrate its added value to the electron microscopy community.},
	language = {en},
	number = {2},
	urldate = {2021-12-13},
	journal = {Journal of Structural Biology},
	author = {Sorzano, C.O.S. and Marabini, R. and Velázquez-Muriel, J. and Bilbao-Castro, J.R. and Scheres, S.H.W. and Carazo, J.M. and Pascual-Montano, A.},
	month = nov,
	year = {2004},
	pages = {194--204},
	file = {Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:/home/david/Zotero/storage/4WWCHNQ6/Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:application/pdf;Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:/home/david/Zotero/storage/48JAP5NK/Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:application/pdf},
}

@misc{noauthor_xmipp_2021,
	title = {Xmipp},
	copyright = {GPL-3.0},
	url = {https://github.com/I2PC/xmipp/blob/81681048bdc75911293b78fcb50c77c037688ba1/src/xmipp/libraries/data/blobs.cpp},
	abstract = {Xmipp is a suite of image processing programs, primarily aimed at single-particle 3D electron microscopy.},
	urldate = {2021-12-13},
	publisher = {Instruct Image Processing Center},
	month = dec,
	year = {2021},
	note = {original-date: 2018-06-11T13:30:44Z},
}

@incollection{herman_computerized_2015,
	title = {Computerized {Tomography} {Reconstruction} {Methods}},
	isbn = {978-0-12-397316-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123970251002864},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Brain {Mapping}},
	publisher = {Elsevier},
	author = {Herman, G.T.},
	year = {2015},
	doi = {10.1016/B978-0-12-397025-1.00286-4},
	pages = {203--208},
	file = {Herman - 2015 - Computerized Tomography Reconstruction Methods.pdf:/home/david/Zotero/storage/SCFNNYII/Herman - 2015 - Computerized Tomography Reconstruction Methods.pdf:application/pdf},
}

@book{levakhina_three-dimensional_2014,
	address = {Wiesbaden},
	title = {Three-{Dimensional} {Digital} {Tomosynthesis}: {Iterative} {Reconstruction}, {Artifact} {Reduction} and {Alternative} {Acquisition} {Geometry}},
	isbn = {978-3-658-05696-4 978-3-658-05697-1},
	shorttitle = {Three-{Dimensional} {Digital} {Tomosynthesis}},
	url = {http://link.springer.com/10.1007/978-3-658-05697-1},
	language = {en},
	urldate = {2021-12-13},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Levakhina, Yulia},
	year = {2014},
	doi = {10.1007/978-3-658-05697-1},
	file = {Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/TBFGYC39/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf;Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/AWN9BCTH/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf},
}

@article{schmitt_analysis_nodate,
	title = {Analysis of bias induced by various forward projection models in iterative reconstruction},
	abstract = {Discrete representation of the CT image is a major step in the design of iterative reconstruction algorithms, particularly because the decision being made at this level affects both bias and noise properties of the reconstruction, in addition to choices made later in the algorithm design. In this work, we examine the bias induced by popular image representation models, namely Joseph’s method and the basis function approach relying on B-splines and blobs. Our preliminary results highlight a common weakness in terms of overshoot and undershoot artifacts at sharp boundaries. They also show that the Blobs may perform only as well as the B-spline of order two in terms of bias, and that Joseph’s method tends to produce results that are fairly comparable to the B-spline of order one, with a slight advantage in favor of the latter.},
	language = {en},
	author = {Schmitt, K and Schondube, H and Stierstorfer, K and Hornegger, J and Noo, F},
	pages = {5},
	file = {Schmitt et al. - Analysis of bias induced by various forward projec.pdf:/home/david/Zotero/storage/HBHTAIXL/Schmitt et al. - Analysis of bias induced by various forward projec.pdf:application/pdf},
}

@misc{noauthor_kaiser_nodate,
	title = {Kaiser window approximation},
	url = {https://dsp.stackexchange.com/questions/37714/kaiser-window-approximation},
	urldate = {2021-12-13},
	journal = {Signal Processing Stack Exchange},
}

@incollection{govil_fast_2006,
	title = {A {Fast} {Algorithm} for {Spherical} {Basis} {Approximation}},
	volume = {282},
	isbn = {978-1-58488-636-5 978-1-4200-1138-8},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420011388.ch13},
	abstract = {Radial basis functions appear in a wide ﬁeld of applications in numerical mathematics and computer science. We present a fast algorithm for scattered data interpolation and approximation on the sphere with spherical radial basis functions of diﬀerent spatial density. We discuss three settings, each leading to a special structure of the interpolation matrix allowing for an eﬃcient implementation using discrete Fourier transforms. A numerical example is given to show the advantages of spherical radial basis functions with diﬀerent spatial densities.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Frontiers in {Interpolation} and {Approximation}},
	publisher = {Chapman and Hall/CRC},
	author = {Keiner, J and Prestin, J},
	collaborator = {Govil, N and Mhaskar, H and Mohapatra, Ram and Nashed, Zuhair and Szabados, J},
	month = jul,
	year = {2006},
	doi = {10.1201/9781420011388.ch13},
	note = {Series Title: Pure and Applied Mathematics},
	pages = {259--286},
	file = {Keiner and Prestin - 2006 - A Fast Algorithm for Spherical Basis Approximation.pdf:/home/david/Zotero/storage/QBMDC7UF/Keiner and Prestin - 2006 - A Fast Algorithm for Spherical Basis Approximation.pdf:application/pdf},
}

@inproceedings{levakhina_distance-driven_2010,
	address = {Knoxville, TN},
	title = {Distance-driven projection and backprojection for spherically symmetric basis functions in {CT}},
	isbn = {978-1-4244-9106-3},
	url = {http://ieeexplore.ieee.org/document/5874325/},
	doi = {10.1109/NSSMIC.2010.5874325},
	abstract = {Forward- and backprojecton pair plays an important role in computed tomography (CT). Since they are used in clinical routine for ﬁltered backprojection (FBP) reconstruction, in iterative reconstruction methods, for artifact correction and simulation purposes, they have to be fast, accurate and memory efﬁcient. Recently, a distance-driven approach for pixel basis functions has been proposed. At the moment, it is a stateof-the-art method that is almost artifact free, fast and has a predictable memory pattern access. In the work presented here, the distance-driven approach for the two-dimensional case is extended for spherically symmetric Kaiser-Bessel basis functions. Usage of these basis functions allows for constructing a smooth and continuous function for image representation.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {{IEEE} {Nuclear} {Science} {Symposuim} \& {Medical} {Imaging} {Conference}},
	publisher = {IEEE},
	author = {Levakhina, Y and Buzug, T M},
	month = oct,
	year = {2010},
	pages = {2894--2897},
	file = {Levakhina and Buzug - 2010 - Distance-driven projection and backprojection for .pdf:/home/david/Zotero/storage/7QQKY6AS/Levakhina and Buzug - 2010 - Distance-driven projection and backprojection for .pdf:application/pdf},
}

@article{levakhina_weighted_2013,
	title = {Weighted simultaneous algebraic reconstruction technique for tomosynthesis imaging of objects with high-attenuation features: \textbf{ \textit{ω} } {SART} for tomosynthesis of objects with high-attenuation features},
	volume = {40},
	issn = {00942405},
	shorttitle = {Weighted simultaneous algebraic reconstruction technique for tomosynthesis imaging of objects with high-attenuation features},
	url = {http://doi.wiley.com/10.1118/1.4789592},
	doi = {10.1118/1.4789592},
	abstract = {Purpose: This paper introduces a nonlinear weighting scheme into the backprojection operation within the simultaneous algebraic reconstruction technique (SART). It is designed for tomosynthesis imaging of objects with high-attenuation features in order to reduce limited angle artifacts.
Methods: The algorithm estimates which projections potentially produce artifacts in a voxel. The contribution of those projections into the updating term is reduced. In order to identify those projections automatically, a four-dimensional backprojected space representation is used. Weighting coefﬁcients are calculated based on a dissimilarity measure, evaluated in this space. For each combination of an angular view direction and a voxel position an individual weighting coefﬁcient for the updating term is calculated.
Results: The feasibility of the proposed approach is shown based on reconstructions of the following real three-dimensional tomosynthesis datasets: a mammography quality phantom, an apple with metal needles, a dried ﬁnger bone in water, and a human hand. Datasets have been acquired with a Siemens Mammomat Inspiration tomosynthesis device and reconstructed using SART with and without suggested weighting. Out-of-focus artifacts are described using line proﬁles and measured using standard deviation (STD) in the plane and below the plane which contains artifact-causing features. Artifacts distribution in axial direction is measured using an artifact spread function (ASF). The volumes reconstructed with the weighting scheme demonstrate the reduction of out-of-focus artifacts, lower STD (meaning reduction of artifacts), and narrower ASF compared to nonweighted SART reconstruction. It is achieved successfully for different kinds of structures: point-like structures such as phantom features, long structures such as metal needles, and ﬁne structures such as trabecular bone structures.
Conclusions: Results indicate the feasibility of the proposed algorithm to reduce typical tomosynthesis artifacts produced by high-attenuation features. The proposed algorithm assigns weighting coefﬁcients automatically and no segmentation or tissue-classiﬁcation steps are required. The algorithm can be included into various iterative reconstruction algorithms with an additive updating strategy. It can also be extended to computed tomography case with the complete set of angular data. © 2013 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4789592]},
	language = {en},
	number = {3},
	urldate = {2021-12-13},
	journal = {Medical Physics},
	author = {Levakhina, Y. M. and Müller, J. and Duschka, R. L. and Vogt, F. and Barkhausen, J. and Buzug, T. M.},
	month = feb,
	year = {2013},
	pages = {031106},
	file = {Levakhina et al. - 2013 - Weighted simultaneous algebraic reconstruction tec.pdf:/home/david/Zotero/storage/FFCW9GUQ/Levakhina et al. - 2013 - Weighted simultaneous algebraic reconstruction tec.pdf:application/pdf},
}

@inproceedings{levakhina_two-step_2010,
	title = {Two-step metal artifact reduction using {2D}-{NFFT} and spherically symmetric basis functions},
	doi = {10.1109/NSSMIC.2010.5874424},
	abstract = {In computed tomography metal objects lead to inconsistencies in the acquired data which result in image artifacts after reconstruction. To enhance the image quality and allow for a more accurate diagnosis, a metal artifact reduction is required. Approaches may base on a recomputation of metal influenced values or use an alternative image reconstruction technique, which is less sensitive to inconsistent raw data than the standard filtered backprojection. Here, a two-step algorithm for metal artifact reduction is proposed, which combines a 2D NFFT-based interpolation and an MLEM reconstruction with spherically symmetric basis functions (blobs). Reconstructed images have been evaluated visually and quantitatively. Experiments show that combination of a 2D NFFT-based interpolation and blobs offers an effective strategy for enhanced metal artifacts suppression.},
	booktitle = {{IEEE} {Nuclear} {Science} {Symposuim} {Medical} {Imaging} {Conference}},
	author = {Levakhina, Yulia and Kratz, Baerbel and Buzug, Thorsten M.},
	month = oct,
	year = {2010},
	note = {ISSN: 1082-3654},
	keywords = {Biomedical imaging, Interpolation, Computed tomography, Image reconstruction, Pixel, Image edge detection, Metals},
	pages = {3343--3345},
	file = {Levakhina et al. - 2010 - Two-step metal artifact reduction using 2D-NFFT an.pdf:/home/david/Zotero/storage/34UPBSU5/Levakhina et al. - 2010 - Two-step metal artifact reduction using 2D-NFFT an.pdf:application/pdf},
}

@misc{noauthor_approximation_nodate,
	title = {Approximation of functions},
	url = {http://hplgit.github.io/INF5620/doc/pub/sphinx-fem/._main_fem002.html},
	urldate = {2021-12-13},
	file = {Approximation of functions.html:/home/david/Zotero/storage/JW3A2UCB/Approximation of functions.html:text/html},
}

@book{buzug_computed_2008,
	address = {Berlin, Heidelberg},
	edition = {1},
	title = {Computed {Tomography}: {From} {Photon} {Statistics} to {Modern} {Cone}-{Beam} {CT}},
	isbn = {978-3-540-39407-5 978-3-540-39408-2},
	url = {http://link.springer.com/10.1007/978-3-540-39408-2},
	abstract = {Tis book provides an overview of X-ray technology, the historic developmental milestones of modern CT systems, and gives a comprehensive insight into the main reconstruction methods used in computed tomography. Te basis of reconstr- tion is, undoubtedly, mathematics. However, the beauty of computed tomography cannot be understood without a detailed knowledge of X-ray generation, photon– matter interaction, X-ray detection, photon statistics, as well as fundamental signal processing concepts and dedicated measurement systems. Terefore, the reader will ?nd a number of references to these basic disciplines together with a brief introd- tion to the underlying principles of CT. Tis book is structured to cover the basics of CT: from photon statistics to m- ern cone-beam systems. However, the main focus of the book is concerned with - tailed derivations of reconstruction algorithms in ?D and modern ?D cone-beam systems. A thorough analysis of CT artifacts and a discussion of practical issues, such as dose considerations, provide further insight into modern CT systems. While mainly written for graduate students in biomedical engineering, medical engine- ing science, medical physics, medicine (radiology), mathematics, electrical eng- eering, and physics, experienced practitioners in these ?elds will bene?t from this book as well.},
	language = {en},
	urldate = {2021-12-13},
	publisher = {Springer Berlin Heidelberg},
	author = {Buzug, Thorsten M.},
	year = {2008},
	doi = {10.1007/978-3-540-39408-2},
	file = {Buzug - 2008 - Computed Tomography From Photon Statistics to Mod.pdf:/home/david/Zotero/storage/XQ3F48P9/Buzug - 2008 - Computed Tomography From Photon Statistics to Mod.pdf:application/pdf},
}

@misc{noauthor_toast_2021,
	title = {Toast++ - {Image} {Reconstruction} in {Optical} {Tomography}},
	copyright = {GPL-3.0},
	url = {https://github.com/toastpp/toastpp},
	abstract = {Toast++: Forward and inverse modelling in optical tomography},
	urldate = {2021-12-13},
	publisher = {Toast++},
	month = dec,
	year = {2021},
	note = {original-date: 2016-07-07T14:30:16Z},
}

@article{vrcej_efficient_2001,
	title = {Efficient implementation of all-digital interpolation},
	volume = {10},
	issn = {10577149},
	url = {http://ieeexplore.ieee.org/document/967392/},
	doi = {10.1109/83.967392},
	abstract = {B-splines are commonly used for continuous representation of discrete time signals. This kind of representation proves to be very useful in applications such as image interpolation, rotation and edge detection. In all these applications, the ﬁrst step is to compute the B-spline coeﬃcients of the signal, and this involves the use of an IIR noncausal ﬁlter called the direct B-spline ﬁlter. The signal reconstruction is achieved using the indirect B-spline ﬁlter, which in many applications operates at a higher rate.},
	language = {en},
	number = {11},
	urldate = {2021-12-14},
	journal = {IEEE Transactions on Image Processing},
	author = {Vrcej, B. and Vaidyanathan, P.P.},
	month = nov,
	year = {2001},
	pages = {1639--1646},
	file = {Vrcej and Vaidyanathan - 2001 - Efficient implementation of all-digital interpolat.pdf:/home/david/Zotero/storage/6RCG935B/Vrcej and Vaidyanathan - 2001 - Efficient implementation of all-digital interpolat.pdf:application/pdf},
}

@article{trampert_spherically_2017,
	title = {Spherically symmetric volume elements as basis functions for image reconstructions in computed laminography},
	abstract = {BACKGROUND: Laminography is a tomographic technique that allows three-dimensional imaging of ﬂat, elongated objects that stretch beyond the extent of a reconstruction volume. Laminography datasets can be reconstructed using iterative algorithms based on the Kaczmarz method.
OBJECTIVE: The goal of this study is to develop a reconstruction algorithm that provides superior reconstruction quality for a challenging class of problems.
METHODS: Images are represented in computer memory using coefﬁcients over basis functions, typically piecewise constant functions (voxels). By replacing voxels with spherically symmetric volume elements (blobs) based on generalized KaiserBessel window functions, we obtained an adapted version of the algebraic reconstruction technique.
RESULTS: Band-limiting properties of blob functions are beneﬁcial particular in the case of noisy projections and if only a limited number of projections is available. In this case, using blob basis functions improved the full-width-at-half-maximum resolution from 10.2 ± 1.0 to 9.9 ± 0.9 (p value = 2.3·10−4). For the same dataset, the signal-to-noise ratio improved from 16.1 to 31.0. The increased computational demand per iteration is compensated for by a faster convergence rate, such that the overall performance is approximately identical for blobs and voxels.
CONCLUSIONS: Despite the higher complexity, tomographic reconstruction from computed laminography data should be implemented using blob basis functions, especially if noisy data is expected.},
	language = {en},
	author = {Trampert, Patrick and Vogelgesang, Jonas and Schorr, Christian and Maisl, Michael and Bogachev, Sviatoslav and Marniok, Nico and Louis, Alfred and Dahmen, Tim and Slusallek, Philipp},
	year = {2017},
	pages = {14},
	file = {Trampert et al. - Spherically symmetric volume elements as basis fun.pdf:/home/david/Zotero/storage/LRF6XNWR/Trampert et al. - Spherically symmetric volume elements as basis fun.pdf:application/pdf},
}

@book{prautzsch_bezier_2002,
	address = {Berlin, Heidelberg},
	series = {Mathematics and {Visualization}},
	title = {Bézier and {B}-{Spline} {Techniques}},
	isbn = {978-3-642-07842-2 978-3-662-04919-8},
	url = {http://link.springer.com/10.1007/978-3-662-04919-8},
	language = {en},
	urldate = {2021-12-14},
	publisher = {Springer Berlin Heidelberg},
	author = {Prautzsch, Hartmut and Boehm, Wolfgang and Paluszny, Marco},
	editor = {Farin, Gerald and Hege, Hans-Christian and Hoffman, David and Johnson, Christopher R. and Polthier, Konrad},
	year = {2002},
	doi = {10.1007/978-3-662-04919-8},
	file = {Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:/home/david/Zotero/storage/ZKMQQT7S/Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:application/pdf;Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:/home/david/Zotero/storage/G56H24QZ/Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:application/pdf},
}

@article{bilbaocastro_exploiting_2009,
	title = {Exploiting desktop supercomputing for three-dimensional electron microscopy reconstructions using {ART} with blobs},
	volume = {165},
	issn = {10478477},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047847708002323},
	doi = {10.1016/j.jsb.2008.09.009},
	abstract = {Three-dimensional electron microscopy allows direct visualization of biological macromolecules close to their native state. The high impact of this technique in the structural biology ﬁeld is highly correlated with the development of new image processing algorithms. In order to achieve subnanometer resolution, the size and number of images involved in a three-dimensional reconstruction increase and so do computer requirements. New chips integrating multiple processors are hitting the market at a reduced cost. This high-integration, low-cost trend has just begun and is expected to bring real supercomputers to our laboratory desktops in the coming years. This paper proposes a parallel implementation of a computation-intensive algorithm for three-dimensional reconstruction, ART, that takes advantage of the computational power in modern multicore platforms. ART is a sophisticated iterative reconstruction algorithm that has turned out to be well suited for the conditions found in threedimensional electron microscopy. In view of the performance obtained in this work, these modern platforms are expected to play an important role to face the future challenges in three-dimensional electron microscopy.},
	language = {en},
	number = {1},
	urldate = {2021-12-14},
	journal = {Journal of Structural Biology},
	author = {Bilbaocastro, J and Marabini, R and Sorzano, C and Garcia, I and Carazo, J and Fernandez, J},
	month = jan,
	year = {2009},
	pages = {19--26},
	file = {Bilbaocastro et al. - 2009 - Exploiting desktop supercomputing for three-dimens.pdf:/home/david/Zotero/storage/25ZICI85/Bilbaocastro et al. - 2009 - Exploiting desktop supercomputing for three-dimens.pdf:application/pdf},
}

@article{eberly_b-spline_nodate,
	title = {B-{Spline} {Interpolation} on {Lattices}},
	language = {en},
	author = {Eberly, David},
	pages = {41},
	file = {Eberly - B-Spline Interpolation on Lattices.pdf:/home/david/Zotero/storage/7ETA2HKS/Eberly - B-Spline Interpolation on Lattices.pdf:application/pdf},
}

@article{prochazkova_derivative_nodate,
	title = {{DERIVATIVE} {OF} {B}-{SPLINE} {FUNCTION}},
	abstract = {Derivatives are very important in computation in engineering practice on graphics structures. B-spline functions are deﬁned recursive, so direct computation is very diﬃcult. In this article is shown the proof of formula for simpler direct computation of derivatives and its application for derivatives of NURBS curves.},
	language = {en},
	author = {Prochazkova, Jana},
	pages = {6},
	file = {Prochazkova - DERIVATIVE OF B-SPLINE FUNCTION.pdf:/home/david/Zotero/storage/H86WX7IR/Prochazkova - DERIVATIVE OF B-SPLINE FUNCTION.pdf:application/pdf},
}

@article{censor_developments_2021,
	title = {Developments in {Mathematical} {Algorithms} and {Computational} {Tools} for {Proton} {CT} and {Particle} {Therapy} {Treatment} {Planning}},
	issn = {2469-7303},
	doi = {10.1109/TRPMS.2021.3107322},
	abstract = {We summarize recent results and ongoing activities in mathematical algorithms and computer science methods related to proton computed tomography (pCT) and intensity-modulated particle therapy (IMPT) treatment planning. Proton therapy necessitates a high level of delivery accuracy to exploit the selective targeting imparted by the Bragg peak. For this purpose, pCT utilizes the proton beam itself to create images. The technique works by sending a low-intensity beam of protons through the patient and measuring the position, direction, and energy loss of each exiting proton. The pCT technique allows reconstruction of the volumetric distribution of the relative stopping power (RSP) of the patient tissues for use in treatment planning and pre-treatment range verification. We have investigated new ways to make the reconstruction both efficient and accurate. Better accuracy of RSP also enables more robust inverse approaches to IMPT. For IMPT, we developed a framework for performing intensity-modulation of the proton pencil beams. We expect that these developments will lead to additional project work in the years to come, which requires a regular exchange between experts in the fields of mathematics, computer science, and medical physics. We have initiated such an exchange by organizing annual workshops on pCT and IMPT algorithm and technology developments. This report is, admittedly, tilted toward our interdisciplinary work and methods. We offer a comprehensive overview of results, problems, and challenges in pCT and IMPT with the aim of making other scientists wanting to tackle such issues and to strengthen their interdisciplinary collaboration by bringing together cutting-edge know-how from medicine, computer science, physics, and mathematics to bear on medical physics problems at hand.},
	journal = {IEEE Transactions on Radiation and Plasma Medical Sciences},
	author = {Censor, Yair and Schubert, Keith E. and Schulte, Reinhard W.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Radiation and Plasma Medical Sciences},
	keywords = {Image reconstruction, blob basis functions, data partitioning, digital phantoms, intensity-modulated therapy, Inverse problems, Medical treatment, Monte Carlo simulation, motion-adapted reconstruction., Perturbation methods, Planning, Plasmas, proton computed tomography, proton therapy, Protons, superiorization},
	pages = {1--1},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/I45JGSBD/Censor et al. - 2021 - Developments in Mathematical Algorithms and Comput.pdf:application/pdf},
}

@article{horbelt_discretization_2002,
	title = {Discretization of the radon transform and of its inverse by spline convolutions},
	volume = {21},
	issn = {1558-254X},
	doi = {10.1109/TMI.2002.1000260},
	abstract = {We present an explicit formula for B-spline convolution kernels; these are defined as the convolution of several B-splines of variable widths h/sub i/ and degrees n/sub i/. We apply our results to derive spline-convolution-based algorithms for two closely related problems: the computation of the Radon transform and of its inverse. First, we present an efficient discrete implementation of the Radon transform that is optimal in the least-squares sense. We then consider the reverse problem and introduce a new spline-convolution version of the filtered back-projection algorithm for tomographic reconstruction. In both cases, our explicit kernel formula allows for the use of high-degree splines; these offer better approximation performance than the conventional lower-degree formulations (e.g., piecewise constant or piecewise linear models). We present multiple experiments to validate our approach and to find the parameters that give the best tradeoff between image quality and computational complexity. In particular, we find that it can be computationally more efficient to increase the approximation degree than to increase the sampling rate.},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Horbelt, S. and Liebling, M. and Unser, M.},
	month = apr,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Kernel, Spline, Image reconstruction, Image quality, Tomography, Computational complexity, Discrete transforms, Image sampling, Piecewise linear approximation, Piecewise linear techniques},
	pages = {363--376},
	file = {Horbelt et al. - 2002 - Discretization of the radon transform and of its i.pdf:/home/david/Zotero/storage/HUAUNN4V/Horbelt et al. - 2002 - Discretization of the radon transform and of its i.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/HHGPFXCE/1000260.html:text/html},
}

@misc{noauthor_b-spline_nodate,
	title = {B-{Spline} {Convolution}-based {Radon} and {Inverse} {Radon} {Transforms}},
	url = {http://sybil.ece.ucsb.edu/pages/splineradon/splineradon.html},
	urldate = {2021-12-15},
	file = {B-Spline Convolution-based Radon and Inverse Radon.html:/home/david/Zotero/storage/NS55MF6W/B-Spline Convolution-based Radon and Inverse Radon.html:text/html},
}

@misc{rieck_latex-mimosis_2021,
	title = {latex-mimosis: {A} minimal \& modern template for your thesis},
	copyright = {MIT},
	shorttitle = {latex-mimosis},
	url = {https://github.com/Pseudomanifold/latex-mimosis},
	abstract = {A minimal \& modern LaTeX template for your (bachelor's {\textbar} master's {\textbar} doctoral) thesis},
	urldate = {2021-12-18},
	author = {Rieck, Bastian},
	month = dec,
	year = {2021},
	note = {original-date: 2017-05-18T15:49:51Z},
	keywords = {latex, latex-class, latex-template, modern-template, template},
}

@misc{noauthor_latex-mimosislatexmkrc_nodate,
	title = {latex-mimosis/latexmkrc at master · {Pseudomanifold}/latex-mimosis},
	url = {https://github.com/Pseudomanifold/latex-mimosis},
	abstract = {A minimal \& modern LaTeX template for your (bachelor's {\textbar} master's {\textbar} doctoral) thesis - latex-mimosis/latexmkrc at master · Pseudomanifold/latex-mimosis},
	language = {en},
	urldate = {2021-12-18},
	journal = {GitHub},
	file = {Snapshot:/home/david/Zotero/storage/F6RTC7R2/latex-mimosis.html:text/html},
}

@misc{noauthor_thesis_entity_embeddingsmasterthesis_satyaalmasianpdf_nodate,
	title = {Thesis\_Entity\_Embeddings/{MasterThesis}\_SatyaAlmasian.pdf at master · satya77/{Thesis}\_Entity\_Embeddings},
	url = {https://github.com/satya77/Thesis_Entity_Embeddings},
	abstract = {Written Thesis of Entity Embeddings . Contribute to satya77/Thesis\_Entity\_Embeddings development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-12-18},
	journal = {GitHub},
	file = {Snapshot:/home/david/Zotero/storage/2HQ2XBN8/Thesis_Entity_Embeddings.html:text/html},
}

@book{richter_inverse_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Geosystems} {Mathematics} and {Computing}},
	title = {Inverse {Problems}: {Basics}, {Theory} and {Applications} in {Geophysics}},
	isbn = {978-3-030-59316-2 978-3-030-59317-9},
	shorttitle = {Inverse {Problems}},
	url = {https://link.springer.com/10.1007/978-3-030-59317-9},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Springer International Publishing},
	author = {Richter, Mathias},
	year = {2020},
	doi = {10.1007/978-3-030-59317-9},
	file = {Richter - 2020 - Inverse Problems Basics, Theory and Applications .pdf:/home/david/Zotero/storage/ZBBTFXUR/Richter - 2020 - Inverse Problems Basics, Theory and Applications .pdf:application/pdf},
}

@book{vogel_computational_2002,
	title = {Computational {Methods} for {Inverse} {Problems}},
	isbn = {978-0-89871-550-7 978-0-89871-757-0},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898717570},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Vogel, Curtis R.},
	month = jan,
	year = {2002},
	doi = {10.1137/1.9780898717570},
	file = {Vogel - 2002 - Computational Methods for Inverse Problems - 9 Non.pdf:/home/david/Zotero/storage/NRGYWZBK/Vogel - 2002 - Computational Methods for Inverse Problems - 9 Non.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 8 Tot.pdf:/home/david/Zotero/storage/XSQBCD2M/Vogel - 2002 - Computational Methods for Inverse Problems - 8 Tot.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 6 Par.pdf:/home/david/Zotero/storage/4FZGLZUN/Vogel - 2002 - Computational Methods for Inverse Problems - 6 Par.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 7 Reg.pdf:/home/david/Zotero/storage/2MN6YHPI/Vogel - 2002 - Computational Methods for Inverse Problems - 7 Reg.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 4 Sta.pdf:/home/david/Zotero/storage/LIYLI3NK/Vogel - 2002 - Computational Methods for Inverse Problems - 4 Sta.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 3 Num.pdf:/home/david/Zotero/storage/BTKSSJBY/Vogel - 2002 - Computational Methods for Inverse Problems - 3 Num.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 5 Ima.pdf:/home/david/Zotero/storage/56VE67RI/Vogel - 2002 - Computational Methods for Inverse Problems - 5 Ima.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 2 Ana.pdf:/home/david/Zotero/storage/S5RRT4FC/Vogel - 2002 - Computational Methods for Inverse Problems - 2 Ana.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 1 Int.pdf:/home/david/Zotero/storage/T9A4SIW5/Vogel - 2002 - Computational Methods for Inverse Problems - 1 Int.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems.pdf:/home/david/Zotero/storage/NL73W2WJ/Vogel - 2002 - Computational Methods for Inverse Problems.pdf:application/pdf},
}

@book{kirsch_introduction_2021,
	address = {Cham},
	series = {Applied {Mathematical} {Sciences}},
	title = {An {Introduction} to the {Mathematical} {Theory} of {Inverse} {Problems}},
	volume = {120},
	isbn = {978-3-030-63342-4 978-3-030-63343-1},
	url = {http://link.springer.com/10.1007/978-3-030-63343-1},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Springer International Publishing},
	author = {Kirsch, Andreas},
	year = {2021},
	doi = {10.1007/978-3-030-63343-1},
	file = {Kirsch - 2021 - An Introduction to the Mathematical Theory of Inve.pdf:/home/david/Zotero/storage/BQR246X7/Kirsch - 2021 - An Introduction to the Mathematical Theory of Inve.pdf:application/pdf},
}

@article{hanson_local_1985,
	title = {Local basis-function approach to computed tomography},
	volume = {24},
	issn = {0003-6935, 1539-4522},
	url = {https://www.osapublishing.org/abstract.cfm?URI=ao-24-23-4028},
	doi = {10.1364/AO.24.004028},
	language = {en},
	number = {23},
	urldate = {2021-12-19},
	journal = {Applied Optics},
	author = {Hanson, Kenneth M. and Wecksung, George W.},
	month = dec,
	year = {1985},
	pages = {4028},
	file = {Hanson and Wecksung - 1985 - Local basis-function approach to computed tomograp.pdf:/home/david/Zotero/storage/S2GK5W4M/Hanson and Wecksung - 1985 - Local basis-function approach to computed tomograp.pdf:application/pdf},
}

@article{unser_splines_1999,
	title = {Splines: a perfect fit for signal and image processing},
	volume = {16},
	issn = {10535888},
	shorttitle = {Splines},
	url = {http://ieeexplore.ieee.org/document/799930/},
	doi = {10.1109/79.799930},
	language = {en},
	number = {6},
	urldate = {2021-12-19},
	journal = {IEEE Signal Processing Magazine},
	author = {Unser, M.},
	month = nov,
	year = {1999},
	pages = {22--38},
	file = {Unser - 1999 - Splines a perfect fit for signal and image proces.pdf:/home/david/Zotero/storage/PPM7DBKS/Unser - 1999 - Splines a perfect fit for signal and image proces.pdf:application/pdf},
}

@incollection{hawkes_brief_2002,
	series = {Advances in {Imaging} and {Electron} {Physics}},
	title = {A {Brief} {Walk} {Through} {Sampling} {Theory}},
	volume = {124},
	url = {https://www.sciencedirect.com/science/article/pii/S1076567002800428},
	publisher = {Elsevier},
	author = {García, Antonio G.},
	editor = {Hawkes, Peter W.},
	year = {2002},
	doi = {https://doi.org/10.1016/S1076-5670(02)80042-8},
	note = {ISSN: 1076-5670},
	pages = {63--137},
}

@article{unser_sampling-50_2000,
	title = {Sampling-50 years after {Shannon}},
	volume = {88},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/843002/},
	doi = {10.1109/5.843002},
	language = {en},
	number = {4},
	urldate = {2021-12-19},
	journal = {Proceedings of the IEEE},
	author = {Unser, M.},
	month = apr,
	year = {2000},
	pages = {569--587},
	file = {Unser - 2000 - Sampling-50 years after Shannon.pdf:/home/david/Zotero/storage/TEZLLYM7/Unser - 2000 - Sampling-50 years after Shannon.pdf:application/pdf},
}

@article{harauz_interpolation_1983,
	title = {Interpolation in computing forward projections in direct three-dimensional reconstruction},
	volume = {28},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/28/12/007},
	doi = {10.1088/0031-9155/28/12/007},
	language = {en},
	number = {12},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {Harauz, G and Ottensmeyer, F P},
	month = dec,
	year = {1983},
	pages = {1419--1427},
	file = {Harauz and Ottensmeyer - 1983 - Interpolation in computing forward projections in .pdf:/home/david/Zotero/storage/67GNPXG6/Harauz and Ottensmeyer - 1983 - Interpolation in computing forward projections in .pdf:application/pdf},
}

@article{peters_algorithms_1981,
	title = {Algorithms for {Fast} {Back}- and {Re}-{Projection} in {Computed} {Tomography}},
	volume = {28},
	issn = {0018-9499},
	url = {http://ieeexplore.ieee.org/document/4331812/},
	doi = {10.1109/TNS.1981.4331812},
	abstract = {While the computation time for reconstructing images in C.T. is not a problem in commercial systems, there are many experimental and developmental applications where resources are limited and image reconstruction places a heavy burden on the computer system. This paper describes a very efficient backI projection algorithm which results in large time savings when implemented in machine code. Also described is a minor modification to this algorithm which converts it to a re-projection procedure with comparable efficiency.},
	language = {en},
	number = {4},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Nuclear Science},
	author = {Peters, T. M.},
	year = {1981},
	pages = {3641--3647},
	file = {Peters - 1981 - Algorithms for Fast Back- and Re-Projection in Com.pdf:/home/david/Zotero/storage/AV4FBMUV/Peters - 1981 - Algorithms for Fast Back- and Re-Projection in Com.pdf:application/pdf},
}

@article{joseph_improved_1982,
	title = {An {Improved} {Algorithm} for {Reprojecting} {Rays} through {Pixel} {Images}},
	volume = {1},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/4307572/},
	doi = {10.1109/TMI.1982.4307572},
	abstract = {It is often desired to calculate line integrals through a field of reconstructed CT density pixels for the purpose of improving CT image quality. Two algorithms widely published and discussed in the past are known to either degrade spatial resolution or generate errors in the results due to the discontinuous "square pixel" modeling of the reconstructed image. An algorithm is described, based on linear interpolation between pixels, which provides superior accuracy without unnecessary loss of resolution. It was tested on simulated data for a head section and on a narrow Gaussian density distribution. The experimental results demonstrated improved performance. The method is expected to prove useful for many types of post-reconstruction processing, including beam hardening, missing data, and noise supression algorithms.},
	language = {en},
	number = {3},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Joseph, Peter M.},
	month = nov,
	year = {1982},
	pages = {192--196},
	file = {Joseph - 1982 - An Improved Algorithm for Reprojecting Rays throug.pdf:/home/david/Zotero/storage/TWI9GP6K/Joseph - 1982 - An Improved Algorithm for Reprojecting Rays throug.pdf:application/pdf},
}

@article{siddon_fast_1985,
	title = {Fast calculation of the exact radiological path for a three-dimensional {CT} array},
	volume = {12},
	issn = {2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1118/1.595715},
	doi = {10.1118/1.595715},
	abstract = {Ready availability has prompted the use of computed tomography (CT) data in various applications in radiation therapy. For example, some radiation treatment planning systems now utilize CT data in heterogeneous dose calculation algorithms. In radiotherapy imaging applications, CT data are projected onto specified planes, thus producing “radiographs,” which are compared with simulator radiographs to assist in proper patient positioning and delineation of target volumes. All these applications share the common geometric problem of evaluating the radiological path through the CT array. Due to the complexity of the three-dimensional geometry and the enormous amount of CT data, the exact evaluation of the radiological path has proven to be a time consuming and difficult problem. This paper identifies the inefficient aspect of the traditional exact evaluation of the radiological path as that of treating the CT data as individual voxels. Rather than individual voxels, a new exact algorithm is presented that considers the CT data as consisting of the intersection volumes of three orthogonal sets of equally spaced, parallel planes. For a three-dimensional CT array of N3 voxels, the new exact algorithm scales with 3N, the number of planes, rather than N3, the number of voxels. Coded in fortran-77 on a VAX 11/780 with a floating point option, the algorithm requires approximately 5 ms to calculate an average radiological path in a 1003 voxel array.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Siddon, Robert L.},
	year = {1985},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1118/1.595715},
	keywords = {Biomedical imaging, Computed tomography, BIOMEDICAL RADIOGRAPHY, COMPUTERIZED TOMOGRAPHY, DIAGNOSTIC TECHNIQUES, Dosimetry, INHOMOGENEITY, Medical imaging, Non-ionizing radiation equipment and techniques, Radiation therapy, Radiation treatment, Radiography, RADIOTHERAPY},
	pages = {252--255},
	file = {Siddon - 1985 - Fast calculation of the exact radiological path fo.pdf:/home/david/Zotero/storage/YD5NEAPA/Siddon - 1985 - Fast calculation of the exact radiological path fo.pdf:application/pdf},
}

@inproceedings{zhao_fast_2004,
	address = {Portland, OR, USA},
	title = {Fast ray-tracing technique to calculate line integral paths in voxel arrays},
	isbn = {978-0-7803-8257-2},
	url = {http://ieeexplore.ieee.org/document/1352469/},
	doi = {10.1109/NSSMIC.2003.1352469},
	abstract = {The ray-driven projection and back-projection methods, frequently represented as calculating the path of line integration through a pixel or voxel space, are widely applied in various imaging research ﬁelds such as positron emission tomography (PET), computerized tomography (CT) and other volumetric ray tracing studies. For high resolution iterative image reconstruction with large amount of sample events, computing the projections event-by-event is an extremely time consuming task. This paper presents a novel ray-tracing calculation technique applying to a list-mode EM image reconstruction algorithm as a part of the system model to accelerate its projection operations. Compared to both the conventional Siddon algorithm and its accelerated methods, the new algorithm has less average computation operation time on each voxel. The results show that the new algorithm signiﬁcantly improves the calculation speed up to twice as fast than the incremental Siddon algorithm.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {2003 {IEEE} {Nuclear} {Science} {Symposium}. {Conference} {Record} ({IEEE} {Cat}. {No}.{03CH37515})},
	publisher = {IEEE},
	author = {Zhao, Huaxia and Reader, A.J.},
	year = {2004},
	pages = {2808--2812},
	file = {Zhao and Reader - 2004 - Fast ray-tracing technique to calculate line integ.pdf:/home/david/Zotero/storage/3NUNEXV8/Zhao and Reader - 2004 - Fast ray-tracing technique to calculate line integ.pdf:application/pdf},
}

@article{gullberg_attenuated_1985,
	title = {An attenuated projector-backprojector for iterative {SPECT} reconstruction},
	volume = {30},
	issn = {0031-9155},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/30/8/004},
	doi = {10.1088/0031-9155/30/8/004},
	abstract = {A new ray-driuen projector-backprojector which can easily be adapted for hardware implementation is described and simulated in software. The projector-backprcjector discretely models the attenuated Radon transform of a source distributed within an attenuating medium as line integrals of discrete pixels, obtained using the standard sampling technique of averaging the emission source or attenuation distribution over small\$square regions. Attenuation factors are calculated for each pixel during the projection and backprojection operations instead of using precalculated values. The calculation of the factors requires a specification of the attenuation distribution, estimated either from an assumed constant distribution and an approximate body outline or from transmission measurements. The distribution of attenuation coefficientsis stored in memory for efficient access during the projection and backprojection operations. The reconstruction of the source distribution is obtained by using a conjugate gradient or SIRTtype iterative algorithm which requires one projection and one backprojection operation for each iteration.},
	language = {en},
	number = {8},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {Gullberg, G T and Huesman, R H and Malko, J A and Pelc, N J and Budinger, T F},
	month = aug,
	year = {1985},
	pages = {799--816},
	file = {Gullberg et al. - 1985 - An attenuated projector-backprojector for iterativ.pdf:/home/david/Zotero/storage/DSMCP9TB/Gullberg et al. - 1985 - An attenuated projector-backprojector for iterativ.pdf:application/pdf},
}

@article{christiaens_fast_1999,
	title = {A fast, cache-aware algorithm for the calculation of radiological paths exploiting subword parallelism},
	volume = {45},
	issn = {13837621},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1383762198000381},
	doi = {10.1016/S1383-7621(98)00038-1},
	language = {en},
	number = {10},
	urldate = {2021-12-19},
	journal = {Journal of Systems Architecture},
	author = {Christiaens, Mark and De Sutter, Bjorn and De Bosschere, Koen and Van Campenhout, Jan and Lemahieu, Ignace},
	month = apr,
	year = {1999},
	pages = {781--790},
	file = {Christiaens et al. - 1999 - A fast, cache-aware algorithm for the calculation .pdf:/home/david/Zotero/storage/8RB4RBWU/Christiaens et al. - 1999 - A fast, cache-aware algorithm for the calculation .pdf:application/pdf},
}

@article{de_man_distance-driven_2004,
	title = {Distance-driven projection and backprojection in three dimensions},
	volume = {49},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/49/11/024},
	doi = {10.1088/0031-9155/49/11/024},
	abstract = {Projection and backprojection are operations that arise frequently in tomographic imaging. Recently, we proposed a new method for projection and backprojection, which we call distance-driven, and that offers low arithmetic cost and a highly sequential memory access pattern. Furthermore, distance-driven projection and backprojection avoid several artefact-inducing approximations characteristic of some other methods. We have previously demonstrated the application of this method to parallel and fan beam geometries. In this paper, we extend the distance-driven framework to three dimensions and demonstrate its application to cone beam reconstruction. We also present experimental results to demonstrate the computational performance, the artefact characteristics and the noise-resolution characteristics of the distance-driven method in three dimensions.},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {De Man, B. and Basu, S.},
	month = jun,
	year = {2004},
	pages = {2463--2475},
	file = {De Man and Basu - 2004 - Distance-driven projection and backprojection in t.pdf:/home/david/Zotero/storage/NVBE3FUL/De Man and Basu - 2004 - Distance-driven projection and backprojection in t.pdf:application/pdf},
}

@inproceedings{de_man_distance-driven_2002,
	title = {Distance-driven projection and backprojection},
	volume = {3},
	doi = {10.1109/NSSMIC.2002.1239600},
	abstract = {Projection and backprojection are important processes in computed tomography (CT). They are used in iterative reconstruction, simulation, and artifact correction, as well as routine (filtered-backprojection based) reconstruction. Existing methods either have poor performance or result in artifacts. A new method for projecting and backprojecting rays through pixels is presented that has good performance and eliminates artifacts, and could potentially enable routine iterative reconstruction in clinical CT systems. The new method, which we call distance-driven, reconciles the advantages of the common pixel-driven and ray-driven methods. It avoids an artifact-inducing approximation made by the previous methods and has very favorable computational properties. The method is applicable to parallel-beam, fanbeam, and cone-beam geometries. Its performance and artifact behavior are evaluated on a two-dimensional fan-beam geometry with flat detector and compared to the pixel-driven and ray-driven approaches. The distance-driven method prevents the artifacts that are generated in the pixel-driven projection and in the ray-driven backprojection. It outperforms pixel-driven and ray-driven methods from a computational standpoint and is amenable to hardware implementation.},
	booktitle = {2002 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	author = {De Man, B. and Basu, S.},
	month = nov,
	year = {2002},
	keywords = {Computed tomography, Detectors, Image reconstruction, Computational modeling, Geometry, Hardware, Image converters, Image generation, Iterative methods, X-ray imaging},
	pages = {1477--1480 vol.3},
	file = {De Man and Basu - 2002 - Distance-driven projection and backprojection.pdf:/home/david/Zotero/storage/YFYHSSJK/De Man and Basu - 2002 - Distance-driven projection and backprojection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/2GMTKNYD/1239600.html:text/html},
}

@article{liu_gpu-based_2017,
	title = {{GPU}-{Based} {Branchless} {Distance}-{Driven} {Projection} and {Backprojection}},
	volume = {3},
	issn = {2333-9403},
	doi = {10.1109/TCI.2017.2675705},
	abstract = {Projection and backprojection operations are essential in a variety of image reconstruction and physical correction algorithms in computed tomography (CT). The distance-driven (DD) projection and backprojection are widely used for their favorable image quality properties, highly sequential memory access pattern and low arithmetic cost. However, a typical DD implementation has an inner loop that adjusts the calculation depending on the relative position between voxel and detector cell boundaries. The irregularity of the branch behavior makes it inefficient to be implemented on massively parallel computing devices, such as graphics processing units (GPUs). Such irregular branch behaviors can be eliminated by factorizing the DD operation as three branchless steps: integration, linear interpolation, and differentiation, all of which are highly amenable to massive vectorization. In this paper, we implement and evaluate a highly parallel branchless DD algorithm for three-dimensional cone beam CT. The algorithm utilizes the texture memory and hardware interpolation on GPUs to achieve fast computational speed. The developed branchless DD algorithm achieved 137-fold speedup for forward projection and 188-fold speedup for backprojection relative to a single-thread CPU implementation. Compared with a state-of-the-art 32-thread CPU implementation, the proposed branchless DD achieved eight-fold acceleration for forward projection and ten-fold acceleration for backprojection. The GPU-based branchless DD method was evaluated by iterative reconstruction algorithms with both simulation and real datasets. It obtained visually identical images as the CPU reference algorithm.},
	number = {4},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Liu, Rui and Fu, Lin and De Man, Bruno and Yu, Hengyong},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Computational Imaging},
	keywords = {Computed tomography, Detectors, Image reconstruction, Computational modeling, Algorithm design and analysis, backprojection, Branchless distance-driven, computed tomography, GPU, Graphics processing units, projection, reconstruction},
	pages = {617--632},
	file = {Liu et al. - 2017 - GPU-Based Branchless Distance-Driven Projection an.pdf:/home/david/Zotero/storage/2SUS8GEE/Liu et al. - 2017 - GPU-Based Branchless Distance-Driven Projection an.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GGRE8SD7/7866886.html:text/html},
}

@article{long_3d_2010,
	title = {{3D} {Forward} and {Back}-{Projection} for {X}-{Ray} {CT} {Using} {Separable} {Footprints}},
	volume = {29},
	issn = {1558-254X},
	doi = {10.1109/TMI.2010.2050898},
	abstract = {Iterative methods for 3D image reconstruction have the potential to improve image quality over conventional filtered back projection (FBP) in X-ray computed tomography (CT). However, the computation burden of 3D cone-beam forward and back-projectors is one of the greatest challenges facing practical adoption of iterative methods for X-ray CT. Moreover, projector accuracy is also important for iterative methods. This paper describes two new separable footprint (SF) projector methods that approximate the voxel footprint functions as 2D separable functions. Because of the separability of these footprint functions, calculating their integrals over a detector cell is greatly simplified and can be implemented efficiently. The SF-TR projector uses trapezoid functions in the transaxial direction and rectangular functions in the axial direction, whereas the SF-TT projector uses trapezoid functions in both directions. Simulations and experiments showed that both SF projector methods are more accurate than the distance-driven (DD) projector, which is a current state-of-the-art method in the field. The SF-TT projector is more accurate than the SF-TR projector for rays associated with large cone angles. The SF-TR projector has similar computation speed with the DD projector and the SF-TT projector is about two times slower.},
	number = {11},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Long, Yong and Fessler, Jeffrey A. and Balter, James M.},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Computed tomography, Detectors, Image reconstruction, Image quality, Iterative methods, Argon, Cone-beam tomography, forward and back-projection, iterative tomographic image reconstruction, Permission, Postal services, Reconstruction algorithms, USA Councils},
	pages = {1839--1850},
	file = {Long et al. - 2010 - 3D Forward and Back-Projection for X-Ray CT Using .pdf:/home/david/Zotero/storage/KZKMICNW/Long et al. - 2010 - 3D Forward and Back-Projection for X-Ray CT Using .pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/4BP54L79/5482021.html:text/html},
}

@article{long_3d_nodate,
	title = {{3D} {Forward} and {Back}-{Projection} for {X}-{Ray} {CT} {Using} {Separable} {Footprints} with {Trapezoid} {Functions}},
	abstract = {The greatest impediment to practical adoption of iterative methods for X-ray CT is the computation burden of cone-beam forward and back-projectors. Moreover, forward and back-projector accuracy is also crucial to iterative reconstruction methods. We previously described a computationally efﬁcient projector that approximates the voxel footprint functions by the 2D separable products of trapezoid functions in the transaxial plane and rectangular functions in the axial direction [1], [2]. The separability of these footprint functions simpliﬁes calculating their integrals over rectangular detector cells. We showed that this separable footprint (SF-TR) method was more accurate than the distance-driven (DD) method but with comparable computation time. This paper describes a new extension of that projector, called the SF-TT projector, that uses trapezoid functions in both directions. We show that using a trapezoid along the axial direction improves projector accuracy for voxels associated with larger cone angles. However, this improved accuracy requires increased computation compared to the rectangular approximation. Having both options available facilitates evaluation of the trade offs between accuracy and computation for different cone-beam geometries.},
	language = {en},
	author = {Long, Yong and Fessler, Jeffrey A},
	pages = {4},
	file = {Long and Fessler - 3D Forward and Back-Projection for X-Ray CT Using .pdf:/home/david/Zotero/storage/EB74DSN6/Long and Fessler - 3D Forward and Back-Projection for X-Ray CT Using .pdf:application/pdf},
}

@article{ziegler_efficient_2006,
	title = {Efficient projection and backprojection scheme for spherically symmetric basis functions in divergent beam geometry: {Efficient} projection and backprojection scheme},
	volume = {33},
	issn = {00942405},
	shorttitle = {Efficient projection and backprojection scheme for spherically symmetric basis functions in divergent beam geometry},
	url = {http://doi.wiley.com/10.1118/1.2388570},
	doi = {10.1118/1.2388570},
	language = {en},
	number = {12},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Ziegler, Andy and Köhler, Thomas and Nielsen, Tim and Proksa, Roland},
	month = nov,
	year = {2006},
	pages = {4653--4663},
	file = {Ziegler et al. - 2006 - Efficient projection and backprojection scheme for.pdf:/home/david/Zotero/storage/9YQV7PGE/Ziegler et al. - 2006 - Efficient projection and backprojection scheme for.pdf:application/pdf},
}

@article{jacobs_fast_1998,
	title = {A {Fast} {Algorithm} to {Calculate} the {Exact} {Radiological} {Path} through a {Pixel} or {Voxel} {Space}},
	volume = {6},
	abstract = {Calculating the exact radiological path through a pixel or voxel space is a frequently encountered problem in medical image reconstruction from projections and greatly in uences the reconstruction time. Currently, one of the fastest algorithms designed for this purpose was published in 1985 by Robert L. Siddon 1]. In this paper, we propose an improved version of Siddon's algorithm, resulting in a considerable speedup.},
	language = {en},
	number = {1},
	journal = {Journal of computing and information technology},
	author = {Jacobs, Filip and Sundermann, Erik and Sutter, Bjorn De and Christiaens, Mark and Lemahieu, Ignace},
	year = {1998},
	pages = {14},
	file = {Jacobs et al. - 1998 - A Fast Algorithm to Calculate the Exact Radiologic.pdf:/home/david/Zotero/storage/DKCJQJTR/Jacobs et al. - 1998 - A Fast Algorithm to Calculate the Exact Radiologic.pdf:application/pdf},
}

@article{gao_fast_2012,
	title = {Fast parallel algorithms for the x-ray transform and its adjoint: {Fast} parallel x-ray transform and its adjoint},
	volume = {39},
	issn = {00942405},
	shorttitle = {Fast parallel algorithms for the x-ray transform and its adjoint},
	url = {http://doi.wiley.com/10.1118/1.4761867},
	doi = {10.1118/1.4761867},
	abstract = {Purpose: Iterative reconstruction methods often offer better imaging quality and allow for reconstructions with lower imaging dose than classical methods in computed tomography. However, the computational speed is a major concern for these iterative methods, for which the x-ray transform and its adjoint are two most time-consuming components. The speed issue becomes even notable for the 3D imaging such as cone beam scans or helical scans, since the x-ray transform and its adjoint are frequently computed as there is usually not enough computer memory to save the corresponding system matrix. The purpose of this paper is to optimize the algorithm for computing the x-ray transform and its adjoint, and their parallel computation.
Methods: The fast and highly parallelizable algorithms for the x-ray transform and its adjoint are proposed for the inﬁnitely narrow beam in both 2D and 3D. The extension of these fast algorithms to the ﬁnite-size beam is proposed in 2D and discussed in 3D.
Results: The CPU and GPU codes are available at https://sites.google.com/site/fastxraytransform. The proposed algorithm is faster than Siddon’s algorithm for computing the x-ray transform. In particular, the improvement for the parallel computation can be an order of magnitude.
Conclusions: The authors have proposed fast and highly parallelizable algorithms for the x-ray transform and its adjoint, which are extendable for the ﬁnite-size beam. The proposed algorithms are suitable for parallel computing in the sense that the computational cost per parallel thread is O(1). © 2012 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4761867]},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Gao, Hao},
	month = nov,
	year = {2012},
	pages = {7110--7120},
	file = {Gao - 2012 - Fast parallel algorithms for the x-ray transform a.pdf:/home/david/Zotero/storage/N5RT6RMA/Gao - 2012 - Fast parallel algorithms for the x-ray transform a.pdf:application/pdf},
}

@inproceedings{wu_gpu_2011,
	title = {{GPU} acceleration of {3D} forward and backward projection using separable footprints for {X}-ray {CT} image reconstruction},
	volume = {6},
	booktitle = {Proc. {Intl}. {Mtg}. on {Fully} {3D} {Image} {Recon}. in {Rad}. and {Nuc}. {Med}},
	publisher = {Citeseer},
	author = {Wu, Meng and Fessler, Jeffrey A},
	year = {2011},
	pages = {021911},
	file = {Wu and Fessler - 2011 - GPU acceleration of 3D forward and backward projec.pdf:/home/david/Zotero/storage/LQHXSY2T/Wu and Fessler - 2011 - GPU acceleration of 3D forward and backward projec.pdf:application/pdf},
}

@book{carpio_inverse_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Mathematics}},
	title = {Inverse {Problems} and {Imaging}},
	volume = {1943},
	isbn = {978-3-540-78545-3 978-3-540-78547-7},
	url = {http://link.springer.com/10.1007/978-3-540-78547-7},
	language = {en},
	urldate = {2021-12-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Carpio, Ana and Dorn, Oliver and Moscoso, Miguel and Natterer, Frank and Papanicolaou, George C. and Rapún, Maria Luisa and Teta, Alessandro},
	editor = {Bonilla, Luis L. and Morel, J. -M. and Takens, F. and Teissier, B.},
	year = {2008},
	doi = {10.1007/978-3-540-78547-7},
	file = {Carpio et al. - 2008 - Inverse Problems and Imaging.pdf:/home/david/Zotero/storage/WVVKLV8P/Carpio et al. - 2008 - Inverse Problems and Imaging.pdf:application/pdf},
}

@article{kaipio_statistical_2007,
	title = {Statistical inverse problems: {Discretization}, model reduction and inverse crimes},
	volume = {198},
	issn = {03770427},
	shorttitle = {Statistical inverse problems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042705007296},
	doi = {10.1016/j.cam.2005.09.027},
	abstract = {The article discusses the discretization of linear inverse problems. When an inverse problem is formulated in terms of inﬁnitedimensional function spaces and then discretized for computational purposes, a discretization error appears. Since inverse problems are typically ill-posed, neglecting this error may have serious consequences to the quality of the reconstruction. The Bayesian paradigm provides tools to estimate the statistics of the discretization error that is made part of the measurement and modelling errors of the estimation problem. This approach also provides tools to reduce the dimensionality of inverse problems in a controlled manner. The ideas are demonstrated with a computed example.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Kaipio, Jari and Somersalo, Erkki},
	month = jan,
	year = {2007},
	pages = {493--504},
	file = {Kaipio and Somersalo - 2007 - Statistical inverse problems Discretization, mode.pdf:/home/david/Zotero/storage/3ZV3QANY/Kaipio and Somersalo - 2007 - Statistical inverse problems Discretization, mode.pdf:application/pdf},
}

@book{kaipio_statistical_2005,
	address = {New York},
	series = {Applied mathematical sciences},
	title = {Statistical and computational inverse problems},
	isbn = {978-0-387-22073-4},
	language = {en},
	number = {v. 160},
	publisher = {Springer},
	author = {Kaipio, Jari and Somersalo, Erkki},
	year = {2005},
	keywords = {Inverse problems (Differential equations), Numerical solutions},
	file = {Kaipio and Somersalo - 2005 - Statistical and computational inverse problems.pdf:/home/david/Zotero/storage/U5SPINME/Kaipio and Somersalo - 2005 - Statistical and computational inverse problems.pdf:application/pdf},
}

@article{wirgin_inverse_2004,
	title = {The inverse crime},
	url = {http://arxiv.org/abs/math-ph/0401050},
	abstract = {The inverse crime occurs when the same (or very nearly the same) theoretical ingredients are employed to synthesize as well as to invert data in an inverse problem. This act has been qualified as trivial and therefore to be avoided by Colton and Kress.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:math-ph/0401050},
	author = {Wirgin, Armand},
	month = jan,
	year = {2004},
	note = {arXiv: math-ph/0401050},
	keywords = {Mathematical Physics},
	file = {Wirgin - 2004 - The inverse crime.pdf:/home/david/Zotero/storage/EV2FD3WY/Wirgin - 2004 - The inverse crime.pdf:application/pdf},
}

@article{savanier_magnificationdriven_2021,
	title = {Magnification‐driven {B}‐spline interpolation for cone‐beam projection and backprojection},
	volume = {48},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.15179},
	doi = {10.1002/mp.15179},
	abstract = {Purpose: Discretizing tomographic forward and backward operations is a crucial step in the design of model-based reconstruction algorithms. Standard projectors rely on linear interpolation, whose adjoint introduces discretization errors during backprojection. More advanced techniques are obtained through geometric footprint models that may present a high computational cost and an inner logic that is not suitable for implementation on massively parallel computing architectures. In this work, we take a fresh look at the discretization of resampling transforms and focus on the issue of magniﬁcation-induced local sampling variations by introducing a new magniﬁcation-driven interpolation approach for tomography.
Methods: Starting from the existing literature on spline interpolation for magniﬁcation purposes, we provide a mathematical formulation for discretizing a one-dimensional homography. We then extend our approach to two-dimensional representations in order to account for the geometry of cone-beam computed tomography with a ﬂat panel detector. Our new method relies on the decomposition of signals onto a space generated by nonuniform B-splines so as to capture the spatially varying magniﬁcation that locally affects sampling. We propose various degrees of approximations for a rapid implementation of the proposed approach. Our framework allows us to deﬁne a novel family of projector/backprojector pairs parameterized by the order of the employed B-splines. The state-of -the-art distance-driven interpolation appears to ﬁt into this family thus providing new insight and computational layout for this scheme. The question of data resampling at the detector level is handled and integrated with reconstruction in a single framework.
Results: Results on both synthetic data and real data using a quality assurance phantom, were performed to validate our approach. We show experimentally that our approximate implementations are associated with reduced complexity while achieving a near-optimal performance. In contrast with linear interpolation, B-splines guarantee full usage of all data samples, and thus the X-ray dose, leading to more uniform noise properties. In addition, higher-order B-splines allow analytical and iterative reconstruction to reach higher resolution. These beneﬁts appear more signiﬁcant when downsampling frames acquired by X-ray ﬂat-panel detectors with small pixels.
Conclusions: Magniﬁcation-driven B-spline interpolation is shown to provide high-accuracy projection operators with good-quality adjoints for iterative reconstruction. It equally applies to backprojection for analytical reconstruction and detector data downsampling.},
	language = {en},
	number = {10},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Savanier, Marion and Riddell, Cyril and Trousset, Yves and Chouzenoux, Emilie and Pesquet, Jean‐Christophe},
	month = oct,
	year = {2021},
	pages = {6339--6361},
	file = {Savanier et al. - 2021 - Magnification‐driven B‐spline interpolation for co.pdf:/home/david/Zotero/storage/2PJDR8ND/Savanier et al. - 2021 - Magnification‐driven B‐spline interpolation for co.pdf:application/pdf},
}

@inproceedings{fehringer_versatile_2014,
	title = {A versatile tomographic forward- and backprojection approach on {Multi}-{GPUs}},
	doi = {10.1117/12.2043860},
	author = {Fehringer, Andreas and Lasser, Tobias and Zanette, Irene and Noël, Peter and Pfeiffer, Franz},
	month = mar,
	year = {2014},
	pages = {90344F},
	file = {Fehringer et al. - 2014 - A versatile tomographic forward- and backprojectio.pdf:/home/david/Zotero/storage/ZZLAIZKE/Fehringer et al. - 2014 - A versatile tomographic forward- and backprojectio.pdf:application/pdf},
}

@article{fehringer_real-time_nodate,
	title = {Real-time iterative reconstruction for x-ray computed tomography},
	language = {en},
	author = {Fehringer, Andreas},
	pages = {150},
	file = {Fehringer - Real-time iterative reconstruction for x-ray compu.pdf:/home/david/Zotero/storage/XJX6P8FE/Fehringer - Real-time iterative reconstruction for x-ray compu.pdf:application/pdf},
}

@article{xie_effective_2015,
	title = {An {Effective} {CUDA} {Parallelization} of {Projection} in {Iterative} {Tomography} {Reconstruction}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0142184},
	doi = {10.1371/journal.pone.0142184},
	abstract = {Projection and back-projection are the most computationally intensive parts in Computed Tomography (CT) reconstruction, and are essential to acceleration of CT reconstruction algorithms. Compared to back-projection, parallelization efficiency in projection is highly limited by racing condition and thread unsynchronization. In this paper, a strategy of Fixed Sampling Number Projection (FSNP) is proposed to ensure the operation synchronization in the ray-driven projection with Graphical Processing Unit (GPU). Texture fetching is also used utilized to further accelerate the interpolations in both projection and back-projection. We validate the performance of this FSNP approach using both simulated and real conebeam CT data. Experimental results show that compare to the conventional approach, the proposed FSNP method together with texture fetching is 10{\textasciitilde}16 times faster than the conventional approach based on global memory, and thus leads to more efficient iterative algorithm in CT reconstruction.},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {PLOS ONE},
	author = {Xie, Lizhe and Hu, Yining and Yan, Bin and Wang, Lin and Yang, Benqiang and Liu, Wenyuan and Zhang, Libo and Luo, Limin and Shu, Huazhong and Chen, Yang},
	editor = {Zhang, Qinghui},
	month = nov,
	year = {2015},
	pages = {e0142184},
	file = {Xie et al. - 2015 - An Effective CUDA Parallelization of Projection in.pdf:/home/david/Zotero/storage/YC2N49UR/Xie et al. - 2015 - An Effective CUDA Parallelization of Projection in.pdf:application/pdf},
}

@article{graetz_high_2020,
	title = {High performance volume ray casting: {A} branchless generalized {Joseph} projector},
	shorttitle = {High performance volume ray casting},
	url = {http://arxiv.org/abs/1609.00958},
	abstract = {A concise and highly performant branchless formulation of a Joseph-type interpolating ray-casting algorithm for the computation of X-ray projections is presented. It efﬁciently utilizes the hardware resources of modern graphics processing units at the scale of their theoretic maximum performance reaching access rates of 600 GB/s within read-and-write memory, and is further shown to do so without compromising on image quality. The computation of X-ray projections from discrete voxel grids is an ubiquitous task in many problems related to volume image processing, including tomographic reconstruction and visualization. Although its central role has given rise to numerous publications discussing the optimal modeling of rayvolume intersections, a unique benchmark in this respect does not exist. Here, a 3D Shepp-Logan phantom is used, which allows the computation of analytic reference projections that can further serve as input to iterative reconstructions without committing the inverse crime. The proposed algorithm (GJP) is compared to the competing and widely adopted digital differential analyzer (DDA), which computes exact line-box intersections. It is thereby found to outperform the DDA on recent graphics processors in all respects: Despite accessing twice as much memory, the GJP is still able to calculate projections twice as fast. It further exhibits considerably less discretization artifacts, and neither oversampling of the DDA nor a smooth interpolation kernel within the GJP are able to improve on these results in any respect.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1609.00958 [physics]},
	author = {Graetz, Jonas},
	month = aug,
	year = {2020},
	note = {arXiv: 1609.00958},
	keywords = {Physics - Medical Physics, Computer Science - Graphics},
	file = {Graetz - 2020 - High performance volume ray casting A branchless .pdf:/home/david/Zotero/storage/L4G45GGK/Graetz - 2020 - High performance volume ray casting A branchless .pdf:application/pdf},
}

@inproceedings{flohr_accelerating_2017,
	address = {Orlando, Florida, United States},
	title = {Accelerating separable footprint ({SF}) forward and back projection on {GPU}},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2252010},
	doi = {10.1117/12.2252010},
	abstract = {Statistical image reconstruction (SIR) methods for X-ray CT can improve image quality and reduce radiation dosages over conventional reconstruction methods, such as ﬁltered back projection (FBP). However, SIR methods require much longer computation time. The separable footprint (SF) forward and back projection technique simpliﬁes the calculation of intersecting volumes of image voxels and ﬁnite-size beams in a way that is both accurate and eﬃcient for parallel implementation. We propose a new method to accelerate the SF forward and back projection on GPU with NVIDIA’s CUDA environment. For the forward projection, we parallelize over all detector cells. For the back projection, we parallelize over all 3D image voxels. The simulation results show that the proposed method is faster than the acceleration method of the SF projectors proposed by Wu and Fessler.13 We further accelerate the proposed method using multiple GPUs. The results show that the computation time is reduced approximately proportional to the number of GPUs.},
	language = {en},
	urldate = {2021-12-19},
	author = {Xie, Xiaobin and McGaffin, Madison G. and Long, Yong and Fessler, Jeffrey A. and Wen, Minhua and Lin, James},
	editor = {Flohr, Thomas G. and Lo, Joseph Y. and Gilat Schmidt, Taly},
	month = mar,
	year = {2017},
	pages = {101322S},
	file = {Xie et al. - 2017 - Accelerating separable footprint (SF) forward and .pdf:/home/david/Zotero/storage/FV6ULIGW/Xie et al. - 2017 - Accelerating separable footprint (SF) forward and .pdf:application/pdf},
}

@inproceedings{ha_efficient_2016,
	address = {Bamberg},
	title = {Efficient {Area}-{Based} {Ray} {Integration} {Using} {Summed} {Area} {Tables} and {Regression} {Models}},
	abstract = {The increasing popularity of iterative reconstruction algorithms has raised the attention onto how to build more accurate, realistic CT system models. In our work, we model the CT projectors based on volume integrals. The higher computational complexity in computing the exact volume integration is hidden by memory-efficient, fast, and accurate look-up tables. For further reductions we also derive a simple linear regression model from the table. We demonstrate our ideas with data obtained with a fan-beam flat-detector CT system. We observe speed-ups of up to 30\% while keeping a higher or at least similar image quality than existing advanced CT system models.},
	language = {en},
	author = {Ha, Sungsoo and Li, Heyi and Mueller, Klaus},
	year = {2016},
	pages = {4},
	file = {Ha et al. - 2016 - Efficient Area-Based Ray Integration Using Summed .pdf:/home/david/Zotero/storage/HS9Y4JLD/Ha et al. - 2016 - Efficient Area-Based Ray Integration Using Summed .pdf:application/pdf},
}

@article{ha_look-up_2018,
	title = {A {Look}-{Up} {Table}-{Based} {Ray} {Integration} {Framework} for 2-{D}/3-{D} {Forward} and {Back} {Projection} in {X}-{Ray} {CT}},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/8013154/},
	doi = {10.1109/TMI.2017.2741781},
	abstract = {Iterative algorithms have become increasingly popular in Computed Tomography (CT) image reconstruction since they better deal with the adverse image artifacts arising from low radiation dose image acquisition. But iterative methods remain computationally expensive. The main cost emerges in the projection and backprojection operations where accurate CT system modeling can greatly improve the quality of the reconstructed image. We present a framework that improves upon one particular aspect – the accurate projection of the image basis functions. It differs from current methods in that it substitutes the high computational complexity associated with accurate voxel projection by a small number of memory operations. Coefﬁcients are computed in advance and stored in look-up tables parameterized by the CT system’s projection geometry. The look-up tables only require a few kilobytes of storage and can be efﬁciently accelerated on the GPU. We demonstrate our framework with both numerical and clinical experiments and compare its performance with the current state of the art scheme – the separable footprint method.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ha, Sungsoo and Mueller, Klaus},
	month = feb,
	year = {2018},
	pages = {361--371},
	file = {Ha and Mueller - 2018 - A Look-Up Table-Based Ray Integration Framework fo.pdf:/home/david/Zotero/storage/GCDF7THL/Ha and Mueller - 2018 - A Look-Up Table-Based Ray Integration Framework fo.pdf:application/pdf},
}

@inproceedings{ha_study_2015,
	address = {Newport},
	title = {A {Study} of {Volume} {Integration} {Models} for {Iterative} {Cone}-{Beam} {Computed} {Tomography}},
	abstract = {With the help of modern parallel computers, iterative reconstruction algorithms have become a feasible research topic in the field of CT. These types of algorithms can greatly benefit from an accurate, realistic CT system model. In our study, we model the CT projection as volume integrals and propose a set of methods that can compute the volume integrals either exactly or approximately. Our approximate volume integral methods have a much smaller complexity algorithmically than the exact method, but their accuracy is close to it. More importantly, the proposed approximate methods can be easily ported to modern parallel processors to utilize their massive computation powers.},
	language = {en},
	author = {Ha, Sungsoo and Kumar, Ayush and Mueller, Klaus},
	year = {2015},
	pages = {4},
	file = {Ha et al. - 2015 - A Study of Volume Integration Models for Iterative.pdf:/home/david/Zotero/storage/2GD9XFFT/Ha et al. - 2015 - A Study of Volume Integration Models for Iterative.pdf:application/pdf},
}

@misc{trettin_essential_2007,
	title = {An essential guide to {LATEX} 2ε usage},
	language = {English},
	author = {Trettin, Mark},
	translator = {Fenn, Jürgen},
	month = jun,
	year = {2007},
	file = {Trettin - 2007 - An essential guide to LATEX 2ε usage.pdf:/home/david/Zotero/storage/SD65LDDV/Trettin - 2007 - An essential guide to LATEX 2ε usage.pdf:application/pdf},
}

@article{oetiker_not_2021,
	title = {The {Not} {So} {Short} {Introduction} to {LATEX} 2ε},
	language = {en},
	author = {Oetiker, Tobias and Partl, Hubert and Hyna, Irene and Schlegl, Elisabeth},
	month = mar,
	year = {2021},
	pages = {153},
	file = {Oetiker et al. - The Not So Short Introduction to LATEX 2ε.pdf:/home/david/Zotero/storage/MCUQ5N6C/Oetiker et al. - The Not So Short Introduction to LATEX 2ε.pdf:application/pdf},
}

@article{oetiker_not_2001,
	title = {The {Not} {So} {Short} {Introduction} to {LATEX} 2ε},
	language = {en},
	author = {Oetiker, Tobias and Partl, Hubert and Hyna, Irene and Schlegl, Elisabeth},
	month = aug,
	year = {2001},
	pages = {109},
	file = {Oetiker et al. - The Not So Short Introduction to LATEX 2ε.pdf:/home/david/Zotero/storage/36US9YWJ/Oetiker et al. - The Not So Short Introduction to LATEX 2ε.pdf:application/pdf},
}

@misc{noauthor_latex_2001,
	title = {{LATEX} 2ε for authors},
	url = {https://www.math.uni-hamburg.de/doc/latex/latex2e.pdf},
	urldate = {2021-12-20},
	month = jul,
	year = {2001},
	file = {2001 - LATEX 2ε for authors.pdf:/home/david/Zotero/storage/PKUFNYPJ/2001 - LATEX 2ε for authors.pdf:application/pdf},
}

@misc{kohm_koma_2021,
	title = {{KOMA} - {Script}: ein wandelbares {LATEX} 2ε-{Paket}},
	url = {https://komascript.de/~mkohm/scrguide.pdf},
	urldate = {2021-12-18},
	author = {Kohm, Markus},
	month = nov,
	year = {2021},
	file = {Kohm - 2021 - KOMA - Script ein wandelbares LATEX 2ε-Paket.pdf:/home/david/Zotero/storage/MVDX4MTU/Kohm - 2021 - KOMA - Script ein wandelbares LATEX 2ε-Paket.pdf:application/pdf},
}

@inproceedings{shewchuk_introduction_1994,
	title = {An {Introduction} to the {Conjugate} {Gradient} {Method} {Without} the {Agonizing} {Pain}},
	url = {https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf},
	urldate = {2021-12-23},
	publisher = {Carnegie-Mellon University. Department of Computer Scienc},
	author = {Shewchuk, Jonathan Richard},
	year = {1994},
	file = {Shewchuk - 1994 - An Introduction to the Conjugate Gradient Method W.pdf:/home/david/Zotero/storage/TUAWAFB2/Shewchuk - 1994 - An Introduction to the Conjugate Gradient Method W.pdf:application/pdf},
}

@article{clason_regularization_2021,
	title = {Regularization of {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2001.00617},
	abstract = {These lecture notes for a graduate class present the regularization theory for linear and nonlinear ill-posed operator equations in Hilbert spaces. Covered are the general framework of regularization methods and their analysis via spectral filters as well as the concrete examples of Tikhonov regularization, Landweber iteration, regularization by discretization for linear inverse problems. In the nonlinear setting, Tikhonov regularization and iterative regularization (Landweber, Levenberg-Marquardt, and iteratively regularized Gau\{{\textbackslash}ss\}-Newton methods) are discussed. The necessary background from functional analysis is also briefly summarized. The notes end with a brief outlook to statistical inverse problems from both a frequentist and a Bayesian point of view.},
	language = {en},
	urldate = {2021-12-27},
	journal = {arXiv:2001.00617 [cs, math]},
	author = {Clason, Christian},
	month = feb,
	year = {2021},
	note = {arXiv: 2001.00617},
	keywords = {Mathematics - Functional Analysis, Mathematics - Numerical Analysis},
	file = {Clason - 2021 - Regularization of Inverse Problems.pdf:/home/david/Zotero/storage/PGBIJLFG/Clason - 2021 - Regularization of Inverse Problems.pdf:application/pdf},
}

@article{tibshirani_lasso_2013,
	title = {The lasso problem and uniqueness},
	volume = {7},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/The-lasso-problem-and-uniqueness/10.1214/13-EJS815.full},
	doi = {10.1214/13-EJS815},
	abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p {\textgreater} n, the lasso criterion is not strictly convex, and hence it may not have a unique minimizer. An important question is: when is the lasso solution well-deﬁned (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to 1 penalized minimization problems over a wide range of loss functions.},
	language = {en},
	number = {none},
	urldate = {2021-12-27},
	journal = {Electronic Journal of Statistics},
	author = {Tibshirani, Ryan J.},
	month = jan,
	year = {2013},
	file = {Tibshirani - 2013 - The lasso problem and uniqueness.pdf:/home/david/Zotero/storage/EEJWPKG5/Tibshirani - 2013 - The lasso problem and uniqueness.pdf:application/pdf},
}

@article{tao_local_2015,
	title = {Local {Linear} {Convergence} of {ISTA} and {FISTA} on the {LASSO} {Problem}},
	url = {http://arxiv.org/abs/1501.02888},
	abstract = {We establish local linear convergence bounds for the ISTA and FISTA iterations on the model LASSO problem. We show that FISTA can be viewed as an accelerated ISTA process. Using a spectral analysis, we show that, when close enough to the solution, both iterations converge linearly, but FISTA slows down compared to ISTA, making it advantageous to switch to ISTA toward the end of the iteration processs. We illustrate the results with some synthetic numerical examples.},
	language = {en},
	urldate = {2021-12-27},
	journal = {arXiv:1501.02888 [math]},
	author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02888},
	keywords = {Mathematics - Optimization and Control},
	file = {Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:/home/david/Zotero/storage/DEREK838/Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:application/pdf},
}

@article{urimi_image_nodate,
	title = {{IMAGE} {RECONSTRUCTION} {TECHNIQUES} and {MEASURE} {OF} {QUALITY}: {CLASSICAL} vs. {MODERN} {APPROACHES}},
	language = {en},
	author = {Urimi, Lakshmi P},
	pages = {52},
	file = {Urimi - IMAGE RECONSTRUCTION TECHNIQUES and MEASURE OF QUA.pdf:/home/david/Zotero/storage/ANRHK82E/Urimi - IMAGE RECONSTRUCTION TECHNIQUES and MEASURE OF QUA.pdf:application/pdf},
}

@misc{noauthor_root-mean-square_2021,
	title = {Root-mean-square deviation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Root-mean-square_deviation&oldid=1037360077},
	abstract = {The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.
RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = aug,
	year = {2021},
	note = {Page Version ID: 1037360077},
	file = {Snapshot:/home/david/Zotero/storage/GKPR4G23/index.html:text/html},
}

@misc{noauthor_peak_2021,
	title = {Peak signal-to-noise ratio},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Peak_signal-to-noise_ratio&oldid=1062145991},
	abstract = {Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.
PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1062145991},
	file = {Snapshot:/home/david/Zotero/storage/P437AEZH/index.html:text/html},
}

@misc{noauthor_structural_2021,
	title = {Structural similarity},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Structural_similarity&oldid=1053007931},
	abstract = {The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos.  SSIM is used for measuring the similarity between two images. The SSIM index is a full reference metric; in other words, the measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as reference.
SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms.  The difference with other techniques such as MSE or PSNR is that these approaches estimate absolute errors.  Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions (in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become less visible where there is significant activity or "texture" in the image.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = nov,
	year = {2021},
	note = {Page Version ID: 1053007931},
	file = {Snapshot:/home/david/Zotero/storage/DJXTI8CV/index.html:text/html},
}

@article{lin_zhang_fsim_2011,
	title = {{FSIM}: {A} {Feature} {Similarity} {Index} for {Image} {Quality} {Assessment}},
	volume = {20},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{FSIM}},
	url = {http://ieeexplore.ieee.org/document/5705575/},
	doi = {10.1109/TIP.2011.2109730},
	abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural-similarity (SSIM) index brings IQA from pixel-based stage to structure-based stage. In this paper, a novel feature-similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS’ perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
	language = {en},
	number = {8},
	urldate = {2022-01-04},
	journal = {IEEE Transactions on Image Processing},
	author = {{Lin Zhang} and {Lei Zhang} and {Xuanqin Mou} and Zhang, D.},
	month = aug,
	year = {2011},
	pages = {2378--2386},
	file = {Lin Zhang et al. - 2011 - FSIM A Feature Similarity Index for Image Quality.pdf:/home/david/Zotero/storage/N3WVMQ7Z/Lin Zhang et al. - 2011 - FSIM A Feature Similarity Index for Image Quality.pdf:application/pdf},
}

@misc{noauthor_image_2022,
	title = {Image {Similarity} {Measures}},
	copyright = {MIT},
	url = {https://github.com/up42/image-similarity-measures},
	abstract = {:chart\_with\_upwards\_trend: Implementation of eight evaluation metrics to access the similarity between two images. The eight metrics are as follows: RMSE, PSNR, SSIM, ISSM, FSIM, SRE, SAM, and UIQ.},
	urldate = {2022-01-04},
	publisher = {UP42},
	month = jan,
	year = {2022},
	note = {original-date: 2020-04-06T07:27:49Z},
	keywords = {evaluation-metrics, image, machine-learning, metrics},
}

@article{mittal_making_2013,
	title = {Making a “{Completely} {Blind}” {Image} {Quality} {Analyzer}},
	volume = {20},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/6353522/},
	doi = {10.1109/LSP.2012.2227726},
	abstract = {An important aim of research on the blind image quality assessment (IQA) problem is to devise perceptual models that can predict the quality of distorted images with as little prior knowledge of the images or their distortions as possible. Current state-of-the-art ‘general purpose’ no reference (NR) IQA algorithms require knowledge about anticipated distortions in the form of training examples and corresponding human opinion scores. However we have recently derived a blind IQA model that only makes use of measurable deviations from statistical regularities observed in natural images, without training on human-rated distorted images, and, indeed without any exposure to distorted images. Thus, it is ‘completely blind.’ The new IQA model, which we call the Natural Image Quality Evaluator (NIQE) is based on the construction of a ‘quality aware’ collection of statistical features based on a simple and successful space domain natural scene statistic (NSS) model. These features are derived from a corpus of natural, undistorted images. Experimental results show that the new index delivers performance comparable to top performing NR IQA models that require training on large databases of human opinions of distorted images. A software release is available at:http://live.ece.utexas.edu/research/quality/niqe release.zip.},
	language = {en},
	number = {3},
	urldate = {2022-01-04},
	journal = {IEEE Signal Processing Letters},
	author = {Mittal, A. and Soundararajan, R. and Bovik, A. C.},
	month = mar,
	year = {2013},
	pages = {209--212},
	file = {Mittal et al. - 2013 - Making a “Completely Blind” Image Quality Analyzer.pdf:/home/david/Zotero/storage/J7ZH6ASK/Mittal et al. - 2013 - Making a “Completely Blind” Image Quality Analyzer.pdf:application/pdf},
}

@inproceedings{venkatanath_n_blind_2015,
	address = {Mumbai, India},
	title = {Blind image quality evaluation using perception based features},
	isbn = {978-1-4799-6619-6},
	url = {http://ieeexplore.ieee.org/document/7084843/},
	doi = {10.1109/NCC.2015.7084843},
	abstract = {This paper proposes a novel no-reference Perception-based Image QUality Evaluator (PIQUE) for realworld imagery. A majority of the existing methods for blind image quality assessment rely on opinion-based supervised learning for quality score prediction. Unlike these methods, we propose an opinion unaware methodology that attempts to quantify distortion without the need for any training data. Our method relies on extracting local features for predicting quality. Additionally, to mimic human behavior, we estimate quality only from perceptually signiﬁcant spatial regions. Further, the choice of our features enables us to generate a ﬁne-grained block level distortion map. Our algorithm is competitive with the state-of-the-art based on evaluation over several popular datasets including LIVE IQA, TID \& CSIQ. Finally, our algorithm has low computational complexity despite working at the block-level.},
	language = {en},
	urldate = {2022-01-04},
	booktitle = {2015 {Twenty} {First} {National} {Conference} on {Communications} ({NCC})},
	publisher = {IEEE},
	author = {{Venkatanath N} and {Praneeth D} and {Maruthi Chandrasekhar Bh} and Channappayya, Sumohana S. and Medasani, Swarup S.},
	month = feb,
	year = {2015},
	pages = {1--6},
	file = {Venkatanath N et al. - 2015 - Blind image quality evaluation using perception ba.pdf:/home/david/Zotero/storage/5W5E6EB7/Venkatanath N et al. - 2015 - Blind image quality evaluation using perception ba.pdf:application/pdf},
}

@inproceedings{samajdar_analysis_2015,
	address = {New Delhi},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Analysis and {Evaluation} of {Image} {Quality} {Metrics}},
	isbn = {978-81-322-2247-7},
	doi = {10.1007/978-81-322-2247-7_38},
	abstract = {Image Quality Assessment (IQA) is a very difficult task, yet highly important characteristic for evaluation of the image quality. Widely popular IQA techniques, belonging to objective fidelity, like Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR) or subjective fidelity which corresponds to the human visual system (HVS), like, Universal Quality Index (UQI), Structural SIMilarity (SSIM), Feature SIMilarity (FSIM), Feature SIMilarity for color images (FSIMc), Gradient Magnitude Similarity (GSM) have been discussed in this paper. Also quality measured on basis of degradation model and Noise Quality Measure (NQM) has been discussed. Experiments have been conducted on IVC database available online at http://www.irccyn.ec-nantes.fr/ivcdb/ and verified from the CSIQ database and LAR database available online at http://vision.okstate.edu/?loc=csiq and http://www.irccyn.ec-nantes.fr/{\textasciitilde}autrusse/Databases/LAR/. On the basis of the obtained values judgements about the image distortion and hence the optimum image quality metric has been decided. It has been found from all the experiments conducted that FSIM is the best metric for the JPEG, JPEG2000, blur and LAR whereas UQI failed to give better results for all except JPEG2000.},
	language = {en},
	booktitle = {Information {Systems} {Design} and {Intelligent} {Applications}},
	publisher = {Springer India},
	author = {Samajdar, Tina and Quraishi, Md. Iqbal},
	editor = {Mandal, J. K. and Satapathy, Suresh Chandra and Kumar Sanyal, Manas and Sarkar, Partha Pratim and Mukhopadhyay, Anirban},
	year = {2015},
	keywords = {FSIM, FSIMc, GSM, HVS, IQA, MSE, NQM, PSNR, SSIM, UQI},
	pages = {369--378},
}

@misc{ballard_making_2020,
	title = {Making {Matplotlib} {Beautiful} {By} {Default}},
	url = {https://towardsdatascience.com/making-matplotlib-beautiful-by-default-d0d41e3534fd},
	abstract = {Use Seaborn to control Matplotlib defaults (and forget that shade of blue forever)},
	language = {en},
	urldate = {2022-01-13},
	journal = {Medium},
	author = {Ballard, Callum},
	month = may,
	year = {2020},
}

@misc{boston_university_tutorial_2016,
	title = {Tutorial: {Basics} of {Information} {Design} for {Scientific} {Figures}},
	shorttitle = {Tutorial},
	url = {https://www.youtube.com/watch?v=Lb4uG4rIwPA},
	abstract = {In this video, Kelly Krause, Creative Director at Nature, presented a short tutorial session on best practices for design of scientific figures for publication. Kelly spoke at an event sponsored by the Office of the Vice President and Associate Provost for Research. For more information, please see the video from Kelly’s overview of strategies for communicating science through visualization in a publishing context.

February 26, 2016},
	urldate = {2022-01-13},
	author = {{Boston University}},
	month = mar,
	year = {2016},
}

@book{press_fortran_1996,
	address = {Cambridge [England] ; New York},
	edition = {2nd ed},
	title = {{FORTRAN} numerical recipes},
	isbn = {978-0-521-43064-7 978-0-521-57439-6},
	language = {en},
	publisher = {Cambridge University Press},
	editor = {Press, William H.},
	year = {1996},
	keywords = {Computer programs, FORTRAN (Computer program language), Mathematics Computer programs, Numerical analysis, Science},
	file = {Press - 1996 - FORTRAN numerical recipes.pdf:/home/david/Zotero/storage/P4IRRI7H/Press - 1996 - FORTRAN numerical recipes.pdf:application/pdf},
}

@book{levakhina_three-dimensional_2014-1,
	address = {Wiesbaden},
	title = {Three-{Dimensional} {Digital} {Tomosynthesis}: {Iterative} {Reconstruction}, {Artifact} {Reduction} and {Alternative} {Acquisition} {Geometry}},
	isbn = {978-3-658-05696-4 978-3-658-05697-1},
	shorttitle = {Three-{Dimensional} {Digital} {Tomosynthesis}},
	url = {http://link.springer.com/10.1007/978-3-658-05697-1},
	language = {en},
	urldate = {2022-01-28},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Levakhina, Yulia},
	year = {2014},
	doi = {10.1007/978-3-658-05697-1},
	file = {Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/BCFZIF4A/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf},
}

@article{vogelgesang_semi-discrete_nodate,
	title = {Semi-{Discrete} {Iteration} {Methods} in {X}-{Ray} {Tomography}},
	language = {en},
	author = {Vogelgesang, Jonas},
	pages = {114},
	file = {Vogelgesang - Semi-Discrete Iteration Methods in X-Ray Tomograph.pdf:/home/david/Zotero/storage/USKLE36L/Vogelgesang - Semi-Discrete Iteration Methods in X-Ray Tomograph.pdf:application/pdf},
}

@inproceedings{horbelt_spline_2000,
	address = {Istanbul, Turkey},
	title = {Spline kernels for continuous-space image processing},
	volume = {4},
	isbn = {978-0-7803-6293-2},
	url = {http://ieeexplore.ieee.org/document/859272/},
	doi = {10.1109/ICASSP.2000.859272},
	abstract = {We present an explicit formula for spline kernels; these are deﬁned as the convolution of several B-splines of variable widths hi and degrees ni. The spline kernels are useful for continuous signal processing algorithms that involve Bspline inner-products or the convolution of several spline basis functions. We apply our results to the derivation of spline-based algorithms for two classes of problems. The ﬁrst is the resizing of images with arbitrary scaling factors. The second is the computation of the Radon transform and of its inverse; in particular, we present a new spline-based version of the ﬁltered backprojection algorithm for tomographic reconstruction. In both cases, our explicit kernel formula allows for the use of high-degree splines; these oﬀer better approximation performance than the conventional lower-order formulations (e.g., piecewise constant or piecewise linear models).},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100})},
	publisher = {IEEE},
	author = {Horbelt, S. and Munoz, A. and Blu, T. and Unser, M.},
	year = {2000},
	pages = {2191--2194},
	file = {Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:/home/david/Zotero/storage/7279UY5X/Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:application/pdf},
}

@inproceedings{horbelt_spline_2000-1,
	address = {Istanbul, Turkey},
	title = {Spline kernels for continuous-space image processing},
	volume = {4},
	isbn = {978-0-7803-6293-2},
	url = {http://ieeexplore.ieee.org/document/859272/},
	doi = {10.1109/ICASSP.2000.859272},
	abstract = {We present an explicit formula for spline kernels; these are deﬁned as the convolution of several B-splines of variable widths hi and degrees ni. The spline kernels are useful for continuous signal processing algorithms that involve Bspline inner-products or the convolution of several spline basis functions. We apply our results to the derivation of spline-based algorithms for two classes of problems. The ﬁrst is the resizing of images with arbitrary scaling factors. The second is the computation of the Radon transform and of its inverse; in particular, we present a new spline-based version of the ﬁltered backprojection algorithm for tomographic reconstruction. In both cases, our explicit kernel formula allows for the use of high-degree splines; these oﬀer better approximation performance than the conventional lower-order formulations (e.g., piecewise constant or piecewise linear models).},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100})},
	publisher = {IEEE},
	author = {Horbelt, S. and Munoz, A. and Blu, T. and Unser, M.},
	year = {2000},
	pages = {2191--2194},
	file = {Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:/home/david/Zotero/storage/ME9RIKEX/Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:application/pdf},
}

@inproceedings{nilchian_differential_2012,
	title = {Differential phase-contrast {X}-ray computed tomography: {From} model discretization to image reconstruction},
	shorttitle = {Differential phase-contrast {X}-ray computed tomography},
	doi = {10.1109/ISBI.2012.6235491},
	abstract = {Our contribution in this paper is two fold. First, we propose a novel discretization of the forward model for differential phase-contrast imaging that uses B-spline basis functions. The approach yields a fast and accurate algorithm for implementing the forward model, which is based on the first derivative of the Radon transform. Second, as an alternative to the FBP-like approaches that are currently used in practice, we present an iterative reconstruction algorithm that remains more faithful to the data when the number of projections dwindles. Since the reconstruction is an ill-posed problem, we impose a total-variation (TV) regularization constraint. We propose to solve the reconstruction problem using the alternating direction method of multipliers (ADMM). A specificity of our system is the use of a preconditioner that improves the convergence rate of the linear solver in ADMM. Our experiments on test data suggest that our method can achieve the same quality as the standard direct reconstruction, while using only one-third of the projection data. We also find that the approach is much faster than the standard algorithms (ISTA and FISTA) that are typically used for solving linear inverse problems subject to the TV regularization constraint.},
	booktitle = {2012 9th {IEEE} {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Nilchian, Masih and Unser, Michael},
	month = may,
	year = {2012},
	note = {ISSN: 1945-8452},
	keywords = {Computed tomography, Image reconstruction, X-ray imaging, alternating direction method of multipliers (ADMM), differential phase-contrast imaging, filtered back projection (FBP), preconditioned conjugate gradient method, Radon transform, Splines (mathematics), Transforms, Vectors},
	pages = {90--93},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/YF9WSLLA/Nilchian and Unser - 2012 - Differential phase-contrast X-ray computed tomogra.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/Q99JW7GY/6235491.html:text/html},
}

@book{have_environmental_2006,
	address = {Paris},
	series = {Ethics series},
	title = {Environmental ethics and international policy},
	isbn = {978-92-3-104039-9},
	language = {en},
	publisher = {UNESCO},
	editor = {Have, Henk ten},
	year = {2006},
	file = {Have - 2006 - Environmental ethics and international policy.pdf:/home/david/Zotero/storage/PQMRI8MD/Have - 2006 - Environmental ethics and international policy.pdf:application/pdf},
}

@article{asaro_what_2006,
	title = {What {Should} {We} {Want} {From} a {Robot} {Ethic}?},
	volume = {6},
	abstract = {There are at least three things we might mean by “ethics in robotics”: the ethical systems built into robots, the ethics of people who design and use robots, and the ethics of how people treat robots. This paper argues that the best approach to robot ethics is one which addresses all three of these, and to do this it ought to consider robots as socio-technical systems. By so doing, it is possible to think of a continuum of agency that lies between amoral and fully autonomous moral agents. Thus, robots might move gradually along this continuum as they acquire greater capabilities and ethical sophistication. It also argues that many of the issues regarding the distribution of responsibility in complex socio-technical systems might best be addressed by looking to legal theory, rather than moral theory. This is because our overarching interest in robot ethics ought to be the practical one of preventing robots from doing harm, as well as preventing humans from unjustly avoiding responsibility for their actions.},
	language = {en},
	author = {Asaro, Peter M},
	year = {2006},
	pages = {8},
	file = {Asaro - What Should We Want From a Robot Ethic.pdf:/home/david/Zotero/storage/V65GKW3E/Asaro - What Should We Want From a Robot Ethic.pdf:application/pdf},
}

@article{grunwald_armin_technology_1999,
	title = {Technology {Assessment} or {Ethics} of {Technology}?},
	issn = {1783-1431, 1783-1431},
	url = {https://doi.org/10.2143/EP.6.2.505355},
	doi = {10.2143/EP.6.2.505355},
	language = {en},
	number = {2},
	urldate = {2022-01-31},
	journal = {Ethical Perspectives},
	author = {{GRUNWALD, Armin}},
	year = {1999},
	pages = {170--182},
	file = {GRUNWALD, Armin - 1999 - Technology Assessment or Ethics of Technology.pdf:/home/david/Zotero/storage/RJYV2TBQ/GRUNWALD, Armin - 1999 - Technology Assessment or Ethics of Technology.pdf:application/pdf},
}

@article{jonas_toward_2014,
	title = {Toward a {Philosophy} of {Technology}},
	language = {en},
	author = {Jonas, Hans},
	year = {2014},
	pages = {10},
	file = {Jonas - Toward a Philosophy of Technology.pdf:/home/david/Zotero/storage/SANG3Z6P/Jonas - Toward a Philosophy of Technology.pdf:application/pdf},
}

@article{hansson_what_2004,
	title = {What is {Philosophy} of {Risk}?},
	language = {en},
	author = {Hansson, Sven},
	year = {2004},
	pages = {20},
	file = {SPT v8n1 - Philosophical Perspectives on Risk.pdf:/home/david/Zotero/storage/7QEJG67W/SPT v8n1 - Philosophical Perspectives on Risk.pdf:application/pdf},
}

@article{silverstein_international_1987,
	title = {International {Journal} of {Technology} {Assessment} in {Health} {Care}},
	volume = {31},
	issn = {1529-8795},
	url = {http://muse.jhu.edu/content/crossref/journals/perspectives_in_biology_and_medicine/v031/31.1.silverstein.html},
	doi = {10.1353/pbm.1987.0026},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Perspectives in Biology and Medicine},
	author = {Silverstein, Marc D.},
	year = {1987},
	pages = {151--152},
	file = {Silverstein - 1987 - International Journal of Technology Assessment in .pdf:/home/david/Zotero/storage/XWFQ3GX8/Silverstein - 1987 - International Journal of Technology Assessment in .pdf:application/pdf},
}

@book{resnik_ethics_2005,
	title = {The {Ethics} of {Science}: an {Introduction}.},
	isbn = {978-0-203-97906-8},
	shorttitle = {The {Ethics} of {Science}},
	url = {http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=242153},
	abstract = {Ethics of Science is a comprehensive and student-friendly introduction to the study of ethics in science and scientific research. The book covers: * Science and Ethics * Ethical Theory and Applications * Science as a Profession * Standards of Ethical Conduct in Science * Objectivity in Research * Ethical Issues in the Laboratory * The Scientist in Society * Toward a More Ethical Science * Actual case studies include: Baltimore Affair * cold fusion * Milikan's oil drop experiments * human and animal cloning * Cold War experiments * Strategic Defence Initiative * the Challenger accident * Tobacco Research.},
	language = {en},
	urldate = {2022-01-31},
	author = {Resnik, David B},
	year = {2005},
	note = {OCLC: 1048580079},
	file = {Resnik - 2005 - The Ethics of Science an Introduction..pdf:/home/david/Zotero/storage/7RSFKU4E/Resnik - 2005 - The Ethics of Science an Introduction..pdf:application/pdf},
}

@article{van_de_poel_problem_2012,
	title = {The {Problem} of {Many} {Hands}: {Climate} {Change} as an {Example}},
	volume = {18},
	issn = {1353-3452, 1471-5546},
	shorttitle = {The {Problem} of {Many} {Hands}},
	url = {http://link.springer.com/10.1007/s11948-011-9276-0},
	doi = {10.1007/s11948-011-9276-0},
	abstract = {In some situations in which undesirable collective effects occur, it is very hard, if not impossible, to hold any individual reasonably responsible. Such a situation may be referred to as the problem of many hands. In this paper we investigate how the problem of many hands can best be understood and why, and when, it exactly constitutes a problem. After analyzing climate change as an example, we propose to deﬁne the problem of many hands as the occurrence of a gap in the distribution of responsibility that may be considered morally problematic. Whether a gap is morally problematic, we suggest, depends on the reasons why responsibility is distributed. This, in turn, depends, at least in part, on the sense of responsibility employed, a main distinction being that between backward-looking and forward-looking responsibility.},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Science and Engineering Ethics},
	author = {van de Poel, Ibo and Nihlén Fahlquist, Jessica and Doorn, Neelke and Zwart, Sjoerd and Royakkers, Lambèr},
	month = mar,
	year = {2012},
	pages = {49--67},
	file = {van de Poel et al. - 2012 - The Problem of Many Hands Climate Change as an Ex.pdf:/home/david/Zotero/storage/PSDF9P47/van de Poel et al. - 2012 - The Problem of Many Hands Climate Change as an Ex.pdf:application/pdf},
}

@article{veruggio_roboethics_2006,
	title = {Roboethics: a {Bottom}-up {Interdisciplinary} {Discourse} in the {Field} of {Ap}- plied {Ethics} in {Robotics}},
	volume = {6},
	abstract = {This paper deals with the birth of Roboethics. Roboethics is the ethics inspiring the design, development and employment of Intelligent Machines. Roboethics shares many 'sensitive areas' with Computer Ethics, Information Ethics and Bioethics. It investigates the social and ethical problems due to the effects of the Second and Third Industrial Revolutions in the Humans/Machines interaction’s domain. Urged by the responsibilities involved in their professions, an increasing number of roboticists from all over the world have started - in cross-cultural collaboration with scholars of Humanities – to thoroughly develop the Roboethics, the applied ethics that should inspire the design, manufacturing and use of robots. The result is the Roboethics Roadmap.},
	language = {en},
	author = {Veruggio, Gianmarco and Operto, Fiorella},
	year = {2006},
	pages = {7},
	file = {Veruggio and Operto - Roboethics a Bottom-up Interdisciplinary Discours.pdf:/home/david/Zotero/storage/G42TZYC9/Veruggio and Operto - Roboethics a Bottom-up Interdisciplinary Discours.pdf:application/pdf},
}

@misc{bynum_computer_2001,
	title = {Computer and {Information} {Ethics}},
	author = {Bynum, T},
	year = {2001},
	file = {Bynum - 2001 - Computer and Information Ethics.pdf:/home/david/Zotero/storage/DHJZKIHS/Bynum - 2001 - Computer and Information Ethics.pdf:application/pdf},
}

@misc{singer_introduction_1985,
	title = {Introduction to {Ethics}},
	author = {Singer, Peter},
	year = {1985},
	file = {Singer - 1985 - Introduction to Ethics.pdf:/home/david/Zotero/storage/7QNL8B4R/Singer - 1985 - Introduction to Ethics.pdf:application/pdf},
}

@incollection{jackson_introduction_2013,
	title = {An {Introduction} in {Bioethics}},
	booktitle = {Medical {Law}: {Text}, {Cases}, and {Materials}},
	author = {Jackson, Emily},
	year = {2013},
	file = {introduction bioethics jackson.pdf:/home/david/Zotero/storage/24IJLRHP/introduction bioethics jackson.pdf:application/pdf},
}

@article{asaro_what_nodate,
	title = {What {Should} {We} {Want} {From} a {Robot} {Ethic}?},
	volume = {6},
	abstract = {There are at least three things we might mean by “ethics in robotics”: the ethical systems built into robots, the ethics of people who design and use robots, and the ethics of how people treat robots. This paper argues that the best approach to robot ethics is one which addresses all three of these, and to do this it ought to consider robots as socio-technical systems. By so doing, it is possible to think of a continuum of agency that lies between amoral and fully autonomous moral agents. Thus, robots might move gradually along this continuum as they acquire greater capabilities and ethical sophistication. It also argues that many of the issues regarding the distribution of responsibility in complex socio-technical systems might best be addressed by looking to legal theory, rather than moral theory. This is because our overarching interest in robot ethics ought to be the practical one of preventing robots from doing harm, as well as preventing humans from unjustly avoiding responsibility for their actions.},
	language = {en},
	author = {Asaro, Peter M},
	pages = {8},
	file = {Asaro - What Should We Want From a Robot Ethic.pdf:/home/david/Zotero/storage/396BW5F9/Asaro - What Should We Want From a Robot Ethic.pdf:application/pdf},
}

@article{doorn_responsibility_2012,
	title = {Responsibility {Ascriptions} in {Technology} {Development} and {Engineering}: {Three} {Perspectives}},
	volume = {18},
	issn = {1353-3452, 1471-5546},
	shorttitle = {Responsibility {Ascriptions} in {Technology} {Development} and {Engineering}},
	url = {http://link.springer.com/10.1007/s11948-009-9189-3},
	doi = {10.1007/s11948-009-9189-3},
	abstract = {In the last decades increasing attention is paid to the topic of responsibility in technology development and engineering. The discussion of this topic is often guided by questions related to liability and blameworthiness. Recent discussions in engineering ethics call for a reconsideration of the traditional quest for responsibility. Rather than on alleged wrongdoing and blaming, the focus should shift to more socially responsible engineering, some authors argue. The present paper aims at exploring the different approaches to responsibility in order to see which one is most appropriate to apply to engineering and technology development. Using the example of the development of a new sewage water treatment technology, the paper shows how different approaches for ascribing responsibilities have different implications for engineering practice in general, and R\&D or technological design in particular. It was found that there was a tension between the demands that follow from these different approaches, most notably between efﬁcacy and fairness. Although the consequentialist approach with its efﬁcacy criterion turned out to be most powerful, it was also shown that the fairness of responsibility ascriptions should somehow be taken into account. It is proposed to look for alternative, more procedural ways to approach the fairness of responsibility ascriptions.},
	language = {en},
	number = {1},
	urldate = {2022-01-31},
	journal = {Science and Engineering Ethics},
	author = {Doorn, Neelke},
	month = mar,
	year = {2012},
	pages = {69--90},
	file = {Doorn - 2012 - Responsibility Ascriptions in Technology Developme.pdf:/home/david/Zotero/storage/FEWM5SRU/Doorn - 2012 - Responsibility Ascriptions in Technology Developme.pdf:application/pdf},
}

@article{thompson_responsibility_2014,
	title = {Responsibility for {Failures} of {Government}: {The} {Problem} of {Many} {Hands}},
	volume = {44},
	issn = {0275-0740, 1552-3357},
	shorttitle = {Responsibility for {Failures} of {Government}},
	url = {http://journals.sagepub.com/doi/10.1177/0275074014524013},
	doi = {10.1177/0275074014524013},
	abstract = {The problem of many hands—the difficulty of assigning responsibility in organizations in which many different individuals contribute to decisions and policies—stands in the way of investigating and correcting the failures of government. The problem can be mitigated by giving greater attention to the design of processes of organizational responsibility. An independent investigation can identify both the individual actions and the structural defects that contributed to an organizational failure. Then, specific individuals can be designated as overseers, who are held responsible for monitoring the structure and making changes as necessary. Three cases—the official responses to terrorist attacks on the World Trade Center in 2001, the Deepwater Horizon oil spill in 2010, and the financial crisis that began in 2007—illustrate how this prospective approach of designing responsibility could work in practice.},
	language = {en},
	number = {3},
	urldate = {2022-01-31},
	journal = {The American Review of Public Administration},
	author = {Thompson, Dennis F.},
	month = may,
	year = {2014},
	pages = {259--273},
	file = {Thompson - 2014 - Responsibility for Failures of Government The Pro.pdf:/home/david/Zotero/storage/8E2FAZG5/Thompson - 2014 - Responsibility for Failures of Government The Pro.pdf:application/pdf},
}

@article{zandvoort_knowledge_2005,
	title = {Knowledge, {Risk}, and {Liability}. {Analysis} of a {Discussion} {Continuing} {Within} {Science} and {Technology}},
	journal = {Poznan Studies in the Philosophy of the Sciences and the Humanities},
	author = {Zandvoort, Henk},
	month = nov,
	year = {2005},
	pages = {469--498},
}

@misc{jidiette_moral_2014,
	title = {Moral {Responsibility} and {Free} {Will}},
	url = {https://hughjidiette.wordpress.com/2014/06/11/221/},
	abstract = {Typically people associate free will as a condition necessary for moral responsibility.  As John Martin Fischer says: Some philosophers do not distinguish between freedom and moral responsibility. …},
	language = {en},
	urldate = {2022-01-31},
	journal = {Hugh Jidiette},
	author = {Jidiette, Hugh},
	month = jun,
	year = {2014},
	file = {Snapshot:/home/david/Zotero/storage/G9SUPA9L/221.html:text/html},
}

@misc{hownot2code_space_2016,
	title = {A space error: \$370 million for an integer overflow},
	shorttitle = {A space error},
	url = {https://hownot2code.com/2016/09/02/a-space-error-370-million-for-an-integer-overflow/},
	abstract = {The article investigates and reveals the details of a catastrophe that occured in 1996 with Ariane 5 rocket.},
	language = {en},
	urldate = {2022-02-02},
	journal = {How Not To Code},
	author = {{HOWNOT2CODE}},
	month = sep,
	year = {2016},
	file = {Snapshot:/home/david/Zotero/storage/XTQPKV9Y/a-space-error-370-million-for-an-integer-overflow.html:text/html},
}

@misc{noauthor_risks_2012,
	title = {Risks of {Radiation}},
	url = {https://radiology.ucsf.edu/patient-care/patient-safety/radiation-safety/risks-of-radiation},
	abstract = {Weighing the Radiation Risks of CT, X-ray and Other Imaging When X-ray radiation is absorbed within our bodies, it can damage molecular structures and potentially cause harm. Very high doses of radiation cause damage to human cells, as evidenced by skin burns, loss of hair, and increased incidence of cancer. Because high doses of radiation can cause cancer, it is therefore generally assumed that low doses may also cause cancer.},
	language = {en},
	urldate = {2022-02-02},
	journal = {UCSF Radiology},
	month = may,
	year = {2012},
	file = {Snapshot:/home/david/Zotero/storage/BS7VYB3V/risks-of-radiation.html:text/html},
}

@misc{noauthor_log4shell_2022,
	title = {{Log4Shell}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Log4Shell&oldid=1068439279},
	abstract = {Log4Shell (CVE-2021-44228) was a zero-day vulnerability in Log4j, a popular Java logging framework, involving arbitrary code execution. The vulnerability has existed unnoticed since 2013 and was privately disclosed to the Apache Software Foundation, of which Log4j is a project, by Chen Zhaojun of Alibaba Cloud's security team on 24 November 2021, and was publicly disclosed on 9 December 2021. Apache gave Log4Shell a CVSS severity rating of 10, the highest available score. The exploit is simple to execute and is estimated to affect hundreds of millions of devices.The vulnerability takes advantage of Log4j's allowing requests to arbitrary LDAP and JNDI servers, allowing attackers to execute arbitrary Java code on a server or other computer, or leak sensitive information. A list of its affected software projects has been published by the Apache Security Team. Affected commercial services include Amazon Web Services, Cloudflare, iCloud, Minecraft: Java Edition, Steam, Tencent QQ and many others. According to Wiz and EY, the vulnerability affected 93\% of enterprise cloud environments.Experts described Log4Shell as the largest vulnerability ever; LunaSec characterized it as "a design failure of catastrophic proportions", Tenable said the exploit was "the single biggest, most critical vulnerability ever", Ars Technica called it "arguably the most severe vulnerability ever" and The Washington Post said that descriptions by security professionals "border on the apocalyptic".},
	language = {en},
	urldate = {2022-02-02},
	journal = {Wikipedia},
	month = jan,
	year = {2022},
	note = {Page Version ID: 1068439279},
}

@article{winkler_numerical_1993,
	title = {Numerical recipes in {C}: {The} art of scientific computing, second edition},
	volume = {17},
	issn = {01609327},
	shorttitle = {Numerical recipes in {C}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016093279390069F},
	doi = {10.1016/0160-9327(93)90069-F},
	language = {en},
	number = {4},
	urldate = {2022-02-02},
	journal = {Endeavour},
	author = {Winkler, Joab R},
	month = jan,
	year = {1993},
	pages = {201},
	file = {Winkler - 1993 - Numerical recipes in C The art of scientific comp.pdf:/home/david/Zotero/storage/DNR7FMEP/Winkler - 1993 - Numerical recipes in C The art of scientific comp.pdf:application/pdf},
}

@misc{noauthor_libstdc_nodate,
	title = {libstdc++: modified\_bessel\_func.tcc {Source} {File}},
	url = {https://gcc.gnu.org/onlinedocs/libstdc++/libstdc++-html-USERS-4.3/a01938.html},
	urldate = {2022-02-02},
	file = {libstdc++\: modified_bessel_func.tcc Source File:/home/david/Zotero/storage/CQR84E5G/a01938.html:text/html},
}

@book{abramowitz_handbook_1972,
	address = {New York},
	edition = {Unabridged, unaltered and corr. republ. of the 1964 ed},
	series = {Dover books on advanced mathematics},
	title = {Handbook of {Mathematical} {Functions}},
	isbn = {978-0-486-61272-0},
	shorttitle = {Handbook of mathematical functions},
	language = {eng},
	publisher = {Dover publ},
	author = {Abramowitz, Milton and Stegun, Irene A.},
	collaborator = {Conference on mathematical tables and National science foundation and Massachusetts institute of technology},
	year = {1972},
	file = {Abramowitz and Stegun - 1972 - Handbook of mathematical functions with formulas,.pdf:/home/david/Zotero/storage/7Y6SYIXP/Abramowitz and Stegun - 1972 - Handbook of mathematical functions with formulas,.pdf:application/pdf},
}

@misc{noauthor_xmipp_2022,
	title = {Xmipp},
	copyright = {GPL-3.0},
	url = {https://github.com/I2PC/xmipp/blob/57a4d8f7942dcd6cebcffaf7c6c664fdd0c5dbe8/src/xmipp/libraries/reconstruction_cuda/cuda_gpu_reconstruct_fourier.cpp},
	abstract = {Xmipp is a suite of image processing programs, primarily aimed at single-particle 3D electron microscopy.},
	urldate = {2022-02-02},
	publisher = {Instruct Image Processing Center},
	month = jan,
	year = {2022},
	note = {original-date: 2018-06-11T13:30:44Z},
}

@misc{noauthor_xmipp_2022-1,
	title = {Xmipp},
	copyright = {GPL-3.0},
	url = {https://github.com/I2PC/xmipp/blob/57a4d8f7942dcd6cebcffaf7c6c664fdd0c5dbe8/src/xmipp/libraries/reconstruction_starpu/reconstruct_fourier_codelet_reconstruct.cpp},
	abstract = {Xmipp is a suite of image processing programs, primarily aimed at single-particle 3D electron microscopy.},
	urldate = {2022-02-02},
	publisher = {Instruct Image Processing Center},
	month = jan,
	year = {2022},
	note = {original-date: 2018-06-11T13:30:44Z},
}

@article{felsner_phase-sensitive_2018,
	title = {Phase-{Sensitive} {Region}-of-{Interest} {Computed} {Tomography}},
	url = {http://arxiv.org/abs/1805.09528},
	abstract = {X-Ray Phase-Contrast Imaging (PCI) yields absorption, differential phase, and dark-ﬁeld images. Computed Tomography (CT) of grating-based PCI can in principle provide high-resolution soft-tissue contrast. Recently, grating-based PCI took several hurdles towards clinical implementation by addressing, for example, acquisition speed, high X-ray energies, and system vibrations. However, a critical impediment in all grating-based systems lies in limits that constrain the grating diameter to few centimeters.},
	language = {en},
	urldate = {2022-02-05},
	journal = {arXiv:1805.09528 [physics]},
	author = {Felsner, Lina and Berger, Martin and Kaeppler, Sebastian and Bopp, Johannes and Ludwig, Veronika and Weber, Thomas and Pelzer, Georg and Michel, Thilo and Maier, Andreas and Anton, Gisela and Riess, Christian},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09528},
	keywords = {Physics - Medical Physics},
	file = {Felsner et al. - 2018 - Phase-Sensitive Region-of-Interest Computed Tomogr.pdf:/home/david/Zotero/storage/4AEJ7EWY/Felsner et al. - 2018 - Phase-Sensitive Region-of-Interest Computed Tomogr.pdf:application/pdf},
}

@article{bayer_reconstruction_2014,
	title = {Reconstruction of scalar and vectorial components in {X}-ray dark-field tomography},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1321080111},
	doi = {10.1073/pnas.1321080111},
	language = {en},
	number = {35},
	urldate = {2022-02-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bayer, F. L. and Hu, S. and Maier, A. and Weber, T. and Anton, G. and Michel, T. and Riess, C. P.},
	month = sep,
	year = {2014},
	pages = {12699--12704},
	file = {Bayer et al. - 2014 - Reconstruction of scalar and vectorial components .pdf:/home/david/Zotero/storage/G4UGQVME/Bayer et al. - 2014 - Reconstruction of scalar and vectorial components .pdf:application/pdf},
}

@article{lee_machine_2018,
	title = {Machine {Friendly} {Machine} {Learning}: {Interpretation} of {Computed} {Tomography} {Without} {Image} {Reconstruction}},
	shorttitle = {Machine {Friendly} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1812.01068},
	abstract = {Recent advancements in deep learning for automated image processing and classiﬁcation have accelerated many new applications for medical image analysis. However, most deep learning applications have been developed using reconstructed, human-interpretable medical images. While image reconstruction from raw sensor data is required for the creation of medical images, the reconstruction process only uses a partial representation of all the data acquired. Here we report the development of a system to directly process raw computed tomography (CT) data in sinogram-space, bypassing the intermediary step of image reconstruction. Two classiﬁcation tasks were evaluated for their feasibility for sinogram-space machine learning: body region identiﬁcation and intracranial hemorrhage (ICH) detection. Our proposed SinoNet performed favorably compared to conventional reconstructed image-space-based systems for both tasks, regardless of scanning geometries in terms of projections or detectors. Further, SinoNet performed signiﬁcantly better when using sparsely sampled sinograms than conventional networks operating in image-space. As a result, sinogram-space algorithms could be used in ﬁeld settings for binary diagnosis testing, triage, and in clinical settings where low radiation dose is desired. These ﬁndings also demonstrate another strength of deep learning where it can analyze and interpret sinograms that are virtually impossible for human experts.},
	language = {en},
	urldate = {2022-02-05},
	journal = {arXiv:1812.01068 [cs]},
	author = {Lee, Hyunkwang and Huang, Chao and Yune, Sehyo and Tajmir, Shahein H. and Kim, Myeongchan and Do, Synho},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lee et al. - 2018 - Machine Friendly Machine Learning Interpretation .pdf:/home/david/Zotero/storage/7XLGLB3Q/Lee et al. - 2018 - Machine Friendly Machine Learning Interpretation .pdf:application/pdf},
}

@inproceedings{modregger_artifacts_2011,
	address = {Chicago, Illinois, (USA)},
	title = {Artifacts in {X}-ray {Dark}-{Field} {Tomography}},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.3625356},
	doi = {10.1063/1.3625356},
	abstract = {Grating-based x-ray imaging provides three principle kinds of contrast: absorption, phase, and dark-field. Due to the availability of tomographic reconstruction algorithms for the dark-field contrast, it is now possible to take advantage of quantitative scatter information. However, the published algorithm is based on several assumptions that might be violated in reality. We use numerical simulations in order to identify artifacts in the reconstructions, which is crucial for the interpretation of experimental data.},
	language = {en},
	urldate = {2022-02-05},
	author = {Modregger, P. and Wang, Z. and Thuering, T. and Pinzer, B. and Stampanoni, M. and McNulty, Ian and Eyberger, Catherine and Lai, Barry},
	year = {2011},
	pages = {269--272},
	file = {Modregger et al. - 2011 - Artifacts in X-ray Dark-Field Tomography.pdf:/home/david/Zotero/storage/EWSZKLRM/Modregger et al. - 2011 - Artifacts in X-ray Dark-Field Tomography.pdf:application/pdf},
}

@misc{noauthor_carbon_nodate,
	title = {Carbon},
	url = {https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=one-dark&wt=none&l=text%2Fx-c%2B%2Bsrc&width=680&ds=true&dsyoff=20px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false&code=template%253Ctypename%2520data_t%253E%250Adata_t%2520reduce%28const%2520DataConatiner%253Cdata_t%253E%2520a%29%2520%257B%250A%2520%2520%2520%2520data_t%2520sum%2520%253D%2520data_t%280%29%253B%250A%2520%2520%2520%2520for%28int%2520i%253D0%253B%2520i%253Ca.size%28%29%253B%2520%252B%252Bi%29%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520sum%2520%252B%253D%2520a%255Bi%255D%253B%250A%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520return%2520sum%253B%250A%257D},
	abstract = {Carbon is the easiest way to create and share beautiful images of your source code.},
	language = {en},
	urldate = {2022-02-06},
}

@misc{pine_octrefpolacode_2022,
	title = {octref/polacode},
	url = {https://github.com/octref/polacode},
	abstract = {📸 Polaroid for your code},
	urldate = {2022-02-06},
	author = {Pine},
	month = feb,
	year = {2022},
	note = {original-date: 2018-02-09T23:10:07Z},
	keywords = {screenshot, snippets, visual-studio-code, vscode},
}

@article{vogel_tomographic_nodate,
	title = {Tomographic {Reconstruction} beyond {Classical} {X}-ray {CT}},
	language = {de},
	author = {Vogel, Jakob},
	pages = {245},
	file = {Vogel - Tomographic Reconstruction beyond Classical X-ray .pdf:/home/david/Zotero/storage/S6NB8THJ/Vogel - Tomographic Reconstruction beyond Classical X-ray .pdf:application/pdf},
}

@article{latt_cross-platform_2021,
	title = {Cross-platform programming model for many-core lattice {Boltzmann} simulations},
	volume = {16},
	issn = {1932-6203},
	url = {http://arxiv.org/abs/2010.11751},
	doi = {10.1371/journal.pone.0250306},
	abstract = {We present a novel, hardware-agnostic implementation strategy for lattice Boltzmann (LB) simulations, which yields massive performance on homogeneous and heterogeneous many-core platforms. Based solely on C++17 Parallel Algorithms, our approach does not rely on any language extensions, external libraries, vendor-speciﬁc code annotations, or pre-compilation steps. Thanks in particular to a recently proposed GPU back-end to C++17 Parallel Algorithms, it is shown that a single code can compile and reach state-of-the-art performance on both many-core CPU and GPU environments for the solution of a given non trivial ﬂuid dynamics problem. The proposed strategy is tested with six different, commonly used implementation schemes to test the performance impact of memory access patterns on different platforms. Nine different LB collision models are included in the tests and exhibit good performance, demonstrating the versatility of our parallel approach. This work shows that it is less than ever necessary to draw a distinction between research and production software, as a concise and generic LB implementation yields performances comparable to those achievable in a hardware speciﬁc programming language. The results also highlight the gains of performance achieved by modern many-core CPUs and their apparent capability to narrow the gap with the traditionally massively faster GPU platforms. All code is made available to the community in form of the open-source project stlbm, which serves both as a stand-alone simulation software and as a collection of reusable patterns for the acceleration of pre-existing LB codes.},
	language = {en},
	number = {4},
	urldate = {2022-02-09},
	journal = {PLOS ONE},
	author = {Latt, Jonas and Coreixas, Christophe and Beny, Joël},
	month = apr,
	year = {2021},
	note = {arXiv: 2010.11751},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Physics - Computational Physics},
	pages = {e0250306},
	file = {Latt et al. - 2021 - Cross-platform programming model for many-core lat.pdf:/home/david/Zotero/storage/LRMMSKZL/Latt et al. - 2021 - Cross-platform programming model for many-core lat.pdf:application/pdf},
}

@misc{noauthor_educational_lbmaa_aosh_nodate,
	title = {educational\_lbm/aa\_aos.h · master · {UniGeHPFS} / stlbm},
	url = {https://gitlab.com/unigehpfs/stlbm/-/blob/master/educational_lbm/aa_aos.h},
	abstract = {STLBM - Lattice Boltzmann simulation framework based on C++ Parallel Algorithms},
	language = {en},
	urldate = {2022-02-09},
	journal = {GitLab},
	file = {Snapshot:/home/david/Zotero/storage/4ZE2HMCK/aa_aos.html:text/html},
}

@misc{the_manim_community_developers_manim_2022,
	title = {Manim – {Mathematical} {Animation} {Framework}},
	copyright = {MIT},
	url = {https://www.manim.community/},
	abstract = {A community-maintained Python framework for creating mathematical animations.},
	urldate = {2022-02-11},
	author = {{The Manim Community Developers}},
	month = jan,
	year = {2022},
	note = {original-date: 2020-05-19T02:37:13Z},
}

@misc{noauthor_proof_nodate,
	title = {Proof that {Convergent} {Sequences} are {Bounded} - {Mathonline}},
	url = {http://mathonline.wikidot.com/proof-that-convergent-sequences-are-bounded},
	urldate = {2022-02-18},
	file = {Proof that Convergent Sequences are Bounded - Mathonline:/home/david/Zotero/storage/4P74JNKV/proof-that-convergent-sequences-are-bounded.html:text/html},
}

@misc{noauthor_lift-projectlift_2022,
	title = {lift-project/lift},
	copyright = {MIT},
	url = {https://github.com/lift-project/lift},
	abstract = {The Lift programming language and compiler},
	urldate = {2022-02-19},
	publisher = {The Lift Project},
	month = feb,
	year = {2022},
	note = {original-date: 2016-12-04T13:47:20Z},
	keywords = {dsl, gpu, lift, lift-language, performance-portability},
}

@misc{noauthor_skelclskelcl_2021,
	title = {skelcl/skelcl},
	url = {https://github.com/skelcl/skelcl},
	abstract = {SkelCL is a library providing high-level abstractions for alleviated programming of modern parallel heterogeneous systems. SkelCL is a research project developed at the research group parallel and distributed systems at University of Münster which is located in Germany.},
	urldate = {2022-02-19},
	publisher = {The SkelCL research project},
	month = dec,
	year = {2021},
	note = {original-date: 2012-05-25T00:57:49Z},
}

@incollection{petrenko_transformation-based_2018,
	address = {Cham},
	title = {A {Transformation}-{Based} {Approach} to {Developing} {High}-{Performance} {GPU} {Programs}},
	volume = {10742},
	isbn = {978-3-319-74312-7 978-3-319-74313-4},
	url = {http://link.springer.com/10.1007/978-3-319-74313-4_14},
	abstract = {We advocate the use of formal patterns and transformations for programming modern many-core processors like Graphics Processing Units (GPU), as an alternative to the currently used low-level, ad hoc programming approaches like CUDA or OpenCL. Our new contribution is introducing an intermediate level of low-level patterns in order to bridge the abstraction gap between popular high-level patterns (map, fold/reduce, zip, etc.) and imperative, executable code for many-cores. We deﬁne our low-level patterns based on the OpenCL programming model which is portable across parallel architectures of diﬀerent vendors, and we introduce semantics-preserving rewrite rules that transform programs with high-level patterns into programs with low-level patterns, from which executable OpenCL programs are automatically generated. We show that program design decisions and optimizations, which are usually applied ad-hoc by experts, are systematically expressed in our approach as provably-correct transformations for high- and low-level patterns. We evaluate our approach by systematically deriving several differently optimized OpenCL implementations of parallel reduction that achieve performance competitive with OpenCL programs which are manually written and highly tuned by performance experts.},
	language = {en},
	urldate = {2022-02-19},
	booktitle = {Perspectives of {System} {Informatics}},
	publisher = {Springer International Publishing},
	author = {Hagedorn, Bastian and Steuwer, Michel and Gorlatch, Sergei},
	editor = {Petrenko, Alexander K. and Voronkov, Andrei},
	year = {2018},
	doi = {10.1007/978-3-319-74313-4_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {179--195},
	file = {Hagedorn et al. - 2018 - A Transformation-Based Approach to Developing High.pdf:/home/david/Zotero/storage/N44KYINA/Hagedorn et al. - 2018 - A Transformation-Based Approach to Developing High.pdf:application/pdf},
}

@inproceedings{haidl_towards_2017,
	address = {Austin TX USA},
	title = {Towards {Composable} {GPU} {Programming}: {Programming} {GPUs} with {Eager} {Actions} and {Lazy} {Views}},
	isbn = {978-1-4503-4883-6},
	shorttitle = {Towards {Composable} {GPU} {Programming}},
	url = {https://dl.acm.org/doi/10.1145/3026937.3026942},
	doi = {10.1145/3026937.3026942},
	abstract = {In this paper, we advocate a composable approach to programming systems with Graphics Processing Units (GPU): programs are developed as compositions of generic, reusable patterns. Current GPU programming approaches either rely on low-level, monolithic code without patterns (CUDA and OpenCL), which achieves high performance at the cost of cumbersome and error-prone programming, or they improve the programmability by using pattern-based abstractions (e.g., Thrust) but pay a performance penalty due to inefﬁcient implementations of pattern composition.},
	language = {en},
	urldate = {2022-02-19},
	booktitle = {Proceedings of the 8th {International} {Workshop} on {Programming} {Models} and {Applications} for {Multicores} and {Manycores}},
	publisher = {ACM},
	author = {Haidl, Michael and Steuwer, Michel and Dirks, Hendrik and Humernbrum, Tim and Gorlatch, Sergei},
	month = feb,
	year = {2017},
	pages = {58--67},
	file = {Haidl et al. - 2017 - Towards Composable GPU Programming Programming GP.pdf:/home/david/Zotero/storage/T235UQ3X/Haidl et al. - 2017 - Towards Composable GPU Programming Programming GP.pdf:application/pdf},
}

@article{haidl_multi-stage_nodate,
	title = {Multi-{Stage} {Programming} for {GPUs} in {Modern} {C}++ using {PACXX}},
	abstract = {Writing and optimizing programs for high performance on systems with GPUs remains a challenging task even for expert programmers. One promising optimization technique is to evaluate parts of the program upfront on the CPU and embed the computed results in the GPU code allowing for more aggressive compiler optimizations. This technique is known as multi-stage programming and has proven to allow for signiﬁcant performance beneﬁts. Unfortunately, to achieve such optimizations in current GPU programming models like OpenCL, programmers are forced to manipulate the GPU source code as plain strings, which is error-prone and type-unsafe. In this paper we describe PACXX - a GPU programming approach using modern C++ standards, with the convenient features like type deduction, lambda expressions, and algorithms from the standard template library (STL). Using PACXX, a GPU program is written as a single C++ program, rather than two distinct host and kernel programs. We extend PACXX with an easy-to-use and typesafe API for multi-stage programming avoiding the pitfalls of string manipulation. Using just-in-time compilation techniques, PACXX generates efﬁcient GPU code at runtime.},
	language = {en},
	author = {Haidl, Michael and Steuwer, Michel and Humernbrum, Tim and Gorlatch, Sergei},
	pages = {10},
	file = {Haidl et al. - Multi-Stage Programming for GPUs in Modern C++ usi.pdf:/home/david/Zotero/storage/6LYUI4YI/Haidl et al. - Multi-Stage Programming for GPUs in Modern C++ usi.pdf:application/pdf},
}

@misc{dziepak_ranges_2021,
	title = {Ranges {GPU}},
	copyright = {MIT},
	url = {https://github.com/pdziepak/ranges-gpu},
	abstract = {Experimental ranges for CUDA},
	urldate = {2022-02-19},
	author = {Dziepak, Paweł},
	month = apr,
	year = {2021},
	note = {original-date: 2019-01-30T00:23:34Z},
	keywords = {c-plus-plus, cuda, range},
}

@incollection{goos_segmented_2000,
	address = {Berlin, Heidelberg},
	title = {Segmented {Iterators} and {Hierarchical} {Algorithms}},
	volume = {1766},
	isbn = {978-3-540-41090-4 978-3-540-39953-7},
	url = {http://link.springer.com/10.1007/3-540-39953-4_7},
	abstract = {Many data structures are naturally segmented. Generic algorithms that ignore that feature, and that treat every data structure as a uniform range of elements, are unnecessarily ineﬃcient. A new kind of iterator abstraction, in which segmentation is explicit, makes it possible to write hierarchical algorithms that exploit segmentation.},
	language = {en},
	urldate = {2022-02-20},
	booktitle = {Generic {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Austern, Matthew H.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Jazayeri, Mehdi and Loos, Rüdiger G. K. and Musser, David R.},
	year = {2000},
	doi = {10.1007/3-540-39953-4_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {80--90},
	file = {Austern - 2000 - Segmented Iterators and Hierarchical Algorithms.pdf:/home/david/Zotero/storage/48CG6ELG/Austern - 2000 - Segmented Iterators and Hierarchical Algorithms.pdf:application/pdf},
}

@article{ballard_communication_2014,
	title = {Communication lower bounds and optimal algorithms for numerical linear algebra},
	volume = {23},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492914000038/type/journal_article},
	doi = {10.1017/S0962492914000038},
	abstract = {The traditional metric for the efficiency of a numerical algorithm has been the number of arithmetic operations it performs. Technological trends have long been reducing the time to perform an arithmetic operation, so it is no longer the bottleneck in many algorithms; rather,
              communication
              , or moving data, is the bottleneck. This motivates us to seek algorithms that move as little data as possible, either between levels of a memory hierarchy or between parallel processors over a network. In this paper we summarize recent progress in three aspects of this problem. First we describe lower bounds on communication. Some of these generalize known lower bounds for dense classical
              (O(n
              3
              )) matrix multiplication to all direct methods of linear algebra, to sequential and parallel algorithms, and to dense and sparse matrices. We also present lower bounds for Strassen-like algorithms, and for iterative methods, in particular Krylov subspace methods applied to sparse matrices. Second, we compare these lower bounds to widely used versions of these algorithms, and note that these widely used algorithms usually communicate asymptotically more than is necessary. Third, we identify or invent new algorithms for most linear algebra problems that do attain these lower bounds, and demonstrate large speed-ups in theory and practice.},
	language = {en},
	urldate = {2022-02-22},
	journal = {Acta Numerica},
	author = {Ballard, G. and Carson, E. and Demmel, J. and Hoemmen, M. and Knight, N. and Schwartz, O.},
	month = may,
	year = {2014},
	pages = {1--155},
	file = {Ballard et al. - 2014 - Communication lower bounds and optimal algorithms .pdf:/home/david/Zotero/storage/A2RBEBBW/Ballard et al. - 2014 - Communication lower bounds and optimal algorithms .pdf:application/pdf},
}

@article{horacsek_evaluating_nodate,
	title = {Evaluating {Box} {Splines} with {Reduced} {Complexity}},
	abstract = {For the class of non-degenerate box splines, we present a set construction scheme that yields the explicit piecewise polynomial form for an arbitrary box spline. While it is possible to use the well known recursive formulation to obtain these polynomial pieces, this is quite expensive. Our construction is theoretically less expensive than the recursive formulation and allows us to evaluate box splines with more direction vectors than what would be feasible under the recursive scheme. Finally, using the explicit polynomials in each region of the box spline, we show how to create fast evaluation schemes using this explicit characterization and a spatial data structure.},
	language = {en},
	author = {Horacsek, Joshua and Alim, Usman},
	pages = {25},
	file = {Horacsek and Alim - Evaluating Box Splines with Reduced Complexity.pdf:/home/david/Zotero/storage/8KZVDAEF/Horacsek and Alim - Evaluating Box Splines with Reduced Complexity.pdf:application/pdf},
}

@misc{noauthor_boxspline1_2019,
	title = {boxspline1},
	copyright = {GPL-3.0},
	url = {https://github.com/CFD-GO/boxspline1/blob/90d7964bdb1999088b08e12ecf20adf263de6d8b/R/R/boxspline1.R},
	abstract = {1D box-spline library for limited spline parametrizations},
	urldate = {2022-02-26},
	publisher = {CFD on the GO},
	month = jun,
	year = {2019},
	note = {original-date: 2019-06-14T15:23:02Z},
}

@article{mccann_high-quality_2017,
	title = {High-{Quality} {Parallel}-{Ray} {X}-{Ray} {CT} {Back} {Projection} {Using} {Optimized} {Interpolation}},
	volume = {26},
	issn = {1941-0042},
	doi = {10.1109/TIP.2017.2706521},
	abstract = {We propose a new, cost-efficient method for computing back projections in parallel-ray X-ray CT. Forward and back projections are the basis of almost all X-ray CT reconstruction methods, but computing these accurately is costly. In the special case of parallel-ray geometry, it turns out that reconstruction requires back projection only. One approach to accelerate the back projection is through interpolation: fit a continuous representation to samples of the desired signal, then sample it at the required locations. Instead, we propose applying a prefilter that has the effect of orthogonally projecting the underlying signal onto the space spanned by the interpolator, which can significantly improve the quality of the interpolation. We then build on this idea by using oblique projection, which simplifies the computation while giving effectively the same improvement in quality. Our experiments on analytical phantoms show that this refinement can improve the reconstruction quality for both filtered back projection and iterative reconstruction in the high-quality regime, i.e., with low noise and many measurements.},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {McCann, Michael T. and Unser, Michael},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Interpolation, Kernel, Computed tomography, Image reconstruction, computed tomography, Transforms, Back, interpolation, reconstruction algorithms, Standards, X-ray tomography},
	pages = {4639--4647},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/HF4RDN6N/McCann and Unser - 2017 - High-Quality Parallel-Ray X-Ray CT Back Projection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GBNU3HCK/7932483.html:text/html},
}

@misc{262588213843476_multidimensional_array_viewsmd_nodate,
	title = {multidimensional\_array\_views.md},
	url = {https://gist.github.com/pervognsen/0e1be3b683d62b16fd81381c909bf67e},
	abstract = {GitHub Gist: instantly share code, notes, and snippets.},
	language = {en},
	urldate = {2022-02-26},
	journal = {Gist},
	author = {262588213843476},
}

@misc{noauthor_2_nodate,
	title = {2. {Kaleidoscope}: {Implementing} a {Parser} and {AST} — {LLVM} 15.0.0git documentation},
	url = {https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.html},
	urldate = {2022-02-26},
}

@article{ma_improvements_nodate,
	title = {Improvements to the {Algorithm} that {Uses} {Divided} {Differences} to {Determine} the {Coefficients} in {B}-{Spline} {Interpolation}},
	abstract = {In numerical measuring processes the interpolation is an important part. Some algorithms of B-spline interpolation were developed along the time. Here are presented two additional methods to reduce the interpolation errors. These methods could be used in any case for uniformly spaced data. These interpolation processes requires supplementary numerical calculations. Always is to choose between a smallest amount of computation and better results of interpolation.},
	language = {en},
	author = {Mâ, Liliana},
	pages = {4},
	file = {Mâ - Improvements to the Algorithm that Uses Divided Di.pdf:/home/david/Zotero/storage/ID4GLSRM/Mâ - Improvements to the Algorithm that Uses Divided Di.pdf:application/pdf},
}

@book{klemm_high_2021,
	title = {High {Performance} {Parallel} {Runtimes}: {Design} and {Implementation}},
	isbn = {978-3-11-063272-9},
	shorttitle = {High {Performance} {Parallel} {Runtimes}},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110632729/html},
	language = {en},
	urldate = {2022-02-28},
	publisher = {De Gruyter},
	author = {Klemm, Michael and Cownie, Jim},
	month = feb,
	year = {2021},
	doi = {10.1515/9783110632729},
	file = {Klemm and Cownie - 2021 - High Performance Parallel Runtimes Design and Imp.pdf:/home/david/Zotero/storage/R4UMRCR4/Klemm and Cownie - 2021 - High Performance Parallel Runtimes Design and Imp.pdf:application/pdf},
}

@book{aho_compilers_2007,
	address = {Boston},
	edition = {2nd ed},
	title = {Compilers: principles, techniques, \& tools},
	isbn = {978-0-321-48681-3},
	shorttitle = {Compilers},
	language = {en},
	publisher = {Pearson/Addison Wesley},
	editor = {Aho, Alfred V. and Aho, Alfred V.},
	year = {2007},
	note = {OCLC: ocm70775643},
	keywords = {Compilers (Computer programs)},
	file = {Aho and Aho - 2007 - Compilers principles, techniques, & tools.pdf:/home/david/Zotero/storage/5XGXLRX4/Aho and Aho - 2007 - Compilers principles, techniques, & tools.pdf:application/pdf},
}

@book{cormen_introduction_2009,
	address = {Cambridge, Mass},
	edition = {3rd ed},
	title = {Introduction to algorithms},
	isbn = {978-0-262-03384-8 978-0-262-53305-8},
	language = {en},
	publisher = {MIT Press},
	editor = {Cormen, Thomas H.},
	year = {2009},
	note = {OCLC: ocn311310321},
	keywords = {Computer algorithms, Computer programming},
	file = {Cormen - 2009 - Introduction to algorithms.pdf:/home/david/Zotero/storage/8LF8WBUJ/Cormen - 2009 - Introduction to algorithms.pdf:application/pdf},
}

@book{skiena_algorithm_2008,
	address = {London},
	title = {The {Algorithm} {Design} {Manual}},
	isbn = {978-1-84800-069-8 978-1-84800-070-4},
	url = {http://link.springer.com/10.1007/978-1-84800-070-4},
	language = {en},
	urldate = {2022-02-28},
	publisher = {Springer London},
	author = {Skiena, Steven S.},
	year = {2008},
	doi = {10.1007/978-1-84800-070-4},
	file = {Skiena - 2008 - The Algorithm Design Manual.pdf:/home/david/Zotero/storage/ZU8GEN4F/Skiena - 2008 - The Algorithm Design Manual.pdf:application/pdf},
}

@book{aziz_algorithms_2010,
	address = {Place of publication not identified},
	title = {Algorithms for interviews: a problem-solving approach},
	isbn = {978-1-4537-9299-5},
	shorttitle = {Algorithms for interviews},
	language = {en},
	publisher = {Createspace},
	author = {Aziz, Adnan and Prakash, Amit},
	year = {2010},
	note = {OCLC: 695541858},
	file = {Aziz and Prakash - 2010 - Algorithms for interviews a problem-solving appro.pdf:/home/david/Zotero/storage/LL8V28WK/Aziz and Prakash - 2010 - Algorithms for interviews a problem-solving appro.pdf:application/pdf},
}

@book{manber_introduction_1989,
	address = {Reading, Mass},
	title = {Introduction to algorithms: a creative approach},
	isbn = {978-0-201-12037-0},
	shorttitle = {Introduction to algorithms},
	language = {en},
	publisher = {Addison-Wesley},
	author = {Manber, Udi},
	year = {1989},
	keywords = {Computer algorithms, Data structures (Computer science)},
	file = {Manber - 1989 - Introduction to algorithms a creative approach.pdf:/home/david/Zotero/storage/ZPBWZ6VP/Manber - 1989 - Introduction to algorithms a creative approach.pdf:application/pdf},
}

@book{levitin_introduction_2003,
	address = {Boston},
	title = {Introduction to the design \& analysis of algorithms},
	isbn = {978-0-201-74395-1},
	language = {eng},
	publisher = {Addison-Wesley},
	author = {Levitin, Anany},
	year = {2003},
	file = {Levitin - 2003 - Introduction to the design & analysis of algorithm.pdf:/home/david/Zotero/storage/KHK4EHXD/Levitin - 2003 - Introduction to the design & analysis of algorithm.pdf:application/pdf},
}

@book{kleinberg_algorithm_2006,
	address = {Boston},
	title = {Algorithm design},
	isbn = {978-0-321-29535-4},
	publisher = {Pearson/Addison-Wesley},
	author = {Kleinberg, Jon and Tardos, Éva},
	year = {2006},
	keywords = {Computer algorithms, Data structures (Computer science)},
	file = {Kleinberg and Tardos - 2006 - Algorithm design.pdf:/home/david/Zotero/storage/3TD6U7AB/Kleinberg and Tardos - 2006 - Algorithm design.pdf:application/pdf},
}

@article{milewski_category_nodate,
	title = {Category {Theory} for {Programmers}},
	language = {en},
	author = {Milewski, Bartosz},
	pages = {510},
	file = {Milewski - Category Theory for Programmers.pdf:/home/david/Zotero/storage/PSYR3FXN/Milewski - Category Theory for Programmers.pdf:application/pdf},
}

@article{noauthor_structure_1997,
	title = {Structure and interpretation of computer programs, (second edition)},
	volume = {33},
	issn = {08981221},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0898122197900511},
	doi = {10.1016/S0898-1221(97)90051-1},
	language = {en},
	number = {4},
	urldate = {2022-02-28},
	journal = {Computers \& Mathematics with Applications},
	month = feb,
	year = {1997},
	pages = {133},
	file = {1997 - Structure and interpretation of computer programs,.pdf:/home/david/Zotero/storage/HFU6WXRU/1997 - Structure and interpretation of computer programs,.pdf:application/pdf},
}

@article{takekawa_fast_2021,
	title = {Fast parallel calculation of modified {Bessel} function of the second kind and its derivatives},
	url = {http://arxiv.org/abs/2108.11560},
	abstract = {There are three main types of numerical computations for the Bessel function of the second kind: series expansion, continued fraction, and asymptotic expansion. In addition, they are combined in the appropriate domain for each. However, there are some regions where the combination of these types requires suﬃcient computation time to achieve suﬃcient accuracy, however, eﬃciency is signiﬁcantly reduced when parallelized. In the proposed method, we adopt a simple numerical integration concept of integral representation. We coarsely reﬁne the integration range beforehand, and stabilize the computation time by performing the integration calculation at a ﬁxed number of intervals. Experiments demonstrate that the proposed method can achieve the same level of accuracy as existing methods in less than half the computation time.},
	language = {en},
	urldate = {2022-03-12},
	journal = {arXiv:2108.11560 [cs, math]},
	author = {Takekawa, Takashi},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.11560},
	keywords = {Mathematics - Numerical Analysis, 65D15, G.1.2, G.4},
	file = {Takekawa - 2021 - Fast parallel calculation of modified Bessel funct.pdf:/home/david/Zotero/storage/L6K2HGEU/Takekawa - 2021 - Fast parallel calculation of modified Bessel funct.pdf:application/pdf},
}

@article{gil_evaluation_2002,
	title = {Evaluation of the {Modified} {Bessel} {Function} of the {Third} {Kind} of {Imaginary} {Orders}},
	volume = {175},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999101968949},
	doi = {10.1006/jcph.2001.6894},
	language = {en},
	number = {2},
	urldate = {2022-03-12},
	journal = {Journal of Computational Physics},
	author = {Gil, Amparo and Segura, Javier and Temme, Nico M.},
	month = jan,
	year = {2002},
	pages = {398--411},
	file = {Gil et al. - 2002 - Evaluation of the Modified Bessel Function of the .pdf:/home/david/Zotero/storage/FRVRYISU/Gil et al. - 2002 - Evaluation of the Modified Bessel Function of the .pdf:application/pdf},
}

@article{temme_numerical_1975,
	title = {On the numerical evaluation of the modified bessel function of the third kind},
	volume = {19},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0021999175900820},
	doi = {10.1016/0021-9991(75)90082-0},
	language = {en},
	number = {3},
	urldate = {2022-03-12},
	journal = {Journal of Computational Physics},
	author = {Temme, N.M.},
	month = nov,
	year = {1975},
	pages = {324--337},
	file = {Temme - 1975 - On the numerical evaluation of the modified bessel.pdf:/home/david/Zotero/storage/JDEA9S49/Temme - 1975 - On the numerical evaluation of the modified bessel.pdf:application/pdf},
}

@misc{noauthor_r_nodate,
	title = {r - {Modified} {Bessel} functions of order (n)},
	url = {https://stackoverflow.com/questions/8797722/modified-bessel-functions-of-order-n},
	urldate = {2022-03-12},
	journal = {Stack Overflow},
	file = {Snapshot:/home/david/Zotero/storage/H2RRYICA/modified-bessel-functions-of-order-n.html:text/html},
}

@misc{noauthor_index_nodate,
	title = {Index of {Code} {Identifiers}},
	url = {https://www.astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/index_by_ident.htm},
	urldate = {2022-03-12},
	file = {Index of Code Identifiers:/home/david/Zotero/storage/RV4H9B4N/index_by_ident.html:text/html},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/code/bessel.h},
	urldate = {2022-03-12},
	file = {astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/code/bessel.h:/home/david/Zotero/storage/QW9ZHIZT/bessel.html:text/html},
}

@article{withers_x-ray_2021,
	title = {X-ray computed tomography},
	volume = {1},
	issn = {2662-8449},
	url = {http://www.nature.com/articles/s43586-021-00015-4},
	doi = {10.1038/s43586-021-00015-4},
	abstract = {X-r ay computed tomography (CT) can reveal the internal details of objects in three dimensions non-d estructively. In this Primer, we outline the basic principles of CT and describe the ways in which a CT scan can be acquired using X-r ay tubes and synchrotron sources, including the different possible contrast modes that can be exploited. We explain the process of computationally reconstructing three-d imensional (3D) images from 2D radiographs and how to segment the 3D images for subsequent visualization and quantification. Whereas CT is widely used in medical and heavy industrial contexts at relatively low resolutions, here we focus on the application of higher resolution X-r ay CT across science and engineering. We consider the application of X-ray CT to study subjects across the materials, metrology and manufacturing, engineering, food, biological, geological and palaeontological sciences. We examine how CT can be used to follow the structural evolution of materials in three dimensions in real time or in a time-lapse manner, for example to follow materials manufacturing or the in-s ervice behaviour and degradation of manufactured components. Finally, we consider the potential for radiation damage and common sources of imaging artefacts, discuss reproducibility issues and consider future advances and opportunities.},
	language = {en},
	number = {1},
	urldate = {2022-03-15},
	journal = {Nature Reviews Methods Primers},
	author = {Withers, Philip J. and Bouman, Charles and Carmignato, Simone and Cnudde, Veerle and Grimaldi, David and Hagen, Charlotte K. and Maire, Eric and Manley, Marena and Du Plessis, Anton and Stock, Stuart R.},
	month = dec,
	year = {2021},
	pages = {18},
	file = {Withers et al. - 2021 - X-ray computed tomography.pdf:/home/david/Zotero/storage/AZGEZT7Y/Withers et al. - 2021 - X-ray computed tomography.pdf:application/pdf},
}

@article{der_sarkissian_cone-beam_2019,
	title = {A cone-beam {X}-ray computed tomography data collection designed for machine learning},
	volume = {6},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-019-0235-y},
	doi = {10.1038/s41597-019-0235-y},
	abstract = {Abstract
            Unlike previous works, this open data collection consists of X-ray cone-beam (CB) computed tomography (CT) datasets specifically designed for machine learning applications and high cone-angle artefact reduction. Forty-two walnuts were scanned with a laboratory X-ray set-up to provide not only data from a single object but from a class of objects with natural variability. For each walnut, CB projections on three different source orbits were acquired to provide CB data with different cone angles as well as being able to compute artefact-free, high-quality ground truth images from the combined data that can be used for supervised learning. We provide the complete image reconstruction pipeline: raw projection data, a description of the scanning geometry, pre-processing and reconstruction scripts using open software, and the reconstructed volumes. Due to this, the dataset can not only be used for high cone-angle artefact reduction but also for algorithm development and evaluation for other tasks, such as image reconstruction from limited or sparse-angle (low-dose) scanning, super resolution, or segmentation.},
	language = {en},
	number = {1},
	urldate = {2022-03-15},
	journal = {Scientific Data},
	author = {Der Sarkissian, Henri and Lucka, Felix and van Eijnatten, Maureen and Colacicco, Giulia and Coban, Sophia Bethany and Batenburg, Kees Joost},
	month = dec,
	year = {2019},
	pages = {215},
	file = {Der Sarkissian et al. - 2019 - A cone-beam X-ray computed tomography data collect.pdf:/home/david/Zotero/storage/W7QC52EH/Der Sarkissian et al. - 2019 - A cone-beam X-ray computed tomography data collect.pdf:application/pdf},
}

@misc{der_sarkissian_cone-beam_2019-1,
	title = {Cone-{Beam} {X}-{Ray} {CT} {Data} {Collection} {Designed} for {Machine} {Learning}: {Samples} 1-8},
	shorttitle = {Cone-{Beam} {X}-{Ray} {CT} {Data} {Collection} {Designed} for {Machine} {Learning}},
	url = {https://zenodo.org/record/2686726},
	abstract = {This upload contains samples 1 - 8 from the data collection described in Henri Der Sarkissian, Felix Lucka, Maureen van Eijnatten, Giulia Colacicco, Sophia Bethany Coban, Kees Joost Batenburg, "A Cone-Beam X-Ray CT Data Collection Designed for Machine Learning", Sci Data 6, 215 (2019). https://doi.org/10.1038/s41597-019-0235-y or arXiv:1905.04787 (2019) Abstract: "Unlike previous works, this open data collection consists of X-ray cone-beam (CB) computed tomography (CT) datasets specifically designed for machine learning applications and high cone-angle artefact reduction: Forty-two walnuts were scanned with a laboratory X-ray setup to provide not only data from a single object but from a class of objects with natural variability. For each walnut, CB projections on three different orbits were acquired to provide CB data with different cone angles as well as being able to compute artefact-free, high-quality ground truth images from the combined data that can be used for supervised learning. We provide the complete image reconstruction pipeline: raw projection data, a description of the scanning geometry, pre-processing and reconstruction scripts using open software, and the reconstructed volumes. Due to this, the dataset can not only be used for high cone-angle artefact reduction but also for algorithm development and evaluation for other tasks, such as image reconstruction from limited or sparse-angle (low-dose) scanning, super resolution, or segmentation." The scans are performed using a custom-built, highly flexible X-ray CT scanner, the FleX-ray scanner, developed by XRE nvand located in the FleX-ray Lab at the Centrum Wiskunde \& Informatica (CWI) in Amsterdam, Netherlands. The general purpose of the FleX-ray Lab is to conduct proof of concept experiments directly accessible to researchers in the field of mathematics and computer science. The scanner consists of a cone-beam microfocus X-ray point source that projects polychromatic X-rays onto a 1536-by-1944 pixels, 14-bit flat panel detector (Dexella 1512NDT) and a rotation stage in-between, upon which a sample is mounted. All three components are mounted on translation stages which allow them to move independently from one another. Please refer to the paper for all further technical details. The complete data set can be found via the following links: 1-8, 9-16, 17-24, 25-32, 33-37, 38-42 The corresponding Python scripts for loading, pre-processing and reconstructing the projection data in the way described in the paper can be found on github For more information or guidance in using these dataset, please get in touch with henri.dersarkissian [at] gmail.com Felix.Lucka [at] cwi.nl},
	urldate = {2022-03-15},
	publisher = {Zenodo},
	author = {Der Sarkissian, Henri and Lucka, Felix and van Eijnatten, Maureen and Colacicco, Giulia and Coban, Sophia Bethany and Batenburg, K. Joost},
	month = may,
	year = {2019},
	doi = {10.5281/zenodo.2686726},
	note = {Type: dataset},
	keywords = {computed tomography, cone beam, deep learning, image reconstruction, machine learning, X-ray},
	file = {Zenodo Snapshot:/home/david/Zotero/storage/6VPDHAU3/2686726.html:text/html},
}

@inproceedings{gach_2d_2008,
	address = {Las Vegas, NV, USA},
	title = {{2D} \&\#x00026; {3D} {Shepp}-{Logan} {Phantom} {Standards} for {MRI}},
	isbn = {978-0-7695-3331-5},
	url = {http://ieeexplore.ieee.org/document/4616690/},
	doi = {10.1109/ICSEng.2008.15},
	abstract = {The Shepp-Logan phantom was created as a standard for computerized tomography (CT) image reconstruction simulations of the head. The phantom is also used frequently for magnetic resonance image (MRI) reconstruction and k-space simulations. However, while the CT version incorporated the radiation attenuation properties of the head and brain, the MRI version of the phantom was neither adapted to MR physics nor defined as a viable standard. As a result, it is not currently possible to compare and validate MR simulations from different research studies. In this paper, we present 2D and 3D SheppLogan MRI phantom standards and their analytic kspace representations based on existing CT standards after modification for MR physics. The standard includes T1 and T2 relaxation times for different fields strengths based on current literature.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {2008 19th {International} {Conference} on {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Gach, H. Michael and Tanase, Costin and Boada, Fernando},
	month = aug,
	year = {2008},
	pages = {521--526},
	file = {Gach et al. - 2008 - 2D &#x00026\; 3D Shepp-Logan Phantom Standards for .pdf:/home/david/Zotero/storage/GYBE7RNT/Gach et al. - 2008 - 2D &#x00026\; 3D Shepp-Logan Phantom Standards for .pdf:application/pdf},
}

@article{la_riviere_spline-based_1998,
	title = {Spline-based inverse {Radon} transform in two and three dimensions},
	volume = {45},
	issn = {0018-9499, 1558-1578},
	url = {https://ieeexplore.ieee.org/document/708352/},
	doi = {10.1109/23.708352},
	abstract = {While the exact inverse Radon transform is a continuous integral equation, the discrete nature of the data output by tomographic imaging systems generally demands that images be reconstructed using a discrete approximation to the transform. However, by fitting an analytic function to the projection data prior to reconstruction, one can avoid such approximations and preserve and exploit the continuous nature of the inverse transform.},
	language = {en},
	number = {4},
	urldate = {2022-03-18},
	journal = {IEEE Transactions on Nuclear Science},
	author = {La Riviere, P.J. and Pan, X.},
	month = aug,
	year = {1998},
	pages = {2224--2231},
	file = {La Riviere and Pan - 1998 - Spline-based inverse Radon transform in two and th.pdf:/home/david/Zotero/storage/4VSCFHND/La Riviere and Pan - 1998 - Spline-based inverse Radon transform in two and th.pdf:application/pdf},
}

@inproceedings{ye_box_2011,
	address = {Chicago, IL, USA},
	title = {Box spline based {3D} tomographic reconstruction of diffusion propagators from {MRI} data},
	isbn = {978-1-4244-4127-3},
	url = {http://ieeexplore.ieee.org/document/5872432/},
	doi = {10.1109/ISBI.2011.5872432},
	abstract = {This paper introduces a tomographic approach for reconstruction of diffusion propagators, P(r), in a box spline framework. Box splines are chosen as basis functions for high-order approximation of P(r) from the diffusion signal. Box splines are a generalization of B-splines to multivariate setting that are particularly useful in the context of tomographic reconstruction. The X-Ray or Radon transform of a (tensor-product B-spline or a non-separable) box spline is a box spline – the space of box splines is closed under the Radon transform.},
	language = {en},
	urldate = {2022-03-19},
	booktitle = {2011 {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {From} {Nano} to {Macro}},
	publisher = {IEEE},
	author = {Ye, Wenxing and Portnoy, Sharon and Entezari, Alireza and Vemuri, Baba C. and Blackband, Stephen J.},
	month = mar,
	year = {2011},
	pages = {397--400},
	file = {Ye et al. - 2011 - Box spline based 3D tomographic reconstruction of .pdf:/home/david/Zotero/storage/4MPYPVU4/Ye et al. - 2011 - Box spline based 3D tomographic reconstruction of .pdf:application/pdf},
}

@inproceedings{mirzargar_spline_2013-1,
	title = {A spline framework for sparse tomographic reconstruction},
	doi = {10.1109/ISBI.2013.6556763},
	abstract = {We present a spline-based sparse tomographic reconstruction framework. The proposed method utilizes the closed-form analytical Radon transform of B-splines and box splines of any order and integrates the (transform-domain) sparsity of the image into the reconstruction algorithm. Our experiments show that the synergy of sparse reconstruction together with higher order basis functions (e.g., cubic B-splines) improves the accuracy of the reconstruction. This gain can also be exploited for reducing the number of projection angles in the data acquisition.},
	booktitle = {2013 {IEEE} 10th {International} {Symposium} on {Biomedical} {Imaging}},
	author = {Mirzargar, Mahsa and Sakhaee, Elham and Entezari, Alireza},
	month = apr,
	year = {2013},
	note = {ISSN: 1945-8452},
	keywords = {Image reconstruction, Tomography, Splines (mathematics), Transforms, Accuracy, Approximation methods, Signal to noise ratio, sparse approximation, splines, Tomographic reconstruction},
	pages = {1272--1275},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/AWNBSLVV/Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GHXESPZT/6556763.html:text/html},
}

@article{wan_high-performance_2012,
	title = {High-performance blob-based iterative three-dimensional reconstruction in electron tomography using multi-{GPUs}},
	volume = {13},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-S10-S4},
	doi = {10.1186/1471-2105-13-S10-S4},
	abstract = {Background: Three-dimensional (3D) reconstruction in electron tomography (ET) has emerged as a leading technique to elucidate the molecular structures of complex biological specimens. Blob-based iterative methods are advantageous reconstruction methods for 3D reconstruction in ET, but demand huge computational costs. Multiple graphic processing units (multi-GPUs) offer an affordable platform to meet these demands. However, a synchronous communication scheme between multi-GPUs leads to idle GPU time, and a weighted matrix involved in iterative methods cannot be loaded into GPUs especially for large images due to the limited available memory of GPUs.
Results: In this paper we propose a multilevel parallel strategy combined with an asynchronous communication scheme and a blob-ELLR data structure to efficiently perform blob-based iterative reconstructions on multi-GPUs. The asynchronous communication scheme is used to minimize the idle GPU time so as to asynchronously overlap communications with computations. The blob-ELLR data structure only needs nearly 1/16 of the storage space in comparison with ELLPACK-R (ELLR) data structure and yields significant acceleration.
Conclusions: Experimental results indicate that the multilevel parallel scheme combined with the asynchronous communication scheme and the blob-ELLR data structure allows efficient implementations of 3D reconstruction in ET on multi-GPUs.},
	language = {en},
	number = {S10},
	urldate = {2022-03-19},
	journal = {BMC Bioinformatics},
	author = {Wan, Xiaohua and Zhang, Fa and Chu, Qi and Liu, Zhiyong},
	month = jun,
	year = {2012},
	pages = {S4},
	file = {Wan et al. - 2012 - High-performance blob-based iterative three-dimens.pdf:/home/david/Zotero/storage/U3JHAGCL/Wan et al. - 2012 - High-performance blob-based iterative three-dimens.pdf:application/pdf},
}

@phdthesis{erlandsson_positron_1996,
	address = {Lund},
	title = {Positron emission tomography with three-dimensional reconstruction},
	language = {en},
	author = {Erlandsson, Kjell},
	year = {1996},
	note = {ISBN: 9789162821890
OCLC: 186220689},
	file = {Erlandsson - 1996 - Positron emission tomography with three-dimensiona.pdf:/home/david/Zotero/storage/HCFMWIVN/Erlandsson - 1996 - Positron emission tomography with three-dimensiona.pdf:application/pdf},
}

@article{zhang_convolutional_2019,
	title = {A {Convolutional} {Forward} and {Back}-{Projection} {Model} for {Fan}-{Beam} {Geometry}},
	url = {http://arxiv.org/abs/1907.10526},
	abstract = {Iterative methods for tomographic image reconstruction have great potential for enabling high quality imaging from low-dose projection data. The computational burden of iterative reconstruction algorithms, however, has been an impediment in their adoption in practical CT reconstruction problems. We present an approach for highly efﬁcient and accurate computation of forward model for image reconstruction in fan-beam geometry in X-ray CT. The efﬁciency of computations makes this approach suitable for large-scale optimization algorithms with on-the-ﬂy, memory-less, computations of the forward and back-projection. Our experiments demonstrate the improvements in accuracy as well as efﬁciency of our model, speciﬁcally for ﬁrst-order box splines (i.e., pixel-basis) compared to recently developed methods for this purpose, namely Look-up Table-based Ray Integration (LTRI) and Separable Footprints (SF) in 2-D.},
	language = {en},
	urldate = {2022-03-19},
	journal = {arXiv:1907.10526 [cs, eess]},
	author = {Zhang, Kai and Entezari, Alireza},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.10526},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhang and Entezari - 2019 - A Convolutional Forward and Back-Projection Model .pdf:/home/david/Zotero/storage/A96JJE6K/Zhang and Entezari - 2019 - A Convolutional Forward and Back-Projection Model .pdf:application/pdf},
}

@article{agulleiro_evaluation_2012,
	title = {Evaluation of a {Multicore}-{Optimized} {Implementation} for {Tomographic} {Reconstruction}},
	volume = {7},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0048261},
	doi = {10.1371/journal.pone.0048261},
	abstract = {Tomography allows elucidation of the three-dimensional structure of an object from a set of projection images. In life sciences, electron microscope tomography is providing invaluable information about the cell structure at a resolution of a few nanometres. Here, large images are required to combine wide fields of view with high resolution requirements. The computational complexity of the algorithms along with the large image size then turns tomographic reconstruction into a computationally demanding problem. Traditionally, high-performance computing techniques have been applied to cope with such demands on supercomputers, distributed systems and computer clusters. In the last few years, the trend has turned towards graphics processing units (GPUs). Here we present a detailed description and a thorough evaluation of an alternative approach that relies on exploitation of the power available in modern multicore computers. The combination of single-core code optimization, vector processing, multithreading and efficient disk I/O operations succeeds in providing fast tomographic reconstructions on standard computers. The approach turns out to be competitive with the fastest GPU-based solutions thus far.},
	language = {en},
	number = {11},
	urldate = {2022-03-19},
	journal = {PLoS ONE},
	author = {Agulleiro, Jose-Ignacio and Fernández, José Jesús},
	editor = {Frischknecht, Friedrich},
	month = nov,
	year = {2012},
	pages = {e48261},
	file = {Agulleiro and Fernández - 2012 - Evaluation of a Multicore-Optimized Implementation.pdf:/home/david/Zotero/storage/V4U5C5EH/Agulleiro and Fernández - 2012 - Evaluation of a Multicore-Optimized Implementation.pdf:application/pdf},
}

@book{bebis_advances_2014-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Visual} {Computing}: 10th {International} {Symposium}, {ISVC} 2014, {Las} {Vegas}, {NV}, {USA}, {December} 8-10, 2014, {Proceedings}, {Part} {I}},
	volume = {8887},
	isbn = {978-3-319-14248-7 978-3-319-14249-4},
	shorttitle = {Advances in {Visual} {Computing}},
	url = {http://link.springer.com/10.1007/978-3-319-14249-4},
	language = {en},
	urldate = {2022-03-19},
	publisher = {Springer International Publishing},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and McMahan, Ryan and Jerald, Jason and Zhang, Hui and Drucker, Steven M. and Kambhamettu, Chandra and El Choubassi, Maha and Deng, Zhigang and Carlson, Mark},
	year = {2014},
	doi = {10.1007/978-3-319-14249-4},
	file = {Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:/home/david/Zotero/storage/Q49UGWSI/Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:application/pdf},
}

@article{kaufman_maximum_1993,
	title = {Maximum likelihood, least squares, and penalized least squares for {PET}},
	volume = {12},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/232249/},
	doi = {10.1109/42.232249},
	abstract = {The EM algorithm is the basic approach used to maximize the log likelihood objective function for the reconstruction problem in PET. The EM algorithm is a scaled steepest ascent algorithm that elegantly handles the nonnegativity constraints of the problem. We show that the same scaled steepest descent algorithm can be applied to the least squares merit function, and that it can be accelerated using the conjugate gradient approach. Our experiments suggest that one can cut the computation by about a factor of 3 by using this technique. Our results also apply to various penalized least squares functions which might be used to produce a smoother image.},
	language = {en},
	number = {2},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Kaufman, L.},
	month = jun,
	year = {1993},
	pages = {200--214},
	file = {Kaufman - 1993 - Maximum likelihood, least squares, and penalized l.pdf:/home/david/Zotero/storage/TXLSS8VP/Kaufman - 1993 - Maximum likelihood, least squares, and penalized l.pdf:application/pdf},
}

@article{ollinger_maximum-likelihood_1994,
	title = {Maximum-likelihood reconstruction of transmission images in emission computed tomography via the {EM} algorithm},
	volume = {13},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/276147/},
	doi = {10.1109/42.276147},
	abstract = {The expectation-maximization (EM) algm·ithm fm' computing maximum-likelihood estimates of transmission images in positron-emission tomogl'aJ{\textgreater}hy (PET) [1) is extended to include measurement enm·, accidental coincidences and Compton scattel'. A method for accomplishing the maximization stet{\textgreater} using one step of Newton's method is tn·oposed. The algorithm is regularized with the method of sieves. Evaluations using both Monte Carlo simulations and phantom studies on the Siemens 953B scanne1· suggest that the algorithm yields unbiased images with significantly lower variances than filtered-backprojcclion when the images are reconstructed to the intrinsic resolution. Large features in the images converge in under 200 iterations while the smallest features required up to 2,000 iterations. All but the smallest features in typical transmission scans converge In approximately 250 iterations. The initial implementation of the algorithm requires 50 sec per iteration on a DECStation 5000.},
	language = {en},
	number = {1},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ollinger, J.M.},
	month = mar,
	year = {1994},
	pages = {89--101},
	file = {Ollinger - 1994 - Maximum-likelihood reconstruction of transmission .pdf:/home/david/Zotero/storage/ZIY2QA8M/Ollinger - 1994 - Maximum-likelihood reconstruction of transmission .pdf:application/pdf},
}

@book{lengauer_domain-specific_2004,
	address = {Berlin ; New York},
	series = {Lecture notes in computer science, {State}-of-the-art survey},
	title = {Domain-specific program generation: international seminar, {Dagstuhl} {Castle}, {Germany}, {March} 23-28, 2003: revised papers},
	isbn = {978-3-540-22119-7},
	shorttitle = {Domain-specific program generation},
	language = {en},
	number = {3016},
	publisher = {Springer},
	editor = {Lengauer, Christian},
	year = {2004},
	note = {OCLC: ocm55600479},
	keywords = {Congresses, Generative programming (Computer science), Parallel programming (Computer science), Programming languages (Electronic computers)},
	file = {Lengauer - 2004 - Domain-specific program generation international .pdf:/home/david/Zotero/storage/MTYQZ87M/Lengauer - 2004 - Domain-specific program generation international .pdf:application/pdf},
}

@book{halefoglu_computed_2017,
	title = {Computed {Tomography} - {Advanced} {Applications}},
	isbn = {978-953-51-3368-1},
	url = {https://www.intechopen.com/books/6005},
	abstract = {The advent and rapid diffusion of advanced multidetector-row scanner technology offers comprehensive evaluation of different anatomic structures in daily practice. The aim of this book is to introduce the applications of CT imaging in not only general medicine but also in different fields especially in veterinary medicine, dentistry, and engineering. Recent developments in CT technology have led to a widening of its applications on many areas like material testing in engineering, 3D evaluation of teeth, and the vascular and cardiac evaluations of small animals.},
	language = {en},
	urldate = {2022-03-24},
	author = {Halefoglu, Ahmet Mesrur},
	month = aug,
	year = {2017},
	doi = {10.5772/66614},
	file = {Snapshot:/home/david/Zotero/storage/CDYVVUZM/6005.html:text/html},
}

@book{herman_discrete_1999,
	address = {Boston, MA},
	title = {Discrete {Tomography} {Foundations}, {Algorithms}, and {Applications}.},
	isbn = {978-1-4612-1568-4},
	url = {http://public.eblib.com/choice/PublicFullRecord.aspx?p=6489295},
	language = {en},
	urldate = {2022-03-24},
	publisher = {Birkh??user Boston},
	author = {Herman, Gabor T and Kuba, Attila},
	year = {1999},
	note = {OCLC: 1245672421},
	file = {Herman and Kuba - 1999 - Discrete Tomography Foundations, Algorithms, and A.pdf:/home/david/Zotero/storage/5WAZT7JH/Herman and Kuba - 1999 - Discrete Tomography Foundations, Algorithms, and A.pdf:application/pdf},
}

@book{carmignato_industrial_2018,
	address = {Cham},
	edition = {1st ed. 2018},
	title = {Industrial {X}-{Ray} {Computed} {Tomography}},
	isbn = {978-3-319-59573-3},
	abstract = {This book covers all aspects of industrial X-Ray computed tomography (XCT) including history, basics, different instrument architectures, hardware and software, error sources, traceability and calibration, and applications. The book is intended for users and developers of XCT instrumentation and software in both industry and academia, being also suitable for postgraduate students},
	language = {en},
	publisher = {Springer International Publishing : Imprint: Springer},
	editor = {Carmignato, Simone and Dewulf, Wim and Leach, Richard},
	year = {2018},
	doi = {10.1007/978-3-319-59573-3},
	keywords = {Atomic, Molecular, Optical and Plasma Physics, Atoms, Characterization and Evaluation of Materials, Electronic materials, Manufactures, Manufacturing, Machines, Tools, Processes, Materials science, Optical and Electronic Materials, Optical materials, Physics},
	file = {Carmignato et al. - 2018 - Industrial X-Ray Computed Tomography.pdf:/home/david/Zotero/storage/UNEGITUR/Carmignato et al. - 2018 - Industrial X-Ray Computed Tomography.pdf:application/pdf},
}

@article{weitkamp_x-ray_2008,
	series = {Proceedings of the 5th {Medical} {Application} of {Synchrotron} {Radiation} 2007},
	title = {X-ray phase radiography and tomography of soft tissue using grating interferometry},
	volume = {68},
	issn = {0720-048X},
	url = {https://www.sciencedirect.com/science/article/pii/S0720048X08002738},
	doi = {10.1016/j.ejrad.2008.04.031},
	abstract = {X-ray phase and absorption radiographs and tomograms of the heart of a rat were taken with an X-ray grating interferometer with monochromatic synchrotron radiation at a photon energy of 17.5keV. The phase images show largely superior quality with respect to the absorption images taken with the same dose, particularly much better contrast and contrast-to-noise ratio. Different tissues can clearly be distinguished. The results demonstrate the potential of grating interferometry for two- and three-dimensional X-ray imaging of biological soft tissue in an aqueous environment.},
	language = {en},
	number = {3, Supplement},
	urldate = {2022-03-24},
	journal = {European Journal of Radiology},
	author = {Weitkamp, Timm and David, Christian and Bunk, Oliver and Bruder, Jens and Cloetens, Peter and Pfeiffer, Franz},
	month = dec,
	year = {2008},
	keywords = {Tomography, Radiography, Talbot interferometer, X-ray interferometer, X-ray phase contrast},
	pages = {S13--S17},
	file = {ScienceDirect Snapshot:/home/david/Zotero/storage/AHNDKVZB/S0720048X08002738.html:text/html},
}

@article{weitkamp_x-ray_2005,
	title = {X-ray phase imaging with a grating interferometer},
	abstract = {Using a high-efﬁciency grating interferometer for hard X rays (10–30 keV) and a phase-stepping technique, separate radiographs of the phase and absorption proﬁles of bulk samples can be obtained from a single set of measurements. Tomographic reconstruction yields quantitative three-dimensional maps of the X-ray refractive index, with a spatial resolution down to a few microns. The method is mechanically robust, requires little spatial coherence and monochromaticity, and can be scaled up to large ﬁelds of view, with a detector of correspondingly moderate spatial resolution. These are important prerequisites for use with laboratory X-ray sources.},
	language = {en},
	author = {Weitkamp, Timm and Diaz, Ana and David, Christian and Pfeiffer, Franz and Stampanoni, Marco and Cloetens, Peter and Ziegler, Eric},
	year = {2005},
	pages = {9},
	file = {Weitkamp et al. - 2005 - X-ray phase imaging with a grating interferometer.pdf:/home/david/Zotero/storage/8V67HT5L/Weitkamp et al. - 2005 - X-ray phase imaging with a grating interferometer.pdf:application/pdf},
}

@article{teuffenbach_grating-based_2017,
	title = {Grating-based phase-contrast and dark-field computed tomography: a single-shot method},
	doi = {https://doi.org/10.1038/s41598-017-06729-4},
	language = {en},
	journal = {SCienTifiC REPorts},
	author = {Teuffenbach, M.v. and Koehler, T. and Fehringer, A.},
	year = {2017},
	pages = {8},
	file = {Grating-based phase-contrast and dark-field comput.pdf:/home/david/Zotero/storage/G95PIVFX/Grating-based phase-contrast and dark-field comput.pdf:application/pdf},
}

@article{viermetz_dark-field_2022,
	title = {Dark-field computed tomography reaches the human scale},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2118799119},
	doi = {10.1073/pnas.2118799119},
	abstract = {Significance
            X-ray computed tomography (CT) is one of the most commonly used diagnostic three-dimensional imaging modalities today. Conventionally, this noninvasive technique generates contrast by measuring the X-ray attenuation properties of different tissues. Considering the wave nature of X-rays, complementary contrast can be achieved by further measuring their small-angle scattering (dark-field) properties. This provides additional valuable diagnostic information on otherwise unresolved tissue microstructure. In our work, we have translated this wave-optical mechanism from the optical bench to a human-sized prototype CT system. This involved the integration of an interferometer into a clinical CT gantry and overcoming several associated challenges regarding vibrations, continuous gantry rotation, and large field of view. This development puts complementary X-ray contrast within reach for real-word medical applications.
          , 
            X-ray computed tomography (CT) is one of the most commonly used three-dimensional medical imaging modalities today. It has been refined over several decades, with the most recent innovations including dual-energy and spectral photon-counting technologies. Nevertheless, it has been discovered that wave-optical contrast mechanisms—beyond the presently used X-ray attenuation—offer the potential of complementary information, particularly on otherwise unresolved tissue microstructure. One such approach is dark-field imaging, which has recently been introduced and already demonstrated significantly improved radiological benefit in small-animal models, especially for lung diseases. Until now, however, dark-field CT could not yet be translated to the human scale and has been restricted to benchtop and small-animal systems, with scan durations of several minutes or more. This is mainly because the adaption and upscaling to the mechanical complexity, speed, and size of a human CT scanner so far remained an unsolved challenge. Here, we now report the successful integration of a Talbot–Lau interferometer into a clinical CT gantry and present dark-field CT results of a human-sized anthropomorphic body phantom, reconstructed from a single rotation scan performed in 1 s. Moreover, we present our key hardware and software solutions to the previously unsolved roadblocks, which so far have kept dark-field CT from being translated from the optical bench into a rapidly rotating CT gantry, with all its associated challenges like vibrations, continuous rotation, and large field of view. This development enables clinical dark-field CT studies with human patients in the near future.},
	language = {en},
	number = {8},
	urldate = {2022-03-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Viermetz, Manuel and Gustschin, Nikolai and Schmid, Clemens and Haeusele, Jakob and von Teuffenbach, Maximilian and Meyer, Pascal and Bergner, Frank and Lasser, Tobias and Proksa, Roland and Koehler, Thomas and Pfeiffer, Franz},
	month = feb,
	year = {2022},
	pages = {e2118799119},
	file = {Viermetz et al. - 2022 - Dark-field computed tomography reaches the human s.pdf:/home/david/Zotero/storage/UF7NGGY3/Viermetz et al. - 2022 - Dark-field computed tomography reaches the human s.pdf:application/pdf},
}

@article{stefanoiu_what_2020,
	title = {What about computational super-resolution in fluorescence {Fourier} light field microscopy?},
	volume = {28},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-28-11-16554},
	doi = {10.1364/OE.391189},
	abstract = {Recently, Fourier light ﬁeld microscopy was proposed to overcome the limitations in conventional light ﬁeld microscopy by placing a micro-lens array at the aperture stop of the microscope objective instead of the image plane. In this way, a collection of orthographic views from diﬀerent perspectives are directly captured. When inspecting ﬂuorescent samples, the sensitivity and noise of the sensors are a major concern and large sensor pixels are required to cope with low-light conditions, which implies under-sampling issues. In this context, we analyze the sampling patterns in Fourier light ﬁeld microscopy to understand to what extent computational super-resolution can be triggered during deconvolution in order to improve the resolution of the 3D reconstruction of the imaged data.},
	language = {en},
	number = {11},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Stefanoiu, Anca and Scrofani, Gabriele and Saavedra, Genaro and Martínez-Corral, Manuel and Lasser, Tobias},
	month = may,
	year = {2020},
	pages = {16554},
	file = {Stefanoiu et al. - 2020 - What about computational super-resolution in fluor.pdf:/home/david/Zotero/storage/5BILIN4C/Stefanoiu et al. - 2020 - What about computational super-resolution in fluor.pdf:application/pdf},
}

@article{stefanoiu_artifact-free_2019,
	title = {Artifact-free deconvolution in light field microscopy},
	volume = {27},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-27-22-31644},
	doi = {10.1364/OE.27.031644},
	abstract = {The sampling patterns of the light ﬁeld microscope (LFM) are highly depth-dependent, which implies non-uniform recoverable lateral resolution across depth. Moreover, reconstructions using state-of-the-art approaches suﬀer from strong artifacts at axial ranges, where the LFM samples the light ﬁeld at a coarse rate. In this work, we analyze the sampling patterns of the LFM, and introduce a ﬂexible light ﬁeld point spread function model (LFPSF) to cope with arbitrary LFM designs. We then propose a novel aliasing-aware deconvolution scheme to address the sampling artifacts. We demonstrate the high potential of the proposed method on real experimental data.},
	language = {en},
	number = {22},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Stefanoiu, Anca and Page, Josue and Symvoulidis, Panagiotis and Westmeyer, Gil G. and Lasser, Tobias},
	month = oct,
	year = {2019},
	pages = {31644},
	file = {Stefanoiu et al. - 2019 - Artifact-free deconvolution in light field microsc.pdf:/home/david/Zotero/storage/QDEF2JHT/Stefanoiu et al. - 2019 - Artifact-free deconvolution in light field microsc.pdf:application/pdf},
}

@article{wieczorek_brain_2018,
	title = {Brain {Connectivity} {Exposed} by {Anisotropic} {X}-ray {Dark}-field {Tomography}},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-32023-y},
	doi = {10.1038/s41598-018-32023-y},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Wieczorek, Matthias and Schaff, Florian and Jud, Christoph and Pfeiffer, Daniela and Pfeiffer, Franz and Lasser, Tobias},
	month = dec,
	year = {2018},
	pages = {14345},
	file = {Wieczorek et al. - 2018 - Brain Connectivity Exposed by Anisotropic X-ray Da.pdf:/home/david/Zotero/storage/FAUJDD33/Wieczorek et al. - 2018 - Brain Connectivity Exposed by Anisotropic X-ray Da.pdf:application/pdf},
}

@article{sharma_design_2017,
	title = {Design of {Acquisition} {Schemes} and {Setup} {Geometry} for {Anisotropic} {X}-ray {Dark}-{Field} {Tomography} ({AXDT})},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-03329-0},
	doi = {10.1038/s41598-017-03329-0},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Sharma, Y. and Schaff, F. and Wieczorek, M. and Pfeiffer, F. and Lasser, T.},
	month = dec,
	year = {2017},
	pages = {3195},
	file = {Sharma et al. - 2017 - Design of Acquisition Schemes and Setup Geometry f.pdf:/home/david/Zotero/storage/JK6UFIHT/Sharma et al. - 2017 - Design of Acquisition Schemes and Setup Geometry f.pdf:application/pdf},
}

@article{pfeiffer_hard-x-ray_2008,
	title = {Hard-{X}-ray dark-field imaging using a grating interferometer},
	volume = {7},
	issn = {1476-1122, 1476-4660},
	url = {http://www.nature.com/articles/nmat2096},
	doi = {10.1038/nmat2096},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Nature Materials},
	author = {Pfeiffer, F. and Bech, M. and Bunk, O. and Kraft, P. and Eikenberry, E. F. and Brönnimann, Ch. and Grünzweig, C. and David, C.},
	month = feb,
	year = {2008},
	pages = {134--137},
	file = {Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:/home/david/Zotero/storage/JC5CASMM/Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:application/pdf},
}

@article{pfeiffer_phase_2006,
	title = {Phase retrieval and differential phase-contrast imaging with low-brilliance {X}-ray sources},
	volume = {2},
	issn = {1745-2473, 1745-2481},
	url = {http://www.nature.com/articles/nphys265},
	doi = {10.1038/nphys265},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Nature Physics},
	author = {Pfeiffer, Franz and Weitkamp, Timm and Bunk, Oliver and David, Christian},
	month = apr,
	year = {2006},
	pages = {258--261},
	file = {Pfeiffer et al. - 2006 - Phase retrieval and differential phase-contrast im.pdf:/home/david/Zotero/storage/G94ZMJNE/Pfeiffer et al. - 2006 - Phase retrieval and differential phase-contrast im.pdf:application/pdf},
}

@book{natterer_mathematics_1986,
	address = {Wiesbaden},
	title = {The {Mathematics} of {Computerized} {Tomography}},
	isbn = {978-3-519-02103-2 978-3-663-01409-6},
	url = {http://link.springer.com/10.1007/978-3-663-01409-6},
	language = {en},
	urldate = {2022-03-25},
	publisher = {Vieweg+Teubner Verlag},
	author = {Natterer, F.},
	year = {1986},
	doi = {10.1007/978-3-663-01409-6},
	file = {Natterer - 1986 - The Mathematics of Computerized Tomography.pdf:/home/david/Zotero/storage/ZMS3QN4V/Natterer - 1986 - The Mathematics of Computerized Tomography.pdf:application/pdf},
}

@article{hahn_statistical_2015,
	title = {Statistical iterative reconstruction algorithm for {X}-ray phase-contrast {CT}},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10452},
	doi = {10.1038/srep10452},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Hahn, Dieter and Thibault, Pierre and Fehringer, Andreas and Bech, Martin and Koehler, Thomas and Pfeiffer, Franz and Noël, Peter B.},
	month = sep,
	year = {2015},
	pages = {10452},
	file = {Hahn et al. - 2015 - Statistical iterative reconstruction algorithm for.pdf:/home/david/Zotero/storage/PXQS9TA9/Hahn et al. - 2015 - Statistical iterative reconstruction algorithm for.pdf:application/pdf},
}

@phdthesis{hahn_statistical_2014,
	address = {München},
	type = {Dissertation},
	title = {Statistical {Iterative} {Reconstruction} for {X}-ray {Phase}-{Contrast} {Computed} {Tomography}},
	school = {Technische Universität München},
	author = {Hahn, Dieter},
	year = {2014},
	keywords = {X-ray imaging, computed tomography, reconstruction, differential phase contrast},
	file = {Hahn - 2014 - Statistical Iterative Reconstruction for X-ray Pha.pdf:/home/david/Zotero/storage/82DSI6KV/Hahn - 2014 - Statistical Iterative Reconstruction for X-ray Pha.pdf:application/pdf},
}

@phdthesis{hehn_model-based_2020,
	address = {München},
	type = {Dissertation},
	title = {Model-{Based} {Iterative} {Reconstruction} for {X}-ray {Computed} {Tomography} {Using} {Attenuation} and {Propagation}-{Based} {Phase} {Contrast}},
	school = {Technische Universität München},
	author = {Hehn, Lorenz},
	year = {2020},
	file = {Hehn - 2020 - Model-Based Iterative Reconstruction for X-ray Com.pdf:/home/david/Zotero/storage/6GE3FT5F/Hehn - 2020 - Model-Based Iterative Reconstruction for X-ray Com.pdf:application/pdf},
}

@article{mcdonald_advanced_2009,
	title = {Advanced phase-contrast imaging using a grating interferometer},
	volume = {16},
	issn = {0909-0495},
	url = {http://scripts.iucr.org/cgi-bin/paper?S0909049509017920},
	doi = {10.1107/S0909049509017920},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Journal of Synchrotron Radiation},
	author = {McDonald, Samuel Alan and Marone, Federica and Hintermüller, Christoph and Mikuljan, Gordan and David, Christian and Pfeiffer, Franz and Stampanoni, Marco},
	month = jul,
	year = {2009},
	pages = {562--572},
	file = {McDonald et al. - 2009 - Advanced phase-contrast imaging using a grating in.pdf:/home/david/Zotero/storage/Y8932Y92/McDonald et al. - 2009 - Advanced phase-contrast imaging using a grating in.pdf:application/pdf},
}

@article{teuffenbach_grating-based_2017-1,
	title = {Grating-based phase-contrast and dark-field computed tomography: a single-shot method},
	volume = {7},
	issn = {2045-2322},
	shorttitle = {Grating-based phase-contrast and dark-field computed tomography},
	url = {http://www.nature.com/articles/s41598-017-06729-4},
	doi = {10.1038/s41598-017-06729-4},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Teuffenbach, Maximilian von and Koehler, Thomas and Fehringer, Andreas and Viermetz, Manuel and Brendel, Bernhard and Herzen, Julia and Proksa, Roland and Rummeny, Ernst J. and Pfeiffer, Franz and Noël, Peter B.},
	month = dec,
	year = {2017},
	pages = {7476},
	file = {Teuffenbach et al. - 2017 - Grating-based phase-contrast and dark-field comput.pdf:/home/david/Zotero/storage/X9P6758U/Teuffenbach et al. - 2017 - Grating-based phase-contrast and dark-field comput.pdf:application/pdf},
}

@article{shanblatt_forward_2019,
	title = {Forward model for propagation-based x-ray phase contrast imaging in parallel- and cone-beam geometry},
	volume = {27},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-27-4-4504},
	doi = {10.1364/OE.27.004504},
	abstract = {We demonstrate a fast, flexible, and accurate paraxial wave propagation model to serve as a forward model for propagation-based X-ray phase contrast imaging (XPCI) in parallel-beam or cone-beam geometry. This model incorporates geometric cone-beam effects into the multi-slice beam propagation method. It enables rapid prototyping and is well suited to serve as a forward model for propagation-based X-ray phase contrast tomographic reconstructions. Furthermore, it is capable of modeling arbitrary objects, including those that are strongly or multi-scattering. Simulation studies were conducted to compare our model to other forward models in the X-ray regime, such as the Mie and full-wave Rytov solutions.},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Shanblatt, Elisabeth R. and Sung, Yongjin and Gupta, Rajiv and Nelson, Brandon J. and Leng, Shuai and Graves, William S. and McCollough, Cynthia H.},
	month = feb,
	year = {2019},
	pages = {4504},
	file = {Shanblatt et al. - 2019 - Forward model for propagation-based x-ray phase co.pdf:/home/david/Zotero/storage/EVYXJM7C/Shanblatt et al. - 2019 - Forward model for propagation-based x-ray phase co.pdf:application/pdf},
}

@article{pfeiffer_tomographic_2007,
	title = {Tomographic reconstruction of three-dimensional objects from hard {X}-ray differential phase contrast projection images},
	volume = {580},
	issn = {01689002},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168900207012910},
	doi = {10.1016/j.nima.2007.06.104},
	abstract = {We report on a method for tomographic phase contrast imaging of centimeter sized objects. As opposed to existing techniques, our approach can be used with low-brilliance, lab based X-ray sources and thus is of interest to a wide range of applications in medicine, biology, and non-destructive testing. The work is based on the recent development of a hard X-ray grating interferometer, which has been demonstrated to yield differential phase contrast projection images. Here we particularly focus on how this method can be used for tomographic reconstructions using ﬁltered backprojection algorithms to yield quantitative volumetric information of the real part of the samples’s refractive index.},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	author = {Pfeiffer, F. and Bunk, O. and Kottler, C. and David, C.},
	month = oct,
	year = {2007},
	pages = {925--928},
	file = {Pfeiffer et al. - 2007 - Tomographic reconstruction of three-dimensional ob.pdf:/home/david/Zotero/storage/WRRB2EGW/Pfeiffer et al. - 2007 - Tomographic reconstruction of three-dimensional ob.pdf:application/pdf},
}

@article{schmitt_analysis_nodate-1,
	title = {Analysis of bias induced by various forward projection models in iterative reconstruction},
	abstract = {Discrete representation of the CT image is a major step in the design of iterative reconstruction algorithms, particularly because the decision being made at this level affects both bias and noise properties of the reconstruction, in addition to choices made later in the algorithm design. In this work, we examine the bias induced by popular image representation models, namely Joseph’s method and the basis function approach relying on B-splines and blobs. Our preliminary results highlight a common weakness in terms of overshoot and undershoot artifacts at sharp boundaries. They also show that the Blobs may perform only as well as the B-spline of order two in terms of bias, and that Joseph’s method tends to produce results that are fairly comparable to the B-spline of order one, with a slight advantage in favor of the latter.},
	language = {en},
	author = {Schmitt, K and Schondube, H and Stierstorfer, K and Hornegger, J and Noo, F},
	pages = {5},
	file = {Schmitt et al. - Analysis of bias induced by various forward projec.pdf:/home/david/Zotero/storage/YGP889DV/Schmitt et al. - Analysis of bias induced by various forward projec.pdf:application/pdf},
}

@article{chapdelaine_new_nodate,
	title = {New {GPU} implementation of {Separable} {Footprint} ({SF}) {Projector} and {Backprojector}: first results},
	abstract = {Model-based iterative reconstruction methods enable to improve the quality of reconstruction in 3D X-ray Computed Tomography (CT). The main computational burden of these methods lies in successive projection and backprojection operations. Among existing pairs of projector and backprojector, Separable Footprint (SF) pair combines computational efﬁciency and accurate modelling of X-rays passing through the volume to image. In order to accelerate these operators, implementations on Graphical Processor Units (GPUs) for parallel-computing have been proposed for SF pair. Due to a CPU-loop, these implementations involve many memory transfers between CPU and GPU which are known to be the main bottleneck for GPU computing. In this paper, we investigate a new GPU implementation of SF projector and backprojector in order to minimize these memory transfers. Our proposed GPU SF projector and backprojector have no CPU-loop, and use two ray-driven kernels for the projection and one voxel-driven kernel for the backprojection. After having described their implementations, we study these operators as single modules and validate it in a MBIR method. Perspectives for this work are GPU optimizations and comparisons with the other existing implementations of SF pair.},
	language = {en},
	author = {Chapdelaine, Camille and Gac, Nicolas and Djafari, Ali-Mohammad and Parra-Denis, Estelle},
	pages = {5},
	file = {Chapdelaine et al. - New GPU implementation of Separable Footprint (SF).pdf:/home/david/Zotero/storage/PQY2XH2L/Chapdelaine et al. - New GPU implementation of Separable Footprint (SF).pdf:application/pdf},
}

@article{garcia_latex_nodate,
	title = {{LATEX} and the different bibliography styles},
	language = {en},
	author = {Garcia, Federico},
	pages = {14},
	file = {Garcia - LATEX and the different bibliography styles.pdf:/home/david/Zotero/storage/5H7SVNVW/Garcia - LATEX and the different bibliography styles.pdf:application/pdf},
}

@article{amanatides_fast_nodate,
	title = {A {Fast} {Voxel} {Traversal} {Algorithm} for {Ray} {Tracing}},
	abstract = {A fast and simple voxel traversal algorithm through a 3D space partition is introduced. Going from one voxel to its neighbour requires only two ﬂoating point comparisons and one ﬂoating point addition. Also, multiple ray intersections with objects that are in more than one voxel are eliminated.},
	language = {en},
	author = {Amanatides, John and Woo, Andrew},
	pages = {6},
	file = {Amanatides and Woo - A Fast Voxel Traversal Algorithm for Ray Tracing.pdf:/home/david/Zotero/storage/U3CDVTRD/Amanatides and Woo - A Fast Voxel Traversal Algorithm for Ray Tracing.pdf:application/pdf},
}

@article{de_greef_accelerated_2009,
	title = {Accelerated ray tracing for radiotherapy dose calculations on a {GPU}: {GPU} accelerated dose calculation},
	volume = {36},
	issn = {00942405},
	shorttitle = {Accelerated ray tracing for radiotherapy dose calculations on a {GPU}},
	url = {http://doi.wiley.com/10.1118/1.3190156},
	doi = {10.1118/1.3190156},
	abstract = {Purpose: The graphical processing unit ͑GPU͒ on modern graphics cards offers the possibility of accelerating arithmetically intensive tasks. By splitting the work into a large number of independent jobs, order-of-magnitude speedups are reported. In this article, the possible speedup of PLATO’s ray tracing algorithm for dose calculations using a GPU is investigated.
Methods: A GPU version of the ray tracing algorithm was implemented using NVIDIA’s CUDA, which extends the standard C language with functionality to program graphics cards. The developed algorithm was compared based on the accuracy and speed to a multithreaded version of the PLATO ray tracing algorithm. This comparison was performed for three test geometries, a phantom and two radiotherapy planning CT datasets ͑a pelvic and a head-and-neck case͒. For each geometry, four different source positions were evaluated. In addition to this, for the head-and-neck case also a vertex ﬁeld was evaluated.
Results: The GPU algorithm was proven to be more accurate than the PLATO algorithm by elimination of the look-up table for z indices that introduces discretization errors in the reference algorithm. Speedups for ray tracing were found to be in the range of 2.1–10.1, relative to the multithreaded PLATO algorithm running four threads. For dose calculations the speedup measured was in the range of 1.5–6.2. For the speedup of both the ray tracing and the dose calculation, a strong dependency on the tested geometry was found. This dependency is related to the fraction of air within the patient’s bounding box resulting in idle threads.
Conclusions: With the use of a GPU, ray tracing for dose calculations can be performed accurately in considerably less time. Ray tracing was accelerated, on average, with a factor of 6 for the evaluated cases. Dose calculation for a single beam can typically be carried out in 0.6– 0.9 s for clinically realistic datasets. These ﬁndings can be used in conventional planning to enable ͑nearly͒ real-time dose calculations. Also the importance for treatment optimization techniques is evident. © 2009 American Association of Physicists in Medicine. ͓DOI: 10.1118/1.3190156͔},
	language = {en},
	number = {9Part1},
	urldate = {2022-03-27},
	journal = {Medical Physics},
	author = {de Greef, M. and Crezee, J. and van Eijk, J. C. and Pool, R. and Bel, A.},
	month = aug,
	year = {2009},
	pages = {4095--4102},
	file = {de Greef et al. - 2009 - Accelerated ray tracing for radiotherapy dose calc.pdf:/home/david/Zotero/storage/WCQXHS88/de Greef et al. - 2009 - Accelerated ray tracing for radiotherapy dose calc.pdf:application/pdf},
}

@article{xiao_efficient_2012,
	title = {Efficient implementation of the {3D}‐{DDA} ray traversal algorithm on {GPU} and its application in radiation dose calculation},
	volume = {39},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1118/1.4767755},
	doi = {10.1118/1.4767755},
	abstract = {Purpose: The three-dimensional digital differential analyzer (3D-DDA) algorithm is a widely used ray traversal method, which is also at the core of many convolution/superposition (C/S) dose calculation approaches. However, porting existing C/S dose calculation methods onto graphics processing unit (GPU) has brought challenges to retaining the efﬁciency of this algorithm. In particular, straightforward implementation of the original 3D-DDA algorithm inﬂicts a lot of branch divergence which conﬂicts with the GPU programming model and leads to suboptimal performance. In this paper, an efﬁcient GPU implementation of the 3D-DDA algorithm is proposed, which effectively reduces such branch divergence and improves performance of the C/S dose calculation programs running on GPU.
Methods: The main idea of the proposed method is to convert a number of conditional statements in the original 3D-DDA algorithm into a set of simple operations (e.g., arithmetic, comparison, and logic) which are better supported by the GPU architecture. To verify and demonstrate the performance improvement, this ray traversal method was integrated into a GPU-based collapsed cone convolution/superposition (CCCS) dose calculation program.
Results: The proposed method has been tested using a water phantom and various clinical cases on an NVIDIA GTX570 GPU. The CCCS dose calculation program based on the efﬁcient 3D-DDA ray traversal implementation runs 1.42 ∼ 2.67× faster than the one based on the original 3D-DDA implementation, without losing any accuracy.
Conclusions: The results show that the proposed method can effectively reduce branch divergence in the original 3D-DDA ray traversal algorithm and improve the performance of the CCCS program running on GPU. Considering the wide utilization of the 3D-DDA algorithm, various applications can beneﬁt from this implementation method. © 2012 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4767755]},
	language = {en},
	number = {12},
	urldate = {2022-03-27},
	journal = {Medical Physics},
	author = {Xiao, Kai and Chen, Danny Z. and Hu, X. Sharon and Zhou, Bo},
	month = dec,
	year = {2012},
	pages = {7619--7625},
	file = {Xiao et al. - 2012 - Efficient implementation of the 3D‐DDA ray travers.pdf:/home/david/Zotero/storage/Y6DPWJTU/Xiao et al. - 2012 - Efficient implementation of the 3D‐DDA ray travers.pdf:application/pdf},
}

@article{thompson_gpu_nodate,
	title = {{GPU} {Accelerated} {Structure}-{Exploiting} {Matched} {Forward} and {Back} {Projection} for {Algebraic} {Iterative} {Cone} {Beam} {CT} {Reconstruction}},
	abstract = {Algebraic iterative reconstruction algorithms have been the subject of much research into problems of limited data CT reconstruction. However, their use for practical problems, particularly for high resolution systems and corresponding large volume sizes, has often been considered unfeasible due to their high computational demands. For cone beam geometry, we present a highly parallel adaptation of Siddon’s algorithm for discrete forward and back projection, based on exploitation of structure in the pattern of the cone beam rays. The proposed algorithm has been implemented for nVidia GPUs using CUDA, resulting in speedup of an order of magnitude when tested against a non-structure exploiting parallel CPU implementation of Jacobs’ algorithm. The work is presented in the context of circular scan, ﬂat panel detector micro CT, but could easily be adapted to other scanning trajectories and detector conﬁgurations.},
	language = {en},
	author = {Thompson, William M and Lionheart, William R B},
	pages = {5},
	file = {Thompson and Lionheart - GPU Accelerated Structure-Exploiting Matched Forwa.pdf:/home/david/Zotero/storage/K3CXQVKY/Thompson and Lionheart - GPU Accelerated Structure-Exploiting Matched Forwa.pdf:application/pdf},
}

@article{ha_study_nodate,
	title = {A {Study} of {Volume} {Integration} {Models} for {Iterative} {Cone}-{Beam} {Computed} {Tomography}},
	abstract = {With the help of modern parallel computers, iterative reconstruction algorithms have become a feasible research topic in the field of CT. These types of algorithms can greatly benefit from an accurate, realistic CT system model. In our study, we model the CT projection as volume integrals and propose a set of methods that can compute the volume integrals either exactly or approximately. Our approximate volume integral methods have a much smaller complexity algorithmically than the exact method, but their accuracy is close to it. More importantly, the proposed approximate methods can be easily ported to modern parallel processors to utilize their massive computation powers.},
	language = {en},
	author = {Ha, Sungsoo and Kumar, Ayush and Mueller, Klaus},
	pages = {4},
	file = {Ha et al. - A Study of Volume Integration Models for Iterative.pdf:/home/david/Zotero/storage/2HNPEQZS/Ha et al. - A Study of Volume Integration Models for Iterative.pdf:application/pdf},
}

@inproceedings{xu_comparative_2006,
	title = {A comparative study of popular interpolation and integration methods for use in computed tomography},
	doi = {10.1109/ISBI.2006.1625152},
	abstract = {We compare various popular methods available for projection and backprojection in CT. Assuming linear rays and a simple density integration along them, we consider both line- and area-based methods. Here, two key components govern the quality of a projection result, given the discrete nature of the data and reconstruction result: interpolation and integration. Both of these are studied here. In order to separate these fundamental issues from those related to perspective fan and cone-beam effects, we restrict ourselves to a parallel-beam projection geometry. We also compare these different methods in light of a possible efficient implementation on programmable commodity graphics hardware (GPUs). To this end, we propose a new method for interpolation based on hexagonal subsampling, which achieves superior results. In order to achieve a data-independent comparison, we employ a dataset of very high and uniform frequency content, the so-called Marschner-Lobb dataset},
	booktitle = {3rd {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {Nano} to {Macro}, 2006.},
	author = {Xu, Fang and Mueller, K.},
	month = apr,
	year = {2006},
	note = {ISSN: 1945-8452},
	keywords = {Interpolation, Kernel, Computed tomography, Computer science, Equations, Filters, Frequency, Taylor series, Testing, Visualization},
	pages = {1252--1255},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/AS88X5WZ/Xu and Mueller - 2006 - A comparative study of popular interpolation and i.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/2Q3LFSYH/1625152.html:text/html},
}

@article{fang_xu_accelerating_2005,
	title = {Accelerating popular tomographic reconstruction algorithms on commodity {PC} graphics hardware},
	volume = {52},
	issn = {0018-9499},
	url = {http://ieeexplore.ieee.org/document/1487696/},
	doi = {10.1109/TNS.2005.851398},
	abstract = {The task of reconstructing an object from its projections via tomographic methods is a time-consuming process due to the vast complexity of the data. For this reason, manufacturers of equipment for medical computed tomography (CT) rely mostly on special application speciﬁed integrated circuits (ASICs) to obtain the fast reconstruction times required in clinical settings. Although modern CPUs have gained sufﬁcient power in recent years to be competitive for two-dimensional (2D) reconstruction, this is not the case for three-dimensional (3D) reconstructions, especially not when iterative algorithms must be applied. The recent evolution of commodity PC computer graphics boards (GPUs) has the potential to change this picture in a very dramatic way. In this paper we will show how the new ﬂoating point GPUs can be exploited to perform both analytical and iterative reconstruction from X-ray and functional imaging data. For this purpose, we decompose three popular three-dimensional (3D) reconstruction algorithms (Feldkamp ﬁltered backprojection, the simultaneous algebraic reconstruction technique, and expectation maximization) into a common set of base modules, which all can be executed on the GPU and their output linked internally. Visualization of the reconstructed object is easily achieved since the object already resides in the graphics hardware, allowing one to run a visualization module at any time to view the reconstruction results. Our implementation allows speedups of over an order of magnitude with respect to CPU implementations, at comparable image quality.},
	language = {en},
	number = {3},
	urldate = {2022-03-27},
	journal = {IEEE Transactions on Nuclear Science},
	author = {{Fang Xu} and Mueller, K.},
	month = jun,
	year = {2005},
	pages = {654--663},
	file = {Fang Xu and Mueller - 2005 - Accelerating popular tomographic reconstruction al.pdf:/home/david/Zotero/storage/HTQ4QZLQ/Fang Xu and Mueller - 2005 - Accelerating popular tomographic reconstruction al.pdf:application/pdf},
}

@article{hahn_comparison_2016,
	title = {A comparison of linear interpolation models for iterative {CT} reconstruction: {Linear} interpolation models in iterative {CT} reconstruction},
	volume = {43},
	issn = {00942405},
	shorttitle = {A comparison of linear interpolation models for iterative {CT} reconstruction},
	url = {http://doi.wiley.com/10.1118/1.4966134},
	doi = {10.1118/1.4966134},
	abstract = {Purpose: Recent reports indicate that model-based iterative reconstruction methods may improve image quality in computed tomography (CT). One difficulty with these methods is the number of options available to implement them, including the selection of the forward projection model and the penalty term. Currently, the literature is fairly scarce in terms of guidance regarding this selection step, whereas these options impact image quality. Here, the authors investigate the merits of three forward projection models that rely on linear interpolation: the distance-driven method, Joseph’s method, and the bilinear method. The authors’ selection is motivated by three factors: (1) in CT, linear interpolation is often seen as a suitable trade-off between discretization errors and computational cost, (2) the first two methods are popular with manufacturers, and (3) the third method enables assessing the importance of a key assumption in the other methods.
Methods: One approach to evaluate forward projection models is to inspect their effect on discretized images, as well as the effect of their transpose on data sets, but significance of such studies is unclear since the matrix and its transpose are always jointly used in iterative reconstruction. Another approach is to investigate the models in the context they are used, i.e., together with statistical weights and a penalty term. Unfortunately, this approach requires the selection of a preferred objective function and does not provide clear information on features that are intrinsic to the model. The authors adopted the following two-stage methodology. First, the authors analyze images that progressively include components of the singular value decomposition of the model in a reconstructed image without statistical weights and penalty term. Next, the authors examine the impact of weights and penalty on observed differences.
Results: Image quality metrics were investigated for 16 different fan-beam imaging scenarios that enabled probing various aspects of all models. The metrics include a surrogate for computational cost, as well as bias, noise, and an estimation task, all at matched resolution. The analysis revealed fundamental differences in terms of both bias and noise. Task-based assessment appears to be required to appreciate the differences in noise; the estimation task the authors selected showed that these differences balance out to yield similar performance. Some scenarios highlighted merits for the distance-driven method in terms of bias but with an increase in computational cost. Three combinations of statistical weights and penalty term showed that the observed differences remain the same, but strong edge-preserving penalty can dramatically reduce the magnitude of these differences.
Conclusions: In many scenarios, Joseph’s method seems to offer an interesting compromise between cost and computational effort. The distance-driven method offers the possibility to reduce bias but with an increase in computational cost. The bilinear method indicated that a key assumption in the other two methods is highly robust. Last, strong edge-preserving penalty can act as a compensator for insufficiencies in the forward projection model, bringing all models to similar levels in the most challenging imaging scenarios. Also, the authors find that their evaluation methodology helps appreciating how model, statistical weights, and penalty term interplay together. C 2016 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4966134]},
	language = {en},
	number = {12},
	urldate = {2022-03-27},
	journal = {Medical Physics},
	author = {Hahn, Katharina and Schöndube, Harald and Stierstorfer, Karl and Hornegger, Joachim and Noo, Frédéric},
	month = nov,
	year = {2016},
	pages = {6455--6473},
	file = {Hahn et al. - 2016 - A comparison of linear interpolation models for it.pdf:/home/david/Zotero/storage/PZIIYIYV/Hahn et al. - 2016 - A comparison of linear interpolation models for it.pdf:application/pdf},
}

@inproceedings{wang_image_2011,
	address = {Valencia, Spain},
	title = {Image representation by blob and {CT} reconstruction from few projections},
	isbn = {978-1-4673-0120-6 978-1-4673-0118-3 978-1-4673-0119-0},
	url = {http://ieeexplore.ieee.org/document/6153755/},
	doi = {10.1109/NSSMIC.2011.6153755},
	abstract = {The localized radial symmetric function, or the blob, is an ideal alternative to the pixel for CT image reconstruction. In this paper we develop image representation models using blob and propose reconstruction methods for few projections CT data. The image is represented by a single scale Gaussian blob or multiscale blobs of different frequency selectivity, and is reconstructed by minimizing the Total Variation or the ℓ1 norm of blob coefﬁcients. Some 2D numerical results are presented, where we use the GPU platform for fast numerical evaluations of the operations such as the X-Ray projection and back projection.},
	language = {en},
	urldate = {2022-03-27},
	booktitle = {2011 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	publisher = {IEEE},
	author = {Wang, Han and Desbat, Laurent and Legoupil, Samuel},
	month = oct,
	year = {2011},
	pages = {3971--3976},
	file = {Wang et al. - 2011 - Image representation by blob and CT reconstruction.pdf:/home/david/Zotero/storage/E34AQ7WT/Wang et al. - 2011 - Image representation by blob and CT reconstruction.pdf:application/pdf},
}

@book{noauthor_seven_nodate,
	title = {Seven {Languages} in {Seven} {Weeks}},
	isbn = {978-1-68050-005-9},
	url = {https://learning.oreilly.com/library/view/seven-languages-in/9781680500059/},
	abstract = {You should learn a programming language every year, as recommended by The Pragmatic Programmer. But if one per year is good, how about Seven Languages in Seven Weeks? In this book you'll get a...},
	language = {en},
	urldate = {2022-03-28},
	file = {Snapshot:/home/david/Zotero/storage/5SPKEFY9/9781680500059.html:text/html},
}

@book{noauthor_seven_nodate-1,
	title = {Seven {More} {Languages} in {Seven} {Weeks}},
	isbn = {978-1-68050-051-6},
	url = {https://learning.oreilly.com/library/view/seven-more-languages/9781680500516/},
	abstract = {Great programmers aren't born--they're made. The industry is moving from object-oriented languages to functional languages, and you need to commit to radical improvement. New programming languages...},
	language = {en},
	urldate = {2022-03-28},
	file = {Snapshot:/home/david/Zotero/storage/M85KH37M/9781680500516.html:text/html},
}

@misc{noauthor_modern_nodate,
	title = {Modern {C}++ {Design} {Patterns}},
	url = {https://learning.oreilly.com/videos/modern-c-design/9781491978719/},
	abstract = {This course provides beginning to intermediate C++ developers with the knowledge required for up-to-date C++ programming. It begins by introducing the concepts of design patterns and idioms,...},
	language = {en},
	urldate = {2022-03-28},
	journal = {O'Reilly Online Learning},
}

@book{noauthor_c_nodate,
	title = {C++ {Software} {Design}},
	isbn = {978-1-09-811315-5},
	url = {https://learning.oreilly.com/library/view/c-software-design/9781098113155/},
	abstract = {Good software design is essential for the success of your project, but designing software is hard to do. You need to have a deep understanding of the consequences of design decisions and a good...},
	language = {en},
	urldate = {2022-03-28},
}

@book{noauthor_modern_nodate-1,
	title = {The {Modern} {C}++ {Challenge}},
	isbn = {978-1-78899-386-9},
	url = {https://learning.oreilly.com/library/view/the-modern-c/9781788993869/},
	abstract = {Test your C++ programming skills by solving real-world programming problems covered in the bookAbout This BookSolve a variety of real-world programming and logic problems by leveraging the power...},
	language = {en},
	urldate = {2022-03-28},
}

@book{noauthor_software_nodate,
	title = {Software {Architecture} with {C}++},
	isbn = {978-1-83855-459-0},
	url = {https://learning.oreilly.com/library/view/software-architecture-with/9781838554590/},
	abstract = {Apply business requirements to IT infrastructure and deliver a high-quality product by understanding architectures such as microservices, DevOps, and cloud-native using modern C++ standards and...},
	language = {en},
	urldate = {2022-03-28},
}

@book{noauthor_modern_nodate-2,
	title = {Modern {C}++: {Efficient} and {Scalable} {Application} {Development}},
	isbn = {978-1-78995-173-8},
	shorttitle = {Modern {C}++},
	url = {https://learning.oreilly.com/library/view/modern-c-efficient/9781789951738/},
	abstract = {Create apps in C++ and leverage its latest features using modern programming techniques.Key FeaturesDevelop strong C++ skills to build a variety of applications
Explore features of C++17, such as...},
	language = {en},
	urldate = {2022-03-28},
	file = {Snapshot:/home/david/Zotero/storage/EG7CHZZ8/9781789951738.html:text/html},
}

@book{noauthor_modern_nodate-3,
	title = {Modern {C}++ {Design}: {Generic} {Programming} and {Design} {Patterns} {Applied}},
	isbn = {978-0-201-70431-0},
	shorttitle = {Modern {C}++ {Design}},
	url = {https://learning.oreilly.com/library/view/modern-c-design/0201704315/},
	abstract = {Modern C++ Designis an important book. Fundamentally,
it demonstrates ‘generic patterns’ or ‘pattern
templates’ as a powerful new way of creating extensible
designs in C++–a new way to combine...},
	language = {en},
	urldate = {2022-03-28},
	file = {Snapshot:/home/david/Zotero/storage/DLS2AE8R/0201704315.html:text/html},
}

@misc{noauthor_diving_nodate,
	title = {Diving {Deeper} into {C}++ {Templates}},
	url = {https://learning.oreilly.com/videos/diving-deeper-into/9781491988701/},
	abstract = {Templates form the foundation of modern C++ programming; they must be understood, not only to use the Standard Library, but to take advantage of many of the features found in C++11 and C++14. By...},
	language = {en},
	urldate = {2022-03-28},
	journal = {O'Reilly Online Learning},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {C}++ {Templates}},
	url = {https://learning.oreilly.com/videos/introduction-to-c/9781491988688/},
	abstract = {Templates are a fundamental feature of modern C++ programming. Designed for the beginner to intermediate level C++ programmer, this course explains why templates are important and how to use them....},
	language = {en},
	urldate = {2022-03-28},
	journal = {O'Reilly Online Learning},
}

@misc{noauthor_further_nodate,
	title = {Further {Exploration} of {C}++ {Templates} and {Metaprogramming}},
	url = {https://learning.oreilly.com/videos/further-exploration-of/9781491988732/},
	abstract = {Designed for intermediate level C++ programmers, this course focuses on the advanced features and techniques made possible by the release of C++11 and C++14. It introduces several new Standard...},
	language = {en},
	urldate = {2022-03-28},
	journal = {O'Reilly Online Learning},
}

@book{noauthor_c_nodate-1,
	title = {C++ {Templates}: {The} {Complete} {Guide}, 2nd {Edition}},
	isbn = {978-0-13-477880-8},
	shorttitle = {C++ {Templates}},
	url = {https://learning.oreilly.com/library/view/c-templates-the/9780134778808/},
	abstract = {Templates are among the most powerful features of C++, but they remain misunderstood and underutilized, even as the C++ language and development community have advanced. In C++ Templates, Second...},
	language = {en},
	urldate = {2022-03-28},
}

@book{noauthor_expert_nodate,
	title = {Expert {C}++},
	isbn = {978-1-83855-265-7},
	url = {https://learning.oreilly.com/library/view/expert-c/9781838552657/},
	abstract = {Design and architect real-world scalable C++ applications by exploring advanced techniques in low-level programming, object-oriented programming (OOP), the Standard Template Library (STL),...},
	language = {en},
	urldate = {2022-03-28},
}

@misc{noauthor_c_nodate-2,
	title = {C++ {Concurrency} in {Action}, {Second} {Edition}, {Video} {Edition}},
	url = {https://learning.oreilly.com/videos/c-concurrency-in/9781617294693VE/},
	abstract = {In Video Editions the narrator reads the book while the content, figures, code listings, diagrams, and text appear on the screen. Like an audiobook that you can also watch as a video. "This book...},
	language = {en},
	urldate = {2022-03-28},
	journal = {O'Reilly Online Learning},
	file = {Snapshot:/home/david/Zotero/storage/IIN6YACM/9781617294693VE.html:text/html},
}

@book{bhargava_grokking_2016,
	address = {Shelter Island},
	title = {Grokking algorithms: an illustrated guide for programmers and other curious people},
	isbn = {978-1-61729-223-1},
	shorttitle = {Grokking algorithms},
	abstract = {"Grokking Algorithms is a fully illustrated, friendly guide that teaches you how to apply common algorithms to the practical problems you face every day as a programmer. You'll start with sorting and searching and, as you build up your skills in thinking algorithmically, you'll tackle more complex concerns such as data compression and artificial intelligence. Each carefully presented example includes helpful diagrams and fully annotated code samples in Python."--Publisher's desription},
	language = {en},
	publisher = {Manning},
	author = {Bhargava, Aditya Y.},
	year = {2016},
	keywords = {Computer algorithms, Computer programming, Handbooks, manuals, etc},
	file = {Bhargava - 2016 - Grokking algorithms an illustrated guide for prog.pdf:/home/david/Zotero/storage/M7BKDQXZ/Bhargava - 2016 - Grokking algorithms an illustrated guide for prog.pdf:application/pdf},
}

@article{bergin_history_2007,
	title = {A history of the history of programming languages},
	volume = {50},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1230819.1230841},
	doi = {10.1145/1230819.1230841},
	abstract = {"If I have seen further it is by standing on the shoulders of giants."
              ---Isaac Newton, in a letter to Robert Hooke, Feb. 15, 1676},
	language = {en},
	number = {5},
	urldate = {2022-03-28},
	journal = {Communications of the ACM},
	author = {Bergin, Thomas J. (Tim)},
	month = may,
	year = {2007},
	pages = {69--74},
	file = {Bergin - 2007 - A history of the history of programming languages.pdf:/home/david/Zotero/storage/XFDICQ5M/Bergin - 2007 - A history of the history of programming languages.pdf:application/pdf},
}

@book{noauthor_mathematics_nodate,
	title = {From {Mathematics} to {Generic} {Programming}},
	isbn = {978-0-13-349179-1},
	url = {https://learning.oreilly.com/library/view/from-mathematics-to/9780133491791/},
	abstract = {In this substantive yet accessible book, pioneering software designer Alexander Stepanov and his colleague Daniel Rose illuminate the principles of generic programming and the mathematical concept...},
	language = {en},
	urldate = {2022-03-28},
}

@article{kahanwal_towards_2014,
	title = {Towards {High} {Performance} {Computing} ({HPC}) {Through} {Parallel} {Programming} {Paradigms} and {Their} {Principles}},
	volume = {4},
	issn = {18396291},
	url = {http://www.airccse.org/journal/ijpla/papers/4114ijpla04.pdf},
	doi = {10.5121/ijpla.2014.4104},
	abstract = {Nowadays, we are to find out solutions to huge computing problems very rapidly. It brings the idea of parallel computing in which several machines or processors work cooperatively for computational tasks. In the past decades, there are a lot of variations in perceiving the importance of parallelism in computing machines. And it is observed that the parallel computing is a superior solution to many of the computing limitations like speed and density; non-recurring and high cost; and power consumption and heat dissipation etc. The commercial multiprocessors have emerged with lower prices than the mainframe machines and supercomputers machines. In this article the high performance computing (HPC) through parallel programming paradigms (PPPs) are discussed with their constructs and design approaches.},
	language = {en},
	number = {1},
	urldate = {2022-03-28},
	journal = {International Journal of Programming Languages and Applications},
	author = {Kahanwal, Brijender},
	month = jan,
	year = {2014},
	pages = {45--55},
	file = {Kahanwal - 2014 - Towards High Performance Computing (HPC) Through P.pdf:/home/david/Zotero/storage/WM53XRUF/Kahanwal - 2014 - Towards High Performance Computing (HPC) Through P.pdf:application/pdf},
}

@incollection{gabbrielli_how_2010,
	address = {London},
	series = {Undergraduate {Topics} in {Computer} {Science}},
	title = {How to {Describe} a {Programming} {Language}},
	isbn = {978-1-84882-914-5},
	url = {https://doi.org/10.1007/978-1-84882-914-5_2},
	abstract = {A programming language is an artificial formalism in which algorithms can be expressed. For all its artificiality, this formalism remains a language. Its study can thus make good use of the many concepts and tools developed in the last century in linguistics (which studies both natural and artificial languages). Without going into great detail, this chapter poses the problem of what it means to “give” (define or understand) a programming language and which tools can be used in this undertaking. In particular, we will see the distinction between syntax, semantics, pragmatics and implementation. We will introduce context-free grammars (a formal method for the definition of a language’s syntax), together with the notions of derivations and ambiguity. We will describe the logical structure of a compiler, concluding with a brief overview of structured operational semantics.},
	language = {en},
	urldate = {2022-03-28},
	booktitle = {Programming {Languages}: {Principles} and {Paradigms}},
	publisher = {Springer},
	author = {Gabbrielli, Maurizio and Martini, Simone},
	editor = {Gabbrielli, Maurizio and Martini, Simone},
	year = {2010},
	doi = {10.1007/978-1-84882-914-5_2},
	keywords = {Programming Language, Derivation Tree, Natural Language, Object Language, Operational Semantic},
	pages = {27--55},
}

@incollection{fincher_programming_2019,
	edition = {1},
	title = {Programming {Paradigms} and {Beyond}},
	isbn = {978-1-108-65455-5 978-1-108-49673-5 978-1-108-72189-9},
	url = {https://www.cambridge.org/core/product/identifier/9781108654555%23CN-bp-13/type/book_part},
	abstract = {Programming is a central concern of computer science, so its medium—programming languages—should be a focus of computing education. Unfortunately, much of the community lacks useful tools to understand and organize languages, since the standard literature is mired in the ill-defined and even confusing concept of paradigms.},
	language = {en},
	urldate = {2022-03-28},
	booktitle = {The {Cambridge} {Handbook} of {Computing} {Education} {Research}},
	publisher = {Cambridge University Press},
	author = {Krishnamurthi, Shriram and Fisler, Kathi},
	editor = {Fincher, Sally A. and Robins, Anthony V.},
	month = feb,
	year = {2019},
	doi = {10.1017/9781108654555.014},
	pages = {377--413},
	file = {Krishnamurthi and Fisler - 2019 - Programming Paradigms and Beyond.pdf:/home/david/Zotero/storage/RFDLRUCD/Krishnamurthi and Fisler - 2019 - Programming Paradigms and Beyond.pdf:application/pdf},
}

@article{fernandez-villaverde_lectures_nodate,
	title = {({Lectures} on {High}-performance {Computing} for {Economists} {VII})},
	language = {en},
	author = {Fernandez-Villaverde, Jesus and Guerron, Pablo},
	pages = {21},
	file = {Fernandez-Villaverde and Guerron - (Lectures on High-performance Computing for Econom.pdf:/home/david/Zotero/storage/ZNZ4EKAI/Fernandez-Villaverde and Guerron - (Lectures on High-performance Computing for Econom.pdf:application/pdf},
}

@book{striegnitz_joint_2005,
	address = {Jülich},
	series = {{NIC} series},
	title = {Joint proceedings of the {Workshops} on {Multiparadigm} {Programming} with {Object} {Oriented} {Languages} ({MPOOL} '03), {Declarative} {Programming} in the {Context} of {Object} {Oriented} {Languages} ({DP} {COOL} '03)},
	isbn = {978-3-00-016005-9},
	language = {en},
	number = {Vol. 27},
	publisher = {John-von-Neumann-Institute for Computing (NIC)},
	editor = {Striegnitz, Jörg},
	year = {2005},
	note = {Meeting Name: DP COOL},
	file = {Striegnitz - 2005 - Joint proceedings of the Workshops on Multiparadig.pdf:/home/david/Zotero/storage/2WXKY547/Striegnitz - 2005 - Joint proceedings of the Workshops on Multiparadig.pdf:application/pdf},
}

@book{shalom_review_2015,
	title = {A {Review} of {Programming} {Paradigms} {Throughout} the {History}: {With} a {Suggestion} {Toward} a {Future} {Approach}},
	isbn = {978-1-976850-91-2},
	shorttitle = {A {Review} of {Programming} {Paradigms} {Throughout} the {History}},
	language = {English},
	publisher = {Independently published},
	author = {Shalom, Elad},
	month = nov,
	year = {2015},
}

@book{noauthor_variational_2009,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Variational {Methods} in {Imaging}},
	volume = {167},
	isbn = {978-0-387-30931-6 978-0-387-69277-7},
	url = {http://link.springer.com/10.1007/978-0-387-69277-7},
	language = {en},
	urldate = {2022-03-28},
	publisher = {Springer New York},
	year = {2009},
	doi = {10.1007/978-0-387-69277-7},
	note = {ISSN: 0066-5452},
	file = {2009 - Variational Methods in Imaging.pdf:/home/david/Zotero/storage/6ZJZ64W2/2009 - Variational Methods in Imaging.pdf:application/pdf},
}

@book{bergounioux_variational_2017,
	address = {Berlin ; Boston},
	series = {Radon series on computational and applied mathematics},
	title = {Variational methods in imaging and geometric control},
	isbn = {978-3-11-043923-6 978-3-11-043049-3},
	language = {en},
	number = {18},
	publisher = {Walter de Gruyter GmbH},
	editor = {Bergounioux, Maïtine},
	year = {2017},
	keywords = {Image analysis, Image processing, Mathematics, Variational principles},
	file = {Bergounioux - 2017 - Variational methods in imaging and geometric contr.pdf:/home/david/Zotero/storage/3KHIHSDP/Bergounioux - 2017 - Variational methods in imaging and geometric contr.pdf:application/pdf},
}

@phdthesis{toft_radon_1996,
	title = {The {Radon} {Transform} - {Theory} and {Implementation}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Toft, Peter Aundal},
	year = {1996},
	file = {Aundal - The Radon Transform - Theory and Implementation.pdf:/home/david/Zotero/storage/5NXUZM6N/Aundal - The Radon Transform - Theory and Implementation.pdf:application/pdf},
}

@misc{garrett_science_2022,
	title = {Science {Plots}},
	copyright = {MIT},
	url = {https://github.com/garrettj403/SciencePlots},
	abstract = {Matplotlib styles for scientific plotting},
	urldate = {2022-03-28},
	author = {Garrett, John},
	month = mar,
	year = {2022},
	note = {original-date: 2018-08-13T16:23:03Z},
	keywords = {latex, cjk-fonts, ieee-paper, matplotlib-figures, matplotlib-style-sheets, matplotlib-styles, python, scientific-papers, thesis-template},
}

@misc{noauthor_overview_nodate,
	title = {Overview — {Pycairo} documentation},
	url = {https://pycairo.readthedocs.io/en/latest/},
	urldate = {2022-03-28},
}

@misc{noauthor_stdcyl_bessel_i_nodate,
	title = {std::cyl\_bessel\_i, std::cyl\_bessel\_if, std::cyl\_bessel\_il - cppreference.com},
	url = {https://en.cppreference.com/w/cpp/numeric/special_functions/cyl_bessel_i},
	urldate = {2022-03-29},
	file = {std\:\:cyl_bessel_i, std\:\:cyl_bessel_if, std\:\:cyl_bessel_il - cppreference.com:/home/david/Zotero/storage/7LX9RJ8M/cyl_bessel_i.html:text/html},
}

@misc{noauthor_libstdc_nodate-1,
	title = {libstdc++: {Mathematical} {Special} {Functions}},
	url = {https://gcc.gnu.org/onlinedocs/gcc-10.1.0/libstdc++/api/a01503.html},
	urldate = {2022-03-29},
	file = {libstdc++\: Mathematical Special Functions:/home/david/Zotero/storage/HZJF7HT7/a01503.html:text/html},
}

@article{lewitt_multidimensional_1990,
	title = {Multidimensional digital image representations using generalized {Kaiser}–{Bessel} window functions},
	volume = {7},
	url = {http://opg.optica.org/josaa/abstract.cfm?URI=josaa-7-10-1834},
	doi = {10.1364/JOSAA.7.001834},
	abstract = {Inverse problems that require the solution of integral equations are inherent in a number of indirect imaging applications, such as computerized tomography. Numerical solutions based on discretization of the mathematical model of the imaging process, or on discretization of analytic formulas for iterative inversion of the integral equations, require a discrete representation of an underlying continuous image. This paper describes discrete image representations, in n-dimensional space, that are constructed by the superposition of shifted copies of a rotationally symmetric basis function. The basis function is constructed using a generalization of the Kaiser–Bessel window function of digital signal processing. The generalization of the window function involves going from one dimension to a rotationally symmetric function in n dimensions and going from the zero-order modified Bessel function of the standard window to a function involving the modified Bessel function of order m. Three methods are given for the construction, in n-dimensional space, of basis functions having a specified (finite) number of continuous derivatives, and formulas are derived for the Fourier transform, the x-ray transform, the gradient, and the Laplacian of these basis functions. Properties of the new image representations using these basis functions are discussed, primarily in the context of two-dimensional and three-dimensional image reconstruction from line-integral data by iterative inversion of the x-ray transform. Potential applications to three-dimensional image display are also mentioned.},
	number = {10},
	journal = {J. Opt. Soc. Am. A},
	author = {Lewitt, Robert M.},
	month = oct,
	year = {1990},
	note = {Publisher: OSA},
	keywords = {Inverse problems, Image processing, Digital image processing, Fourier transforms, Signal processing, Three dimensional reconstruction},
	pages = {1834--1846},
}

@misc{noauthor_c_nodate-3,
	title = {C++ {Mathematical} {Special} {Functions} {Proposal}},
	url = {http://open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1422.html},
	urldate = {2022-03-29},
	file = {C++ Mathematical Special Functions Proposal:/home/david/Zotero/storage/YT57KPSN/n1422.html:text/html},
}

@article{marabini_3d_1998,
	title = {{3D} reconstruction in electron microscopy using {ART} with smooth spherically symmetric volume elements (blobs)},
	volume = {72},
	issn = {0304-3991},
	url = {https://www.sciencedirect.com/science/article/pii/S0304399197001277},
	doi = {10.1016/S0304-3991(97)00127-7},
	abstract = {Algebraic reconstruction techniques (ART) are iterative procedures for solving systems of linear equations. They have been used in tomography to recover objects from their projections. In this work we apply an ART approach in which the basis functions used to describe the objects are not based on voxels, but are much smoother functions named “blobs”. The data collection studied in this work follows the so-called “conical tilt geometry” that is commonly used in many applications of three-dimensional electron microscopy of biological macromolecules. The performance of ART with blobs is carefully compared with a currently well-known three dimensional (3D) reconstruction algorithm (weighted backprojection) using a methodology which assigns a level of statistical significance to a claim of relative superiority of one algorithm over another for a particular task. The conclusion we reach is that ART with blobs produces high-quality reconstructions and is, in particular, superior to weighted backprojection in recovering features along the “vertical” direction. For the exact implementation recommended in this paper, the computational costs of ART are almost an order of magnitude smaller than those of WBP.},
	language = {en},
	number = {1},
	urldate = {2022-03-29},
	journal = {Ultramicroscopy},
	author = {Marabini, Roberto and Herman, Gabor T and Carazo, José M},
	month = apr,
	year = {1998},
	keywords = {electron microscopy},
	pages = {53--65},
	file = {ScienceDirect Snapshot:/home/david/Zotero/storage/BEQMU6ZU/S0304399197001277.html:text/html},
}

@article{garduno_optimization_2001,
	title = {Optimization of {Basis} {Functions} for {Both} {Reconstruction} and {Visualization}},
	volume = {46},
	issn = {1571-0661},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066104810016},
	doi = {https://doi.org/10.1016/S1571-0661(04)81001-6},
	abstract = {Algebraic Reconstruction Techniques (ART) for the reconstruction of distributions from projections have yielded improvements in diverse fields such as medical imaging and electron microscopy. An important property of these methods is that they allow the use of various basis functions. Recently spherically symmetric functions (blobs) have been introduced as efficacious basis functions for reconstruction. However, basis functions whose parameters were found to be appropriate for use in reconstruction are not necessarily good for visualization. We propose a method of selecting blob parameters for both reconstruction and visualization.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Garduño, Edgar and Herman, Gabor T.},
	year = {2001},
	keywords = {electron microscopy},
	pages = {413--429},
	file = {Garduno and Herman - Optimization of Basis Functions for Both Reconstru.pdf:/home/david/Zotero/storage/6B27WM88/Garduno and Herman - Optimization of Basis Functions for Both Reconstru.pdf:application/pdf},
}

@article{chlewicki_noise_2004,
	title = {Noise reduction and convergence of {Bayesian} algorithms with blobs based on the {Huber} function and median root prior},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/20/004},
	doi = {10.1088/0031-9155/49/20/004},
	abstract = {Iterative image reconstruction algorithms have the potential to produce low noise images. Early stopping of the iteration process is problematic because some features of the image may converge slowly. On the other hand, there may be noise build-up with increased number of iterations. Therefore, we examined the stabilizing effect of using two different prior functions as well as image representation by blobs so that the number of iterations could be increased without noise build-up. Reconstruction was performed of simulated phantoms and of real data acquired by positron emission tomography. Image quality measures were calculated for images reconstructed with or without priors. Both priors stabilized the iteration process. The first prior based on the Huber function reduced the noise without significant loss of contrast recovery of small spots, but the drawback of the method was the difficulty in finding optimal values of two free parameters. The second method based on a median root prior has only one Bayesian parameter which was easy to set, but it should be taken into account that the image resolution while using that prior has to be chosen sufficiently high not to cause the complete removal of small spots. In conclusion, the Huber penalty function gives accurate and low noise images, but it may be difficult to determine the parameters. The median root prior method is not quite as accurate but may be used if image resolution is increased.},
	language = {en},
	number = {20},
	urldate = {2022-03-29},
	journal = {Physics in Medicine and Biology},
	author = {Chlewicki, W. and Hermansen, F. and Hansen, S. B.},
	month = sep,
	year = {2004},
	note = {Publisher: IOP Publishing},
	keywords = {positron emission tomography},
	pages = {4717--4730},
}

@article{jacobs_comparative_1999,
	title = {A {Comparative} {Study} of {2D} {Reconstruction} {Algorithms} {Using} {Pixels} and {Optimized} {Blobs} {Applied} to {Fourier} {Rebinned} {3D} {Data}},
	language = {en},
	author = {Jacobs, Filip and Matej, Samuel and Lewitt, Robert M and Lemahieu, Ignace},
	month = jan,
	year = {1999},
	keywords = {positron emission tomography},
	pages = {4},
	file = {Jacobs et al. - A Comparative Study of 2D Reconstruction Algorithm.pdf:/home/david/Zotero/storage/WF8XNFYS/Jacobs et al. - A Comparative Study of 2D Reconstruction Algorithm.pdf:application/pdf},
}

@article{yendiki_comparison_2004,
	title = {A comparison of rotation- and blob-based system models for {3D} {SPECT} with depth-dependent detector response},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/11/003},
	doi = {10.1088/0031-9155/49/11/003},
	abstract = {We compare two different implementations of a 3D SPECT system model for iterative reconstruction, both of which compensate for non-uniform photon attenuation and depth-dependent system response. One implementation performs fast rotation of images represented using a basis of rectangular voxels, whereas the other represents images using a basis of rotationally symmetric volume elements. In our simulations the blob-based approach was found to slightly outperform the rotation-based one in terms of the bias-variance trade-off in the reconstructed images. Their difference can be significant, however, in terms of computational load. The rotation-based method is faster for many typical SPECT reconstruction problems, but the blob-based one can be better-suited to cases where the reconstruction algorithm needs to process one volume element at a time.},
	language = {en},
	number = {11},
	urldate = {2022-03-29},
	journal = {Physics in Medicine and Biology},
	author = {Yendiki, A. and Fessler, J. A.},
	month = may,
	year = {2004},
	note = {Publisher: IOP Publishing},
	keywords = {single-photon emission tomography},
	pages = {2157--2168},
	file = {Yendiki and Fessler - 2004 - A comparison of rotation- and blob-based system mo.pdf:/home/david/Zotero/storage/6ECMH8R7/Yendiki and Fessler - 2004 - A comparison of rotation- and blob-based system mo.pdf:application/pdf},
}

@article{wang_3d_2004,
	title = {{3D} {RBI}-{EM} reconstruction with spherically-symmetric basis function for {SPECT} rotating slat collimator},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/11/011},
	doi = {10.1088/0031-9155/49/11/011},
	abstract = {A single photon emission computed tomography (SPECT) rotating slat collimator with strip detector acquires distance-weighted plane integral data, along with the attenuation factor and distance-dependent detector response. In order to image a 3D object, the slat collimator device has first to spin around its axis and then rotate around the object to produce 3D projection measurements. Compared to the slice-by-slice 2D reconstruction for the parallel-hole collimator and line integral data, a more complex 3D reconstruction is needed for the slat collimator and plane integral data. In this paper, we propose a 3D RBI-EM reconstruction algorithm with spherically-symmetric basis function, also called ‘blobs’, for the slat collimator. It has a closed and spherically symmetric analytical expression for the 3D Radon transform, which makes it easier to compute the plane integral than the voxel. It is completely localized in the spatial domain and nearly band-limited in the frequency domain. Its size and shape can be controlled by several parameters to have desired reconstructed image quality. A mathematical lesion phantom study has demonstrated that the blob reconstruction can achieve better contrast-noise trade-offs than the voxel reconstruction without greatly degrading the image resolution. A real lesion phantom study further confirmed this and showed that a slat collimator with CZT detector has better image quality than the conventional parallel-hole collimator with NaI detector. The improvement might be due to both the slat collimation and the better energy resolution of the CZT detector.},
	language = {en},
	number = {11},
	urldate = {2022-03-29},
	journal = {Physics in Medicine and Biology},
	author = {Wang, Wenli and Hawkins, William and Gagnon, Daniel},
	month = may,
	year = {2004},
	note = {Publisher: IOP Publishing},
	keywords = {single-photon emission tomography},
	pages = {2273--2292},
}

@inproceedings{carvalho_helical_2003,
	title = {Helical {CT} reconstruction from wide cone-beam angle data using {ART}},
	doi = {10.1109/SIBGRA.2003.1241031},
	abstract = {We report on new results on the use of algebraic reconstruction techniques (ART) for reconstructing from helical cone-beam computerized tomography (CT) data. We investigate two variants of ART for this task: a standard one that considers a single ray in an iterative step and a block version which groups several cone-beam projections when calculating an iterative step. Both algorithms were implemented using modified Kaiser-Bessel window functions, also known as blobs, placed on the body-centered cubic (bcc) grid. The algorithms were used to reconstruct a modified 3D Shepp-Logan phantom from data collected for the PI-geometry for two different maximum cone-beam angles (/spl plusmn/9.46/spl deg/ and /spl plusmn/18.43/spl deg/). Both scattering and quantum noise (for three different noise levels) were introduced to create noisy projections. The results presented here (for both noiseless and noisy data sets) point to the fact that, as opposed to filtered backprojection algorithms, the quality of the reconstructions produced by the ART methods does not suffer from the increase in the cone-beam angle.},
	booktitle = {16th {Brazilian} {Symposium} on {Computer} {Graphics} and {Image} {Processing} ({SIBGRAPI} 2003)},
	author = {Carvalho, B.M. and Herman, G.T.},
	month = oct,
	year = {2003},
	note = {ISSN: 1530-1834},
	keywords = {transmission computed tomograpy},
	pages = {363--370},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/PGDQXRNS/1241031.html:text/html},
}

@article{jacobs_iterative_1999,
	title = {Iterative {Image} {Reconstruction} {From} {Projections} {Based} {On} {Generalised} {Kaiser}-{Bessel} {Window} {Functions}},
	abstract = {Tomographic images are calculated from data provided by a scanner. The reconstruction algorithms used for this purpose are either analytical or iterative in nature. The paper focuses on an iterative algorithm called the row-action maximum-likelihood algorithm, and on transmission tomography. Iterative algorithms approximate the image as a linear combination of a limited set of basis functions. The paper introduces a new set of basis functions, called blobs, into the field of process tomography. It also presents preliminary results of a study which evaluates the advantage of using blobs, instead of pixels, for different amounts of available data and different noise levels. The results clearly show that the use of blobs is also beneficial for process tomography.},
	language = {en},
	author = {Jacobs, Filip and Lemahieu, Ignace},
	year = {1999},
	keywords = {transmission computed tomograpy},
	pages = {6},
	file = {Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:/home/david/Zotero/storage/A6SNMWBG/Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:application/pdf},
}

@article{isola_motion-compensated_2008,
	title = {Motion-compensated iterative cone-beam {CT} image reconstruction with adapted blobs as basis functions},
	volume = {53},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/53/23/009},
	doi = {10.1088/0031-9155/53/23/009},
	abstract = {This paper presents a three-dimensional method to reconstruct moving objects from cone-beam X-ray projections using an iterative reconstruction algorithm and a given motion vector field. For the image representation, adapted blobs are used, which can be implemented efficiently as basis functions. Iterative reconstruction requires the calculation of line integrals (forward projections) through the image volume, which are compared with the actual measurements to update the image volume. In the existence of a divergent motion vector field, a change in the volumes of the blobs has to be taken into account in the forward and backprojections. An efficient method to calculate the line integral through the adapted blobs is proposed. It solves the problem, how to compensate for the divergence in the motion vector field on a grid of basis functions. The method is evaluated on two phantoms, which are subject to three different known motions. Moreover, a motion-compensated filtered back-projection reconstruction method is used, and the reconstructed images are compared. Using the correct motion vector field with the iterative motion-compensated reconstruction, sharp images are obtained, with a quality that is significantly better than gated reconstructions.},
	language = {en},
	number = {23},
	urldate = {2022-03-29},
	journal = {Physics in Medicine and Biology},
	author = {Isola, A. A. and Ziegler, A. and Koehler, T. and Niessen, W. J. and Grass, M.},
	month = nov,
	year = {2008},
	note = {Publisher: IOP Publishing},
	keywords = {transmission computed tomograpy},
	pages = {6777--6797},
}

@inproceedings{wu_breast_2010,
	address = {Berlin, Heidelberg},
	title = {Breast {Tomosynthesis} {Reconstruction} {Using} a {Grid} of {Blobs} with {Projection} {Matrices}},
	isbn = {978-3-642-13666-5},
	doi = {10.1007/978-3-642-13666-5_33},
	abstract = {Spherically symmetric basis functions (blobs) are alternatives to the more conventional cubic voxels for image reconstruction in breast tomosynthesis. The volume representation and its projection views (PV) are essential components of iterative algorithms for image reconstruction from data collected from an area detector. This paper addresses the forward projection and backprojection process of three-dimensional (3D) breast reconstruction obtained from cone-beam scans using tomosynthesis imaging equipment. The smoothness of the blob elements allows more realistic modeling of the breast, and the rotational symmetry of the elements leads to more efficient calculation of both directional projection of the represented volume, as required in iterative reconstruction techniques. The combination of blob volume elements and the projection matrix method improves tomosynthesis reconstruction in both accuracy and speed.},
	language = {en},
	booktitle = {Digital {Mammography}},
	publisher = {Springer},
	author = {Wu, Gang and Mainprize, James G. and Yaffe, Martin J.},
	editor = {Martí, Joan and Oliver, Arnau and Freixenet, Jordi and Martí, Robert},
	year = {2010},
	keywords = {breast tomosynthesis},
	pages = {243--250},
}

@article{tran_inverse_2014,
	title = {Inverse {Problem} {Approach} for the {Alignment} of {Electron} {Tomographic} {Series}},
	volume = {69},
	copyright = {© 2013, IFP Energies nouvelles},
	issn = {1294-4475, 1953-8189},
	url = {https://ogst.ifpenergiesnouvelles.fr/articles/ogst/abs/2014/02/ogst120175/ogst120175.html},
	doi = {10.2516/ogst/2013116},
	abstract = {In the refining industry, morphological measurements of particles have become an essential part in the characterization catalyst supports. Through these parameters, one can infer the specific physicochemical properties of the studied materials. One of the main acquisition techniques is electron tomography (or nanotomography). 3D volumes are reconstructed from sets of projections from different angles made by a Transmission Electron Microscope (TEM). This technique provides a real three-dimensional information at the nanometric scale. A major issue in this method is the misalignment of the projections that contributes to the reconstruction. The current alignment techniques usually employ fiducial markers such as gold particles for a correct alignment of the images. When the use of markers is not possible, the correlation between adjacent projections is used to align them. However, this method sometimes fails. In this paper, we propose a new method based on the inverse problem approach where a certain criterion is minimized using a variant of the Nelder and Mead simplex algorithm. The proposed approach is composed of two steps. The first step consists of an initial alignment process, which relies on the minimization of a cost function based on robust statistics measuring the similarity of a projection to its previous projections in the series. It reduces strong shifts resulting from the acquisition between successive projections. In the second step, the pre-registered projections are used to initialize an iterative alignment-refinement process which alternates between (i) volume reconstructions and (ii) registrations of measured projections onto simulated projections computed from the volume reconstructed in (i). At the end of this process, we have a correct reconstruction of the volume, the projections being correctly aligned. Our method is tested on simulated data and shown to estimate accurately the translation, rotation and scale of arbitrary transforms. We have successfully tested our method with real projections of different catalyst supports.},
	language = {en},
	number = {2},
	urldate = {2022-03-29},
	journal = {Oil \& Gas Science and Technology – Revue d’IFP Energies nouvelles},
	author = {Tran, V.-D. and Moreaud, M. and Thiébaut, É and Denis, L. and Becker, J. M.},
	year = {2014},
	note = {Number: 2
Publisher: Technip},
	keywords = {electron tomography},
	pages = {279--291},
	file = {Tran et al. - 2014 - Inverse Problem Approach for the Alignment of Elec.pdf:/home/david/Zotero/storage/CUXCC9V9/Tran et al. - 2014 - Inverse Problem Approach for the Alignment of Elec.pdf:application/pdf},
}

@inproceedings{tran_robust_2013,
	address = {Burlingame, California, USA},
	title = {Robust registration of electron tomography projections without fiducial markers},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2001128},
	doi = {10.1117/12.2001128},
	abstract = {A major issue in electron tomography is the misalignment of the projections contributing to the reconstruction. The current alignment techniques currently use ﬁducial markers such as gold particles. When the use of markers is not possible, the accurate alignment of the projections is a challenge. We describe a new method for the alignment of transmission electron microscopy (TEM) images series without the need of ﬁducial markers. The proposed approach is composed of two steps. The ﬁrst step consists of an initial alignment process, which relies on the minimization of a cost function based on robust statistics measuring the similarity of a projection to its previous projections in the series. It reduces strong shifts resulting from the acquisition between successive projections. The second step aligns the projections ﬁnely. The issue is formalized as an inverse problem. The preregistered projections are used to initialize an iterative alignment-reﬁnement process which alternates between (i) volume reconstructions and (ii) registrations of measured projections onto simulated projections computed from the volume reconstructed in (i). The accuracy of our method is very satisfying; we illustrate it on simulated data and real projections of diﬀerent zeolite supports catalyst.},
	language = {en},
	urldate = {2022-03-29},
	author = {Tran, Viet-Dung and Moreaud, Maxime and Thiébaut, Éric and Dénis, Loïc and Becker, Jean-Marie},
	editor = {Bouman, Charles A. and Pollak, Ilya and Wolfe, Patrick J.},
	month = feb,
	year = {2013},
	keywords = {electron tromography},
	pages = {86570R},
	file = {Tran et al. - 2013 - Robust registration of electron tomography project.pdf:/home/david/Zotero/storage/3C74RGFU/Tran et al. - 2013 - Robust registration of electron tomography project.pdf:application/pdf},
}

@article{yu_simulation_2012,
	title = {Simulation tools for two-dimensional experiments in x-ray computed tomography using the {FORBILD} head phantom},
	volume = {57},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/57/13/n237},
	doi = {10.1088/0031-9155/57/13/N237},
	abstract = {Mathematical phantoms are essential for the development and early stage evaluation of image reconstruction algorithms in x-ray computed tomography (CT). This note offers tools for computer simulations using a two-dimensional (2D) phantom that models the central axial slice through the FORBILD head phantom. Introduced in 1999, in response to a need for a more robust test, the FORBILD head phantom is now seen by many as the gold standard. However, the simple Shepp–Logan phantom is still heavily used by researchers working on 2D image reconstruction. Universal acceptance of the FORBILD head phantom may have been prevented by its significantly higher complexity: software that allows computer simulations with the Shepp–Logan phantom is not readily applicable to the FORBILD head phantom. The tools offered here address this problem. They are designed for use with Matlab®, as well as open-source variants, such as FreeMat and Octave, which are all widely used in both academia and industry. To get started, the interested user can simply copy and paste the codes from this PDF document into Matlab® M-files.},
	language = {en},
	number = {13},
	urldate = {2022-03-29},
	journal = {Physics in Medicine and Biology},
	author = {Yu, Zhicong and Noo, Frédéric and Dennerlein, Frank and Wunderlich, Adam and Lauritsch, Günter and Hornegger, Joachim},
	month = jun,
	year = {2012},
	note = {Publisher: IOP Publishing},
	pages = {N237--N252},
	file = {Accepted Version:/home/david/Zotero/storage/WTLMQKH7/Yu et al. - 2012 - Simulation tools for two-dimensional experiments i.pdf:application/pdf},
}

@article{li_fast_2007,
	title = {A {Fast} {Fully} 4-{D} {Incremental} {Gradient} {Reconstruction} {Algorithm} for {List} {Mode} {PET} {Data}},
	volume = {26},
	issn = {0278-0062},
	url = {http://ieeexplore.ieee.org/document/4039533/},
	doi = {10.1109/TMI.2006.884208},
	abstract = {We describe a fast and globally convergent fully fourdimensional incremental gradient (4DIG) algorithm to estimate the continuous-time tracer density from list mode positron emission tomography (PET) data. Detection of 511-keV photon pairs produced by positron-electron annihilation is modeled as an inhomogeneous Poisson process whose rate function is parameterized using cubic -splines. The rate functions are estimated by minimizing the cost function formed by the sum of the negative log-likelihood of arrival times, spatial and temporal roughness penalties, and a negativity penalty. We ﬁrst derive a computable bound for the norm of the optimal temporal basis function coefﬁcients. Based on this bound we then construct and prove convergence of an incremental gradient algorithm. Fully 4-D simulations demonstrate the substantially faster convergence behavior of the 4DIG algorithm relative to preconditioned conjugate gradient. Four-dimensional reconstructions of real data are also included to illustrate the performance of this method.},
	language = {en},
	number = {1},
	urldate = {2022-03-29},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Li, Quanzheng and Asma, Evren and Ahn, Sangtae and Leahy, Richard M.},
	month = jan,
	year = {2007},
	keywords = {positron emission tomography},
	pages = {58--67},
	file = {Li et al. - 2007 - A Fast Fully 4-D Incremental Gradient Reconstructi.pdf:/home/david/Zotero/storage/DMYTPZXB/Li et al. - 2007 - A Fast Fully 4-D Incremental Gradient Reconstructi.pdf:application/pdf},
}

@article{nichols_spatiotemporal_2002,
	title = {Spatiotemporal reconstruction of list-mode {PET} data},
	volume = {21},
	issn = {0278-0062},
	url = {http://ieeexplore.ieee.org/document/1000263/},
	doi = {10.1109/TMI.2002.1000263},
	language = {en},
	number = {4},
	urldate = {2022-03-29},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Nichols, T.E. and {Jinyi Qi} and Asma, E. and Leahy, R.M.},
	month = apr,
	year = {2002},
	keywords = {positron emission tomography},
	pages = {396--404},
	file = {Nichols et al. - 2002 - Spatiotemporal reconstruction of list-mode PET dat.pdf:/home/david/Zotero/storage/9QWEN7GS/Nichols et al. - 2002 - Spatiotemporal reconstruction of list-mode PET dat.pdf:application/pdf},
}

@article{verhaeghe_investigation_2007,
	title = {An investigation of temporal regularization techniques for dynamic {PET} reconstructions using temporal splines},
	volume = {34},
	issn = {2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1118/1.2723879},
	doi = {10.1118/1.2723879},
	abstract = {The use of a temporal B-spline basis for the reconstruction of dynamic positron emission tomography data was investigated. Maximum likelihood (ML) reconstructions using an expectation maximization framework and maximum A-posteriori (MAP) reconstructions using the generalized expectation maximization framework were evaluated. Different parameters of the B-spline basis of such as order, number of basis functions and knot placing were investigated in a reconstruction task using simulated dynamic list-mode data. We found that a higher order basis reduced both the bias and variance. Using a higher number of basis functions in the modeling of the time activity curves (TACs) allowed the algorithm to model faster changes of the TACs, however, the TACs became noisier. We have compared ML, Gaussian postsmoothed ML and MAP reconstructions. The noise level in the ML reconstructions was controlled by varying the number of basis functions. The MAP algorithm penalized the integrated squared curvature of the reconstructed TAC. The postsmoothed ML was always outperformed in terms of bias and variance properties by the MAP and ML reconstructions. A simple adaptive knot placing strategy was also developed and evaluated. It is based on an arc length redistribution scheme during the reconstruction. The free knot reconstruction allowed a more accurate reconstruction while reducing the noise level especially for fast changing TACs such as blood input functions. Limiting the number of temporal basis functions combined with the adaptive knot placing strategy is in this case advantageous for regularization purposes when compared to the other regularization techniques.},
	language = {en},
	number = {5},
	urldate = {2022-03-29},
	journal = {Medical Physics},
	author = {Verhaeghe, Jeroen and D'Asseler, Yves and Vandenberghe, Stefaan and Staelens, Steven and Lemahieu, Ignace},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1118/1.2723879},
	keywords = {positron emission tomography},
	pages = {1766--1778},
}

@article{despres_review_2017,
	title = {A review of {GPU}-based medical image reconstruction},
	volume = {42},
	issn = {11201797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1120179717302417},
	doi = {10.1016/j.ejmp.2017.07.024},
	abstract = {Tomographic image reconstruction is a computationally demanding task, even more so when advanced models are used to describe a more complete and accurate picture of the image formation process. Such advanced modeling and reconstruction algorithms can lead to better images, often with less dose, but at the price of long calculation times that are hardly compatible with clinical workﬂows. Fortunately, reconstruction tasks can often be executed advantageously on Graphics Processing Units (GPUs), which are exploited as massively parallel computational engines. This review paper focuses on recent developments made in GPU-based medical image reconstruction, from a CT, PET, SPECT, MRI and US perspective. Strategies and approaches to get the most out of GPUs in image reconstruction are presented as well as innovative applications arising from an increased computing capacity. The future of GPU-based image reconstruction is also envisioned, based on current trends in high-performance computing.},
	language = {en},
	urldate = {2022-03-29},
	journal = {Physica Medica},
	author = {Després, Philippe and Jia, Xun},
	month = oct,
	year = {2017},
	pages = {76--92},
	file = {Després and Jia - 2017 - A review of GPU-based medical image reconstruction.pdf:/home/david/Zotero/storage/VMEUVFWR/Després and Jia - 2017 - A review of GPU-based medical image reconstruction.pdf:application/pdf},
}

@inproceedings{reutter_fully_2007,
	title = {Fully 4-{D} dynamic cardiac {SPECT} image reconstruction using spatiotemporal {B}-spline voxelization},
	volume = {6},
	doi = {10.1109/NSSMIC.2007.4437048},
	abstract = {We developed fully 4-D penalized least-squares reconstruction methods that use overlapping multiresolution B-splines to represent radiopharmaceutical distributions that vary smoothly in space and time in human dynamic cardiac SPECT images. This approach does not require segmentation, and improves signal-to-noise and increases computational efficiency compared to methods based on small, non-overlapping cube-shaped voxels and rectangular time windows. The support of spatial B-splines was extended into the time dimension to obtain estimates of time-activity curves directly from projections for a human dynamic Tc-99m-sestamibi cardiac SPECT/CT study. Projection data were acquired in 1-sec time frames with an angular step of 5 deg per frame on a GE millennium VH Hawk-eye SPECT/CT scanner. Attenuation and depth-dependent collimator response were modeled, but not scatter. The 4-D B-splines were piecewise trilinear in space and piecewise quadratic in time. The splines were organized on a 3-D spatial grid that provided uniform sampling of 17.7 mm in each dimension, and on a 1-D temporal grid that provided nonuniform sampling intervals of 0-4, 4-15, 15-48, and 48-144 sec during the first two gantry rotations. The use of nonuniform time sampling with 4-D B-splines that varied quadratically in time yielded smooth time-activity curves that captured the relatively fast rise and fall of tracer in the right and left blood chambers, as well as uptake and retention of tracer in the left ventricular myocardium. These methods can also be applied to dynamic PET.},
	booktitle = {2007 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	author = {Reutter, Bryan W. and Gullberg, Grant T. and Boutchko, Rostyslav and Balakrishnan, Karthikayan and Botvinick, Elias H. and Huesman, Ronald H.},
	month = oct,
	year = {2007},
	note = {ISSN: 1082-3654},
	keywords = {Computed tomography, Dynamic SPECT, fully fourdimensional reconstruction, Humans, Image reconstruction, Image resolution, Index Terms, penalized least-squares, Reconstruction algorithms, Sampling methods, Signal resolution, Spatial resolution, spatiotemporal B-splines, Spatiotemporal phenomena, SPECT/CT, Spline},
	pages = {4217--4221},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/Z3G6GEWF/Reutter et al. - 2007 - Fully 4-D dynamic cardiac SPECT image reconstructi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/8S7LCQTJ/4437048.html:text/html},
}

@inproceedings{guedon_b-spline_1991,
	title = {B-spline {FBP} reconstructions for {SPECT}},
	doi = {10.1109/NSSMIC.1991.259085},
	abstract = {The authors describe a generalization of the filtered backprojection (FBP) algorithm based on a pixel intensity distribution (PIDM) chosen in spline spaces. A set of reconstructions is obtained according to the degree of spline. The single photon emission computed tomography (SPECT) implementation is discussed in order to provide efficient algorithms and take into account the limitations induced by the gamma -camera. A study based on Jaszczack phantom reconstructions shows different ways to distribute the acquired photons onto projections and their implications in terms of algorithms.{\textless}{\textgreater}},
	booktitle = {Conference {Record} of the 1991 {IEEE} {Nuclear} {Science} {Symposium} and {Medical} {Imaging} {Conference}},
	author = {Guedon, J.-P. and Barker, C. and Bizais, Y.},
	month = nov,
	year = {1991},
	keywords = {Biomedical imaging, Filters, Fourier transforms, Hospitals, Image reconstruction, Image sampling, Kernel, Pixel, Single photon emission computed tomography, Spline},
	pages = {1065--1069 vol.2},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/8WJXEGRT/Guedon et al. - 1991 - B-spline FBP reconstructions for SPECT.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/ZADCUWQX/259085.html:text/html},
}

@inproceedings{niu_fully_2010,
	title = {Fully 5d reconstruction of gated dynamic cardiac {SPECT} images using temporal {B}-splines},
	doi = {10.1109/ISBI.2010.5490311},
	abstract = {In previous work we explored a reconstruction approach based on B-spline modeling for dynamic cardiac images from a gated acquisition in single photon emission computed tomography (SPECT), of which the goal is to obtain a single sequence showing both cardiac motion and kinetic tracer distribution change over time simultaneously. In this work we further develop this approach by extending it to reconstruction of fully five-dimensional (5D) images. Besides quantifying its reconstruction accuracy, we also demonstrate the feasibility of using kinetic information from reconstructed dynamic images with this approach for differentiating defects from normal cardiac perfusion. The proposed approach is evaluated using a dynamic version of the 4D NURBS-based cardiac-torso (NCAT) phantom to simulate a gated SPECT perfusion acquisition with Tc99m Teboroxime. Our results demonstrate that, despite the greatly underdetermined nature of the problem, the 5D procedure can lead to faithful reconstruction of gated dynamic images for both cardiac motion and perfusion defect detection.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {From} {Nano} to {Macro}},
	author = {Niu, Xiaofeng and Yang, Yongyi and Jin, Mingwu},
	month = apr,
	year = {2010},
	note = {ISSN: 1945-8452},
	keywords = {5D reconstruction, B-spline, Biomedical imaging, Data acquisition, Dynamic SPECT, gated SPECT, Image reconstruction, Kinetic theory, Medical treatment, motion compensation, Motion compensation, Myocardium, Radiology, Reconstruction algorithms, Spline},
	pages = {460--463},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/YMPBWCXY/Niu et al. - 2010 - Fully 5d reconstruction of gated dynamic cardiac S.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/L73PB596/5490311.html:text/html},
}

@article{nilchian_spline_2015,
	title = {Spline based iterative phase retrieval algorithm for {X}-ray differential phase contrast radiography},
	volume = {23},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-23-8-10631},
	doi = {10.1364/OE.23.010631},
	abstract = {Differential phase contrast imaging using grating interferometer is a promising alternative to conventional X-ray radiographic methods. It provides the absorption, differential phase and scattering information of the underlying sample simultaneously. Phase retrieval from the differential phase signal is an essential problem for quantitative analysis in medical imaging. In this paper, we formalize the phase retrieval as a regularized inverse problem, and propose a novel discretization scheme for the derivative operator based on B-spline calculus. The inverse problem is then solved by a constrained regularized weighted-norm algorithm (CRWN) which adopts the properties of B-spline and ensures a fast implementation. The method is evaluated with a tomographic dataset and differential phase contrast mammography data. We demonstrate that the proposed method is able to produce phase image with enhanced and higher soft tissue contrast compared to conventional absorption-based approach, which can potentially provide useful information to mammographic investigations.},
	language = {en},
	number = {8},
	urldate = {2022-03-29},
	journal = {Optics Express},
	author = {Nilchian, Masih and Wang, Zhentian and Thuering, Thomas and Unser, Michael and Stampanoni, Marco},
	month = apr,
	year = {2015},
	pages = {10631},
	file = {Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:/home/david/Zotero/storage/S3FC3BEC/Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:application/pdf},
}

@article{nilchian_spline_2015-1,
	title = {Spline based iterative phase retrieval algorithm for {X}-ray differential phase contrast radiography},
	volume = {23},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-23-8-10631},
	doi = {10.1364/OE.23.010631},
	abstract = {Differential phase contrast imaging using grating interferometer is a promising alternative to conventional X-ray radiographic methods. It provides the absorption, differential phase and scattering information of the underlying sample simultaneously. Phase retrieval from the differential phase signal is an essential problem for quantitative analysis in medical imaging. In this paper, we formalize the phase retrieval as a regularized inverse problem, and propose a novel discretization scheme for the derivative operator based on B-spline calculus. The inverse problem is then solved by a constrained regularized weighted-norm algorithm (CRWN) which adopts the properties of B-spline and ensures a fast implementation. The method is evaluated with a tomographic dataset and differential phase contrast mammography data. We demonstrate that the proposed method is able to produce phase image with enhanced and higher soft tissue contrast compared to conventional absorption-based approach, which can potentially provide useful information to mammographic investigations.},
	language = {en},
	number = {8},
	urldate = {2022-03-29},
	journal = {Optics Express},
	author = {Nilchian, Masih and Wang, Zhentian and Thuering, Thomas and Unser, Michael and Stampanoni, Marco},
	month = apr,
	year = {2015},
	pages = {10631},
	file = {Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:/home/david/Zotero/storage/HR2J6ZNC/Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:application/pdf},
}

@article{xu_investigation_2012-1,
	title = {Investigation of discrete imaging models and iterative image reconstruction in differential {X}-ray phase-contrast tomography},
	volume = {20},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-20-10-10724},
	doi = {10.1364/OE.20.010724},
	abstract = {Differential X-ray phase-contrast tomography (DPCT) refers to a class of promising methods for reconstructing the X-ray refractive index distribution of materials that present weak X-ray absorption contrast. The tomographic projection data in DPCT, from which an estimate of the refractive index distribution is reconstructed, correspond to onedimensional (1D) derivatives of the two-dimensional (2D) Radon transform of the refractive index distribution. There is an important need for the development of iterative image reconstruction methods for DPCT that can yield useful images from few-view projection data, thereby mitigating the long data-acquisition times and large radiation doses associated with use of analytic reconstruction methods. In this work, we analyze the numerical and statistical properties of two classes of discrete imaging models that form the basis for iterative image reconstruction in DPCT. We also investigate the use of one of the models with a modern image reconstruction algorithm for performing few-view image reconstruction of a tissue specimen.},
	language = {en},
	number = {10},
	urldate = {2022-03-29},
	journal = {Optics Express},
	author = {Xu, Qiaofeng and Sidky, Emil Y. and Pan, Xiaochuan and Stampanoni, Marco and Modregger, Peter and Anastasio, Mark A.},
	month = may,
	year = {2012},
	pages = {10724},
	file = {Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:/home/david/Zotero/storage/C9UR5XQL/Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:application/pdf},
}
