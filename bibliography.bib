
@article{entezari_box_2012,
	title = {A {Box} {Spline} {Calculus} for the {Discretization} of {Computed} {Tomography} {Reconstruction} {Problems}},
	volume = {31},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/6172241/},
	doi = {10.1109/TMI.2012.2191417},
	abstract = {B-splines are attractive basis functions for the continuous-domain representation of biomedical images and volumes. In this paper, we prove that the extended family of box splines are closed under the Radon transform and derive explicit formulae for their transforms. Our results are general; they cover all known brands of compactly-supported box splines (tensorproduct B-splines, separable or not) in any number of dimensions. The proposed box spline approach extends to non-Cartesian lattices used for discretizing the image space. In particular, we prove that the 2-D Radon transform of an N -direction box spline is generally a (non-uniform) polynomial spline of degree N − 1. The proposed framework allows for a proper discretization of a variety of tomographic reconstruction problems in a box spline basis. It is of relevance for imaging modalities such as X-ray computed tomography and cryo-electron microscopy.},
	language = {en},
	number = {8},
	urldate = {2021-02-17},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Entezari, A. and Nilchian, M. and Unser, M.},
	month = aug,
	year = {2012},
	pages = {1532--1541},
	file = {Entezari et al. - 2012 - A Box Spline Calculus for the Discretization of Co.pdf:/home/david/Zotero/storage/7W67K34M/Entezari et al. - 2012 - A Box Spline Calculus for the Discretization of Co.pdf:application/pdf},
}

@phdthesis{wieczorek_anisotropic_2017,
	title = {Anisotropic {X}-ray {Dark}-field {Tomography}},
	abstract = {Modern X-ray based imaging enables recording of phase-contrast (refraction) and dark-field
(Small Angle X-ray Scattering) information using Talbot-Lau interferometry. These X-ray
imaging modalities provide improved contrast where standard absorption based imaging
only provides poor to none. The task of Computed Tomography (CT) amounts to recon-
struction of the physical quantities within the imaged object which caused a specific obser-
vation/measurement. A major prerequisite for tomographic reconstruction is first a model
of the physical properties, e.g. using scalars, vectors or tensors. Second, a forward model is
required which enables simulation of measurements from a given 3D representation of the
physical properties. For X-ray based absorption CT, this describes the task of computing the
accumulative effect on the X-ray beam traversing through the object. The combination of a
forward model and corresponding measurements form an inverse problem. Mathematically,
the task of CT corresponds to the inversion of the forward model which can be computed
using according numerical methods.
While tomographic reconstruction for modalities different than X-ray CT often employs very
similar mathematical concepts, software frameworks are often strictly focused on a specific
modality. The first contribution presented in this thesis is the development of an abstract
software framework for tomographic reconstruction. Within this framework the numerical
methods are implemented independently from the specific forward model which enables
adaptation and application of methods for multiple modalities. Additionally, the framework
supports the composition of various common approaches such as regularization methods which
allows for intensive comparison and evaluation of specific methods for multiple modalities.
Within the scope of this work, this framework will be applied to tomographic reconstruction
of the dark-field signal.
Reconstruction of the dark-field signal poses a particularly challenging problem, as the
scattering within an object depends on the X-ray beam’s direction as well as the grating
orientation in contrast to absorption and phase-contrast imaging. Thus, the physical quantity
at each position cannot be modeled by a scalar entity, but requires a more complex model
instead. A first method has been presented previously in form of X-ray Tensor Tomography
(XTT) where a rank-2 tensor is used to describe the scattering happening in each location of
the measured object. This tensor combines information on the scattering strength as well as
its directional distribution which provides an insight into orientation of microstructures within
the object.
A major limitation of the XTT approach is that a tensor is restricted to a single microstructure
direction. In order to cope with this problem within this thesis a general closed-form, continu-
ous forward model of the Anisotropic X-ray Dark-field Tomography will be presented. This
vii
model contains the XTT model under specific assumptions and in addition enables the tomo-
graphic reconstruction of a spherical function representing the whole scattering profile in each
location of the object. This novel approach provides strongly improved reconstructions using
spherical harmonics. All this is achieved at a computational complexity comparable to that
required by XTT. Additionally, an approach to extract the orientation of the microstructures
causing the scattering will be presented. Experiments show that the method of AXDT is capa-
ble of reconstructing multiple scattering orientations and the corresponding microstructure
orientations.
Finally, a first biomedical experiment on a sample of a human cerebellum indicates that AXDT
could provide a complementary imaging modality for imaging nerve fibers within the Central
Nervous System (CNS).},
	language = {en},
	school = {Technische Universität München},
	author = {Wieczorek, Matthias},
	month = oct,
	year = {2017},
	file = {Wieczorek - 2017 - Anisotropic X-ray Dark-field Tomography.pdf:/home/david/Zotero/storage/VINZ2W2A/Wieczorek - 2017 - Anisotropic X-ray Dark-field Tomography.pdf:application/pdf},
}

@article{nilchian_fast_2013,
	title = {Fast iterative reconstruction of differential phase contrast {X}-ray tomograms},
	volume = {21},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-21-5-5511},
	doi = {10.1364/OE.21.005511},
	abstract = {Differential phase-contrast is a recent technique in the context of X-ray imaging. In order to reduce the specimen’s exposure time, we propose a new iterative algorithm that can achieve the same quality as FBP-type methods, while using substantially fewer angular views. Our approach is based on 1) a novel spline-based discretization of the forward model and 2) an iterative reconstruction algorithm using the alternating direction method of multipliers. Our experimental results on real data suggest that the method allows to reduce the number of required views by at least a factor of four.},
	language = {en},
	number = {5},
	urldate = {2021-02-17},
	journal = {Optics Express},
	author = {Nilchian, Masih and Vonesch, Cédric and Modregger, Peter and Stampanoni, Marco and Unser, Michael},
	month = mar,
	year = {2013},
	pages = {5511},
	file = {Nilchian et al. - 2013 - Fast iterative reconstruction of differential phas.pdf:/home/david/Zotero/storage/YPH6XGY2/Nilchian et al. - 2013 - Fast iterative reconstruction of differential phas.pdf:application/pdf},
}

@book{de_boor_box_1993,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Box {Splines}},
	volume = {98},
	url = {http://link.springer.com/10.1007/978-1-4757-2244-4},
	language = {en},
	urldate = {2021-02-17},
	publisher = {Springer New York},
	author = {de Boor, Carl and Höllig, Klaus and Riemenschneider, Sherman},
	editor = {John, F. and Marsden, J. E. and Sirovich, L.},
	year = {1993},
	doi = {10.1007/978-1-4757-2244-4},
	file = {de Boor et al. - 1993 - Box Splines.pdf:/home/david/Zotero/storage/XZACJRNH/de Boor et al. - 1993 - Box Splines.pdf:application/pdf},
}

@phdthesis{von_teuffenbach_statistical_nodate,
	title = {Statistical signal processing and reconstruction algorithms for grating-based {X}-ray imaging and computed tomography},
	abstract = {Grating-based X-ray interferometry is a novel imaging technique that offers great potential for
the visualization of materials and tissues that are not easily depicted using conventional X-ray
imaging methods. Tomographic reconstruction based on interferometric data provides not only
access to the distribution of an object’s attenuation but also to its refraction and ultra-small-
angle scattering power. These images provide valuable additional information that could well
expand the diagnostic capabilities of a clinical computer tomography (CT) scanner.
One of the main reasons why this technique has not yet been implemented in a modern CT
scanner is that the improved functionality comes at the cost of longer measurement times. Ex-
isting projection-based processing algorithms require not a single measurement per projection
angle but several measurements with precise grating movements in between. A further reason
is that these signal estimation algorithms are also very sensitive to changes in the system align-
ment due to mechanical vibrations or thermal drifts, which are abound in a clinical high-power
CT using a continuously rotating gantry.
Several solutions for these problems have been proposed but all suffer from major drawbacks.
In this thesis first two simple improvements to existing signal estimation methods are pre-
sented and then a novel direct reconstruction method is introduced. A fast algorithm for
reconstructions using this method is developed, the technique is tested at scans using just a
single measurement per angular position and further enhancements that make the reconstruc-
tions robust to vibrations and drifts are implemented and tested.
The results in this thesis demonstrate that it is possible to successfully reconstruct the attenu-
ation, refraction, and ultra-small-angle scattering of an object using only a single measurement
per projection angle on a system influenced by significant vibrations and drifts.
This is a milestone for the future implementation of a grating interferometer onto a continuously
rotating clinical CT scanner.},
	language = {English},
	school = {Technische Universität München},
	author = {von Teuffenbach, Maximilian},
	file = {von Teuffenbach - Statistical signal processing and reconstruction a.pdf:/home/david/Zotero/storage/XDQRZTVK/von Teuffenbach - Statistical signal processing and reconstruction a.pdf:application/pdf},
}

@article{kohler_iterative_2011,
	title = {Iterative reconstruction for differential phase contrast imaging using spherically symmetric basis functions: {Iterative} reconstruction for differential phase constrast imaging},
	volume = {38},
	issn = {00942405},
	shorttitle = {Iterative reconstruction for differential phase contrast imaging using spherically symmetric basis functions},
	url = {http://doi.wiley.com/10.1118/1.3608906},
	doi = {10.1118/1.3608906},
	abstract = {Purpose: The purpose of this work is to combine two areas of active research in tomographic x-ray imaging. The ﬁrst one is the use of iterative reconstruction (IR) techniques. The second one is differential phase contrast imaging (DPCI).
Methods: The authors derive a maximum likelihood (ML) reconstruction algorithm with regularization for DPCI. Forward and back-projection are implemented using spherically symmetric basis functions (blobs) and differential footprints, thus completely avoiding the need for numerical differentiation throughout the reconstruction process. The method is applied to the problem of reconstruction of an object from sparsely sampled projections.
Results: The results show that the proposed method can handle the sparsely sampled data efﬁciently. In particular no streak artifacts are visible which are present in images obtained by ﬁltered back-projection (FBP).
Conclusions: IR algorithms have a wide spectrum of proven advantages in the area of conventional computed tomography. The present work describes for the ﬁrst time, how a matched forward and back-projection can be implemented for DPCI, which is furthermore free of any heuristics. The newly developed ML reconstruction algorithm for DPCI shows that for the case of sparsely sampled projection data, an improvement in image quality is obtained that is qualitatively comparable to a corresponding situation in conventional x-ray imaging. Based on the proposed operators for forward and back-projection, a large variety of IR algorithms is thus made available for DPCI.},
	language = {en},
	number = {8},
	urldate = {2021-02-24},
	journal = {Medical Physics},
	author = {Köhler, Thomas and Brendel, Bernhard and Roessl, Ewald},
	month = jul,
	year = {2011},
	note = {Number: 8},
	pages = {4542--4545},
	file = {Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:/home/david/Zotero/storage/JC48J3TV/Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:application/pdf;Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:/home/david/Zotero/storage/DSPA27N2/Köhler et al. - 2011 - Iterative reconstruction for differential phase co.pdf:application/pdf},
}

@book{bebis_advances_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Visual} {Computing}: 10th {International} {Symposium}, {ISVC} 2014, {Las} {Vegas}, {NV}, {USA}, {December} 8-10, 2014, {Proceedings}, {Part} {I}},
	volume = {8887},
	isbn = {978-3-319-14248-7 978-3-319-14249-4},
	shorttitle = {Advances in {Visual} {Computing}},
	url = {http://link.springer.com/10.1007/978-3-319-14249-4},
	language = {en},
	urldate = {2021-02-24},
	publisher = {Springer International Publishing},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and McMahan, Ryan and Jerald, Jason and Zhang, Hui and Drucker, Steven M. and Kambhamettu, Chandra and El Choubassi, Maha and Deng, Zhigang and Carlson, Mark},
	year = {2014},
	doi = {10.1007/978-3-319-14249-4},
	file = {Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:/home/david/Zotero/storage/BGRSUDL4/Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:application/pdf;Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:/home/david/Zotero/storage/Q49UGWSI/Bebis et al. - 2014 - Advances in Visual Computing 10th International S.pdf:application/pdf},
}

@inproceedings{mirzargar_spline_2013,
	address = {San Francisco, CA, USA},
	title = {A spline framework for sparse tomographic reconstruction},
	isbn = {978-1-4673-6455-3 978-1-4673-6456-0 978-1-4673-6454-6},
	url = {http://ieeexplore.ieee.org/document/6556763/},
	doi = {10.1109/ISBI.2013.6556763},
	abstract = {We present a spline-based sparse tomographic reconstruction framework. The proposed method utilizes the closed-form analytical Radon transform of B-splines and box splines of any order and integrates the (transform-domain) sparsity of the image into the reconstruction algorithm. Our experiments show that the synergy of sparse reconstruction together with higher order basis functions (e.g., cubic B-splines) improves the accuracy of the reconstruction. This gain can also be exploited for reducing the number of projection angles in the data acquisition.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {2013 {IEEE} 10th {International} {Symposium} on {Biomedical} {Imaging}},
	publisher = {IEEE},
	author = {Mirzargar, Mahsa and Sakhaee, Elham and Entezari, Alireza},
	month = apr,
	year = {2013},
	keywords = {Image reconstruction, Tomography, Splines (mathematics), Transforms, Accuracy, Approximation methods, Signal to noise ratio, sparse approximation, splines, Tomographic reconstruction},
	pages = {1272--1275},
	file = {Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:/home/david/Zotero/storage/GHZPRVVJ/Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:application/pdf;IEEE Xplore Full Text PDF:/home/david/Zotero/storage/AWNBSLVV/Mirzargar et al. - 2013 - A spline framework for sparse tomographic reconstr.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GHXESPZT/6556763.html:text/html},
}

@article{momey_spline_2015,
	title = {Spline {Driven}: {High} {Accuracy} {Projectors} for {Tomographic} {Reconstruction} {From} {Few} {Projections}},
	volume = {24},
	shorttitle = {Spline {Driven}},
	url = {http://ieeexplore.ieee.org/document/7182340/},
	doi = {10.1109/TIP.2015.2466083},
	language = {en},
	number = {12},
	urldate = {2021-02-24},
	journal = {IEEE Transactions on Image Processing},
	author = {Momey, Fabien and Denis, Loic and Burnier, Catherine and Thiebaut, Eric and Becker, Jean-Marie and Desbat, Laurent},
	month = dec,
	year = {2015},
	note = {Number: 12},
	pages = {4715--4725},
	file = {Momey et al. - 2015 - Spline Driven High Accuracy Projectors for Tomogr.pdf:/home/david/Zotero/storage/BVR98LW9/Momey et al. - 2015 - Spline Driven High Accuracy Projectors for Tomogr.pdf:application/pdf},
}

@article{momey_b-spline_2012,
	title = {A {B}-spline based and computationally performant projector for iterative reconstruction in tomography},
	language = {en},
	author = {Momey, Fabien and Denis, Loıc and Mennessier, Catherine},
	year = {2012},
	pages = {5},
	file = {Momey et al. - 2012 - A B-spline based and computationally performant pr.pdf:/home/david/Zotero/storage/KKG94HAZ/Momey et al. - 2012 - A B-spline based and computationally performant pr.pdf:application/pdf},
}

@article{kobbelt_stable_1997,
	title = {Stable {Evaluation} of {Box} {Splines}},
	abstract = {The most elegant way to evaluate box-splines is by using their recursive de nition. However, a straightforward implementation reveals numerical di culties. A careful analysis of the algorithm allows a reformulation which overcomes these problems without losing e ciency. A concise vectorized MATLAB-implementation is given.},
	language = {en},
	author = {Kobbelt, Leif},
	month = may,
	year = {1997},
	pages = {4},
	file = {Kobbelt - 1997 - Stable Evaluation of Box Splines.pdf:/home/david/Zotero/storage/4A3VXJDQ/Kobbelt - 1997 - Stable Evaluation of Box Splines.pdf:application/pdf},
}

@article{richter_use_1998,
	title = {Use of box splines in computer tomography},
	volume = {61},
	issn = {0010-485X, 1436-5057},
	url = {http://link.springer.com/10.1007/BF02684410},
	doi = {10.1007/BF02684410},
	abstract = {Box splines are attractive for practical multivariate approximation, since they possess good approximation power and can he evaluated very efficiently.We want to give an idea of how their qualities can be made to come into play in the field of image reconstruction in computerized tomography (CT). To keep the exposition simple, we will concentrate on a special situation: our tomograph will be characterized by the bivariate standard scanning geometry and our reconstructions will alwayslie in scales of the linear space spanned by the integer translates of a fixed piecewise quadratic box spline. On the other hand we give details of an algorithm based on Fourier reconstruction, which produces approximationsof optimal order for the box splines used, whilst the amount of computational work required is of no higher order than for classical Fourier reconstruction. We present another reconstruction procedure based on quasi-interpolation, which compares to filtered backprojection in computational complexity.Along with our exposition,we give a generalization of a certain Theorem due to Nievergelt which may be of interest for practical applications.},
	language = {en},
	number = {2},
	urldate = {2021-02-24},
	journal = {Computing},
	author = {Richter, M.},
	month = jun,
	year = {1998},
	note = {Number: 2},
	pages = {133--150},
	file = {Richter - 1998 - Use of box splines in computer tomography.pdf:/home/david/Zotero/storage/HYN3R3C2/Richter - 1998 - Use of box splines in computer tomography.pdf:application/pdf},
}

@incollection{prautzsch_box_2002,
	title = {Box {Splines}},
	isbn = {0-444-51104-0},
	booktitle = {Handbook of {Computer} {Aided} {Geometric} {Design}},
	publisher = {North Holland; Illustrated Edition},
	author = {Prautzsch, Hartmut and Boehm, Wolfgang},
	month = mar,
	year = {2002},
	keywords = {Box Splines},
	file = {Prautzsch and Boehm - 2002 - Box Splines.pdf:/home/david/Zotero/storage/FJILGBCN/Prautzsch and Boehm - 2002 - Box Splines.pdf:application/pdf},
}

@article{ruijters_gpu_2012,
	title = {{GPU} {Prefilter} for {Accurate} {Cubic} {B}-spline {Interpolation}},
	volume = {55},
	issn = {0010-4620, 1460-2067},
	url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxq086},
	doi = {10.1093/comjnl/bxq086},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {The Computer Journal},
	author = {Ruijters, D. and Thevenaz, P.},
	month = jan,
	year = {2012},
	pages = {15--20},
	file = {Ruijters and Thevenaz - 2012 - GPU Prefilter for Accurate Cubic B-spline Interpol.pdf:/home/david/Zotero/storage/7V32IDUX/Ruijters and Thevenaz - 2012 - GPU Prefilter for Accurate Cubic B-spline Interpol.pdf:application/pdf},
}

@inproceedings{entezari_linear_2004,
	address = {Austin, TX, USA},
	title = {Linear and cubic box splines for the body centered cubic lattice},
	isbn = {978-0-7803-8788-1},
	url = {http://ieeexplore.ieee.org/document/1372174/},
	doi = {10.1109/VISUAL.2004.65},
	abstract = {In this paper we derive piecewise linear and piecewise cubic box spline reconstruction ﬁlters for data sampled on the body centered cubic (BCC) lattice. We analytically derive a time domain representation of these reconstruction ﬁlters and using the Fourier slice-projection theorem we derive their frequency responses. The quality of these ﬁlters, when used in reconstructing BCC sampled volumetric data, is discussed and is demonstrated with a raycaster. Moreover, to demonstrate the superiority of the BCC sampling, the resulting reconstructions are compared with those produced from similar ﬁlters applied to data sampled on the Cartesian lattice.},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {{IEEE} {Visualization} 2004},
	publisher = {IEEE Comput. Soc},
	author = {Entezari, A. and Dyer, R. and Moller, T.},
	year = {2004},
	pages = {11--18},
	file = {Entezari et al. - 2004 - Linear and cubic box splines for the body centered.pdf:/home/david/Zotero/storage/Q4MD9SQB/Entezari et al. - 2004 - Linear and cubic box splines for the body centered.pdf:application/pdf},
}

@article{horacsek_fast_2016,
	title = {Fast and exact evaluation of box splines via the {PP}-form},
	url = {http://arxiv.org/abs/1606.08910},
	abstract = {For the class of non-degenerate box splines, we prove that these box splines are piecewise polynomial. This is not a new result, it is in fact a well known and useful property of box splines. However, our proof is constructive, and the main result of this work is a corollary that follows from this proof, namely one that gives an explicit construction scheme for the polynomial pieces in the interior regions of any non-degenerate box spline.},
	language = {en},
	urldate = {2021-02-24},
	journal = {arXiv:1606.08910 [math]},
	author = {Horacsek, Joshua and Alim, Usman},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.08910},
	keywords = {Mathematics - Functional Analysis, Mathematics - Numerical Analysis},
	file = {Horacsek and Alim - 2016 - Fast and exact evaluation of box splines via the P.pdf:/home/david/Zotero/storage/FDPCAMML/Horacsek and Alim - 2016 - Fast and exact evaluation of box splines via the P.pdf:application/pdf},
}

@article{condat_three-directional_2006,
	title = {Three-directional box-splines: characterization and efficient evaluation},
	volume = {13},
	issn = {1070-9908},
	shorttitle = {Three-directional box-splines},
	url = {http://ieeexplore.ieee.org/document/1642713/},
	doi = {10.1109/LSP.2006.871852},
	abstract = {We propose a new characterization of three-directional box-splines, which are well adapted for interpolation and approximation on hexagonal lattices. Inspired by a construction already applied with success for exponential splines [1] and hex-splines [2], we characterize a box-spline as a convolution of a generating function, which is a Green function of the spline’s associated differential operator, and a discrete ﬁlter that plays the role of a localization operator. This process leads to an elegant analytical expression of three-directional box-splines. It also brings along a particularly efﬁcient implementation.},
	language = {en},
	number = {7},
	urldate = {2021-02-24},
	journal = {IEEE Signal Processing Letters},
	author = {Condat, L. and Van De Ville, D.},
	month = jul,
	year = {2006},
	pages = {417--420},
	file = {Condat and Van De Ville - 2006 - Three-directional box-splines characterization an.pdf:/home/david/Zotero/storage/JSMJQQMN/Condat and Van De Ville - 2006 - Three-directional box-splines characterization an.pdf:application/pdf},
}

@article{de_boor_evaluation_2000,
	title = {On the evaluation of box splines},
	language = {en},
	author = {de Boor, Carl},
	month = feb,
	year = {2000},
	pages = {19},
	file = {de Boor - 2000 - On the evaluation of box splines.pdf:/home/david/Zotero/storage/ZZ7I7GFD/de Boor - 2000 - On the evaluation of box splines.pdf:application/pdf},
}

@article{speleers_inner_2015,
	title = {Inner products of box splines and their derivatives},
	volume = {55},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/s10543-014-0513-1},
	doi = {10.1007/s10543-014-0513-1},
	abstract = {A simple and explicit expression is given for the inner product of (higher order) derivatives of multivariate box splines and their translates. We also show that the energy inner product related to a linear partial differential equation discretized with a set of shifted box splines can be interpreted as an evaluation of the differential operator applied to a higher order box spline.},
	language = {en},
	number = {2},
	urldate = {2021-02-24},
	journal = {BIT Numerical Mathematics},
	author = {Speleers, Hendrik},
	month = jun,
	year = {2015},
	pages = {559--567},
	file = {Speleers - 2015 - Inner products of box splines and their derivative.pdf:/home/david/Zotero/storage/KQNDLECX/Speleers - 2015 - Inner products of box splines and their derivative.pdf:application/pdf},
}

@book{hansen_discrete_2010,
	address = {Philadelphia},
	series = {Fundamentals of algorithms},
	title = {Discrete inverse problems: insight and algorithms},
	isbn = {978-0-89871-696-2},
	shorttitle = {Discrete inverse problems},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Hansen, Per Christian},
	year = {2010},
	note = {OCLC: ocn469915410},
	keywords = {Inverse problems (Differential equations)},
	file = {Hansen - 2010 - Discrete inverse problems insight and algorithms.pdf:/home/david/Zotero/storage/6D4E4743/Hansen - 2010 - Discrete inverse problems insight and algorithms.pdf:application/pdf},
}

@book{rieder_keine_2003,
	address = {Wiesbaden},
	title = {Keine {Probleme} mit {Inversen} {Problemen}},
	isbn = {978-3-528-03198-5 978-3-322-80234-7},
	url = {http://link.springer.com/10.1007/978-3-322-80234-7},
	language = {de},
	urldate = {2021-03-04},
	publisher = {Vieweg+Teubner Verlag},
	author = {Rieder, Andreas},
	year = {2003},
	doi = {10.1007/978-3-322-80234-7},
	file = {Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:/home/david/Zotero/storage/C2Y88G2D/Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:text/html;Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:/home/david/Zotero/storage/U4EAHJF9/Rieder - 2003 - Keine Probleme mit Inversen Problemen.pdf:application/pdf},
}

@article{entezari_practical_2008,
	title = {Practical {Box} {Splines} for {Reconstruction} on the {Body} {Centered} {Cubic} {Lattice}},
	volume = {14},
	issn = {1077-2626, 1941-0506, 2160-9306},
	url = {https://ieeexplore.ieee.org/document/4359498/},
	doi = {10.1109/TVCG.2007.70429},
	abstract = {We introduce a family of box splines for efficient, accurate, and smooth reconstruction of volumetric data sampled on the body-centered cubic (BCC) lattice, which is the favorable volumetric sampling pattern due to its optimal spectral sphere packing property. First, we construct a box spline based on the four principal directions of the BCC lattice that allows for a linear C0 reconstruction. Then, the design is extended for higher degrees of continuity. We derive the explicit piecewise polynomial representations of the C0 and C2 box splines that are useful for practical reconstruction applications. We further demonstrate that approximation in the shift-invariant space—generated by BCC-lattice shifts of these box splines—is twice as efficient as using the tensor-product B-spline solutions on the Cartesian lattice (with comparable smoothness and approximation order and with the same sampling density). Practical evidence is provided demonstrating that the BCC lattice not only is generally a more accurate sampling pattern, but also allows for extremely efficient reconstructions that outperform tensor-product Cartesian reconstructions.},
	language = {en},
	number = {2},
	urldate = {2021-03-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Entezari, Alireza and Van De Ville, Dimitri and Moller, Torsten},
	month = mar,
	year = {2008},
	pages = {313--328},
	file = {Entezari et al. - 2008 - Practical Box Splines for Reconstruction on the Bo.pdf:/home/david/Zotero/storage/FMXDXXTX/Entezari et al. - 2008 - Practical Box Splines for Reconstruction on the Bo.pdf:application/pdf},
}

@article{paleo_practical_2016,
	title = {A practical local tomography reconstruction algorithm based on known subregion},
	url = {http://arxiv.org/abs/1606.04940},
	abstract = {We propose a new method to reconstruct data acquired in a local tomography setup. This method uses an initial reconstruction and reﬁnes it by correcting the low frequency artifacts known as the cupping eﬀect. A basis of Gaussian functions is used to correct the initial reconstruction. The coeﬃcients of this basis are iteratively optimized under the constraint of a known subregion. Using a coarse basis reduces the degrees of freedom of the problem while actually correcting the cupping eﬀect. Simulations show that the known region constraint yields an unbiased reconstruction, in accordance to uniqueness theorems stated in local tomography.},
	language = {en},
	urldate = {2021-03-10},
	journal = {arXiv:1606.04940 [physics]},
	author = {Paleo, Pierre and Desvignes, Michel and Mirone, Alessandro},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04940},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Physics - Medical Physics},
	file = {Paleo et al. - 2016 - A practical local tomography reconstruction algori.pdf:/home/david/Zotero/storage/X5MLXQN7/Paleo et al. - 2016 - A practical local tomography reconstruction algori.pdf:application/pdf},
}

@inproceedings{zhou_blob-based_2008,
	address = {San Diego, CA},
	title = {A blob-based tomographic reconstruction of {3D} coronary trees from rotational x-ray angiography},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.769478},
	doi = {10.1117/12.769478},
	abstract = {A method is proposed for a 3D reconstruction of coronary networks from rotational projections that departs from motion-compensated approaches. It deals with multiple views extracted from a time-stamped image sequence through ECG gating. This statistics-based vessel reconstruction method relies on a new imaging model by considering both the eﬀect of background tissues and the image representation using spherically-symmetric basis functions, also called ’blobs’ . These blobs have a closed analytical expression for the X-ray transform, which makes easier to compute a cone-beam projection than a voxel-based description. A Bayesian maximum a posteriori (MAP) estimation is used with a Poisson distributed projection data instead of the Gaussian approximation often used in tomography reconstruction. A heavy-tailed distribution is proposed as image prior to take into account the sparse nature of the object of interest. The optimization is performed by an expectationmaximization like (EM) block iterative algorithm which oﬀers a fast convergence and a sound introduction of the non-negativity constraint for vessel attenuation coeﬃcients. Simulations are performed using a model of coronary tree extracted from multidetector CT scanner and a performance study is conducted. They point out that, even with severe angular undersampling (6 projections over 110 degrees for instance) and without introducing a prior model of the object, signiﬁcant results can be achieved.},
	language = {en},
	urldate = {2021-03-10},
	author = {Zhou, Jian and Bousse, Alexandre and Yang, Guanyu and Bellanger, Jean-Jacques and Luo, Limin and Toumoulin, Christine and Coatrieux, Jean-Louis},
	editor = {Hsieh, Jiang and Samei, Ehsan},
	month = mar,
	year = {2008},
	pages = {69132N},
	file = {Zhou et al. - 2008 - A blob-based tomographic reconstruction of 3D coro.pdf:/home/david/Zotero/storage/C7TUBHKX/Zhou et al. - 2008 - A blob-based tomographic reconstruction of 3D coro.pdf:application/pdf},
}

@article{castrillo_blob-enhanced_2016,
	title = {Blob-enhanced reconstruction technique},
	volume = {27},
	issn = {0957-0233, 1361-6501},
	url = {https://iopscience.iop.org/article/10.1088/0957-0233/27/9/094011},
	doi = {10.1088/0957-0233/27/9/094011},
	abstract = {A method to enhance the quality of the tomographic reconstruction and, consequently, the 3D velocity measurement accuracy, is presented. The technique is based on integrating information on the objects to be reconstructed within the algebraic reconstruction process. A first guess intensity distribution is produced with a standard algebraic method, then the distribution is rebuilt as a sum of Gaussian blobs, based on location, intensity and size of agglomerates of light intensity surrounding local maxima. The blobs substitution regularizes the particle shape allowing a reduction of the particles discretization errors and of their elongation in the depth direction. The performances of the blob-enhanced reconstruction technique (BERT) are assessed with a 3D synthetic experiment. The results have been compared with those obtained by applying the standard camera simultaneous multiplicative reconstruction technique (CSMART) to the same volume. Several blob-enhanced reconstruction processes, both substituting the blobs at the end of the CSMART algorithm and during the iterations (i.e. using the blob-enhanced reconstruction as predictor for the following iterations), have been tested. The results confirm the enhancement in the velocity measurements accuracy, demonstrating a reduction of the bias error due to the ghost particles. The improvement is more remarkable at the largest tested seeding densities. Additionally, using the blobs distributions as a predictor enables further improvement of the convergence of the reconstruction algorithm, with the improvement being more considerable when substituting the blobs more than once during the process. The BERT process is also applied to multi resolution (MR) CSMART reconstructions, permitting simultaneously to achieve remarkable improvements in the flow field measurements and to benefit from the reduction in computational time due to the MR approach. Finally, BERT is also tested on experimental data, obtaining an increase of the signal-to-noise ratio in the reconstructed flow field and a higher value of the correlation factor in the velocity measurements with respect to the volume to which the particles are not replaced.},
	language = {en},
	number = {9},
	urldate = {2021-03-10},
	journal = {Measurement Science and Technology},
	author = {Castrillo, Giusy and Cafiero, Gioacchino and Discetti, Stefano and Astarita, Tommaso},
	month = sep,
	year = {2016},
	pages = {094011},
	file = {Castrillo et al. - 2016 - Blob-enhanced reconstruction technique.pdf:/home/david/Zotero/storage/ZC3AVLSH/Castrillo et al. - 2016 - Blob-enhanced reconstruction technique.pdf:application/pdf},
}

@article{herman_basis_2015,
	title = {Basis {Functions} in {Image} {Reconstruction} {From} {Projections}: {A} {Tutorial} {Introduction}},
	volume = {16},
	issn = {1557-2064, 1557-2072},
	shorttitle = {Basis {Functions} in {Image} {Reconstruction} {From} {Projections}},
	url = {http://link.springer.com/10.1007/s11220-015-0107-2},
	doi = {10.1007/s11220-015-0107-2},
	abstract = {The series expansion approaches to image reconstruction from projections assume that the object to be reconstructed can be represented as a linear combination of ﬁxed basis functions and the task of the reconstruction algorithm is to estimate the coefﬁcients in such a linear combination based on the measured projection data. It is demonstrated that using spherically symmetric basis functions (blobs), instead of ones based on the more traditional pixels, yields superior reconstructions of medically relevant objects. The demonstration uses simulated computerized tomography projection data of head cross-sections and the series expansion method ART for the reconstruction. In addition to showing the results of one anecdotal example, the relative efﬁcacy of using pixel and blob basis functions in image reconstruction from projections is also evaluated using a statistical hypothesis testing based task oriented comparison methodology. The superiority of the efﬁcacy of blob basis functions over that of pixel basis function is found to be statistically signiﬁcant.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Sensing and Imaging},
	author = {Herman, Gabor T.},
	month = dec,
	year = {2015},
	keywords = {blobs},
	pages = {6},
	file = {Herman - 2015 - Basis Functions in Image Reconstruction From Proje.pdf:/home/david/Zotero/storage/CRBR5QWI/Herman - 2015 - Basis Functions in Image Reconstruction From Proje.pdf:application/pdf},
}

@article{lewitt_alternatives_1992,
	title = {Alternatives to voxels for image representation in iterative reconstruction algorithms},
	volume = {37},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/37/3/015},
	doi = {10.1088/0031-9155/37/3/015},
	abstract = {Spherically symmetric volume elements are alternatives to the more conventional voxels far the Construction o f volume images in the computer. The image representation, and the calculation of projections of it, are essential components of iterative algorithms for image reconstmction from projenion data. A two-parameter family of spherical volume elements is described that allows control of the smoothness properties of the represented image, whereas conventional voxek are discontinuous. The rotational symmetry of the spherical elements leads to efficient calculation of projections of the represented image, as required in iterative reconstruction algorithms. Far volume elements whose shape is ellipsoidal (rather than spherical) it is shown that efficient calculation of the projections is also possible by means of an image space transformation.},
	language = {en},
	number = {3},
	urldate = {2021-03-10},
	journal = {Physics in Medicine and Biology},
	author = {Lewitt, R M},
	month = mar,
	year = {1992},
	pages = {705--716},
	file = {Lewitt - 1992 - Alternatives to voxels for image representation in.pdf:/home/david/Zotero/storage/YXG4DKA2/Lewitt - 1992 - Alternatives to voxels for image representation in.pdf:application/pdf},
}

@article{matej_efficient_1995,
	title = {Efficient {3D} grids for image reconstruction using spherically-symmetric volume elements},
	volume = {42},
	issn = {00189499},
	url = {http://ieeexplore.ieee.org/document/467854/},
	doi = {10.1109/23.467854},
	language = {en},
	number = {4},
	urldate = {2021-03-10},
	journal = {IEEE Transactions on Nuclear Science},
	author = {Matej, S. and Lewitt, R.M.},
	month = aug,
	year = {1995},
	pages = {1361--1370},
	file = {Matej and Lewitt - 1995 - Efficient 3D grids for image reconstruction using .pdf:/home/david/Zotero/storage/YUQSMM4P/Matej and Lewitt - 1995 - Efficient 3D grids for image reconstruction using .pdf:application/pdf},
}

@article{matej_practical_1996,
	title = {Practical considerations for 3-{D} image reconstruction using spherically symmetric volume elements},
	volume = {15},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/481442/},
	doi = {10.1109/42.481442},
	abstract = {Spherically symmetric volume elements with smooth tapering of the values near their boundaries are alternatives to the more conventional voxels for the construction of volume images in the computer. Their use, instead of voxels, introduces additional parameters which enable the user to control the shape of the volume element (blob) and consequently to control the characteristics of the images produced by iterative methods for reconstruction from projection data. For images composed of blobs, efficient algorithms have been designed €or the projection and discrete back-projection operations, which are the crucial parts of iterative reconstruction methods. We have investigated the relationship between the values of the blob parameters and the properties of images represented by the blobs. Experiments show that using blobs in iterative reconstruction methods leads to substantial improvement in the reconstruction performance, based on visual quality and on quantitative measures, in comparison with the voxel case. The images reconstructed using appropriately chosen blobs are characterized by less image noise for both noiseless data and noisy data, without loss of image resolution.},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Matej, S. and Lewitt, R.M.},
	month = feb,
	year = {1996},
	pages = {68--78},
	file = {Matej and Lewitt - 1996 - Practical considerations for 3-D image reconstruct.pdf:/home/david/Zotero/storage/497ZYSHG/Matej and Lewitt - 1996 - Practical considerations for 3-D image reconstruct.pdf:application/pdf},
}

@article{bippus_projector_2011,
	title = {Projector and {Backprojector} for {Iterative} {CT} {Reconstruction} with {Blobs} using {CUDA}},
	abstract = {Using blobs allows modeling the CT system’s geometry more correctly within an iterative reconstruction framework. However their application comes with an increased computational demand. This led us to use blobs for image representation and a dedicated GPU hardware implementation to counteract their computational demand. Making extensive use of the texture interpolation capabilities of CUDA and implementing an asymmetric projector/backprojector pair we achieve reasonable processing times and good system modeling at the same time.},
	language = {en},
	author = {Bippus, Rolf-Dieter and Köhler, Thomas and Bergner, Frank and Brendel, Bernhard and Hansis, Eberhard and Proksa, Roland},
	year = {2011},
	pages = {4},
	file = {Bippus et al. - 2011 - Projector and Backprojector for Iterative CT Recon.pdf:/home/david/Zotero/storage/KEA2CVT8/Bippus et al. - 2011 - Projector and Backprojector for Iterative CT Recon.pdf:application/pdf},
}

@inproceedings{popescu_ray_2004,
	address = {Rome, Italy},
	title = {Ray tracing through a grid of blobs},
	volume = {6},
	isbn = {978-0-7803-8700-3},
	url = {http://ieeexplore.ieee.org/document/1466750/},
	doi = {10.1109/NSSMIC.2004.1466750},
	abstract = {In this paper we describe two ray tracing algorithms for images represented using spherically symmetric basis functions (blobs) on regular grids. The method presented here allows more realistic modeling of the forward projection by considering tube shaped kernels, rather than simple lines. Each kernel is a function of the radial distance r from its center and can vary with the position l along the projection line. The forward projections are computed by convolutions of the kernel with the blob line integrals. Both ray tracing procedures presented incrementally compute the square distance r2 for each visited blob enabling the appropriate resolution kernel to be used. The second variant also computes the l coordinate along the line of response axis allowing for longitudinal variations of the resolution kernel to be considered as well as time-of-ﬂight (TOF) modeling.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {{IEEE} {Symposium} {Conference} {Record} {Nuclear} {Science} 2004.},
	publisher = {IEEE},
	author = {Popescu, L.M. and Lewitt, R.M.},
	year = {2004},
	pages = {3983--3986},
	file = {Popescu and Lewitt - 2004 - Ray tracing through a grid of blobs.pdf:/home/david/Zotero/storage/SZY9DRKS/Popescu and Lewitt - 2004 - Ray tracing through a grid of blobs.pdf:application/pdf},
}

@inproceedings{momey_new_2011,
	address = {Valencia, Spain},
	title = {A new representation and projection model for tomography, based on separable {B}-splines},
	url = {http://ieeexplore.ieee.org/document/6152700/},
	doi = {10.1109/NSSMIC.2011.6152700},
	abstract = {Data modelization in tomography is a key point for iterative reconstruction. The design of the projector, i.e. the numerical model of projection, is mostly inﬂuenced by the representation of the object of interest, decomposed on a discrete basis of functions.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {2011 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	publisher = {IEEE},
	author = {Momey, Fabien and Denis, Loic and Mennessier, Catherine and Thiebaut, Eric and Becker, Jean-Marie and Desbat, Laurent},
	month = oct,
	year = {2011},
	pages = {2602--2609},
	file = {Momey et al. - 2011 - A new representation and projection model for tomo.pdf:/home/david/Zotero/storage/GFS8TT3D/Momey et al. - 2011 - A new representation and projection model for tomo.pdf:application/pdf},
}

@inproceedings{zhang_box_2019,
	address = {Venice, Italy},
	title = {Box {Spline} {Projection} in {Non}-{Parallel} {Geometry}},
	isbn = {978-1-5386-3641-1},
	url = {https://ieeexplore.ieee.org/document/8759327/},
	doi = {10.1109/ISBI.2019.8759327},
	abstract = {The pixel- and voxel-basis are common choices for image discretization in the context of computed tomography (CT). They can also be viewed as ﬁrst-order box splines – a class of functions with closed-form X-ray and Radon transforms that can be computed efﬁciently. In this paper we derive a method for exact projection of box splines in a non-parallel geometry that can be used in fan-beam and cone-beam tomographic image reconstruction algorithms. We also provide efﬁcient computational procedures for evaluation of the basis function in the projection domain.},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {2019 {IEEE} 16th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2019)},
	publisher = {IEEE},
	author = {Zhang, Kai and Entezari, Alireza},
	month = apr,
	year = {2019},
	pages = {1844--1847},
	file = {Zhang and Entezari - 2019 - Box Spline Projection in Non-Parallel Geometry.pdf:/home/david/Zotero/storage/L2VHGIHT/Zhang and Entezari - 2019 - Box Spline Projection in Non-Parallel Geometry.pdf:application/pdf},
}

@inproceedings{lasser_elsa_2019,
	address = {Philadelphia, United States},
	title = {elsa - an elegant framework for tomographic reconstruction},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11072/2534833/elsa---an-elegant-framework-for-tomographic-reconstruction/10.1117/12.2534833.full},
	doi = {10.1117/12.2534833},
	abstract = {Software for tomographic reconstruction has been around for decades now. So why yet another software framework for tomographic reconstruction? Because we needed a ﬂexible, operator- and optimization-based framework in C++ for our own target applications, we developed our own some years ago. As our framework has been applied to many tomographic problems by now, ranging from optical tomography, lightﬁeld tomography, SPECT, to various X-ray based imaging modalities (absorption contrast, differential phase contrast and anisotropic dark-ﬁeld contrast), we decided to open source a modernized version of it. The framework elsa is written in platform-independent modern C++17 using the CMake build system, with high unit-test coverage and continuous integration to ascertain reliability and correctness, as well as a Python interface for easy and rapid prototyping. Our intent in open sourcing the framework and presenting it here is three-fold, ﬁrst for easier reproducibility of our own research, second for use in teaching, and last but not least, in the hopes that some of you also ﬁnd some usefulness in it for your own tasks.},
	language = {en},
	urldate = {2021-04-13},
	booktitle = {15th {International} {Meeting} on {Fully} {Three}-{Dimensional} {Image} {Reconstruction} in {Radiology} and {Nuclear} {Medicine}},
	publisher = {SPIE},
	author = {Lasser, Tobias and Hornung, Maximilian and Frank, David},
	editor = {Matej, Samuel and Metzler, Scott D.},
	month = may,
	year = {2019},
	pages = {69},
	file = {Lasser et al. - 2019 - elsa - an elegant framework for tomographic recons.pdf:/home/david/Zotero/storage/KBNCTUTY/Lasser et al. - 2019 - elsa - an elegant framework for tomographic recons.pdf:application/pdf},
}

@article{kim_optimized_2016,
	title = {Optimized first-order methods for smooth convex minimization},
	volume = {159},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-015-0949-3},
	doi = {10.1007/s10107-015-0949-3},
	abstract = {We introduce new optimized ﬁrst-order methods for smooth unconstrained convex minimization. Drori and Teboulle (Math Program 145(1–2):451–482, 2014. doi:10.1007/s10107-013-0653-0) recently described a numerical method for computing the N -iteration optimal step coefﬁcients in a class of ﬁrst-order algorithms that includes gradient methods, heavy-ball methods (Polyak in USSR Comput Math Math Phys 4(5):1–17, 1964. doi:10.1016/0041-5553(64)90137-5), and Nesterov’s fast gradient methods (Nesterov in Sov Math Dokl 27(2):372–376, 1983; Math Program 103(1):127–152, 2005. doi:10.1007/s10107-004-0552-5). However, the numerical method in Drori and Teboulle (2014) is computationally expensive for large N , and the corresponding numerically optimized ﬁrst-order algorithm in Drori and Teboulle (2014) requires impractical memory and computation for large-scale optimization problems. In this paper, we propose optimized ﬁrst-order algorithms that achieve a convergence bound that is two times smaller than for Nesterov’s fast gradient methods; our bound is found analytically and reﬁnes the numerical bound in Drori and Teboulle (2014). Furthermore, the proposed optimized ﬁrst-order methods have efﬁcient forms that are remarkably similar to Nesterov’s fast gradient methods.},
	language = {en},
	number = {1-2},
	urldate = {2021-04-15},
	journal = {Mathematical Programming},
	author = {Kim, Donghwan and Fessler, Jeffrey A.},
	month = sep,
	year = {2016},
	pages = {81--107},
	file = {Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:/home/david/Zotero/storage/REIHELDD/Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:application/pdf;Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:/home/david/Zotero/storage/WJXYZPC5/Kim and Fessler - 2016 - Optimized first-order methods for smooth convex mi.pdf:application/pdf},
}

@article{wang_image_2011,
	title = {Image representation by blob and its application in {CT} reconstruction from few projections},
	doi = {10.1109/NSSMIC.2011.6153755},
	abstract = {The localized radial symmetric function, or blob, is an ideal alternative to the pixel basis for X-ray computed tomography (CT) image reconstruction. In this paper we develop image representation models using blob, and propose reconstruction methods for few projections data. The image is represented in a shift invariant space generated by a Gaussian blob or a multiscale blob system of different frequency selectivity, and the reconstruction is done through minimizing the Total Variation or the 1 norm of blob coefficients. Some 2D numerical results are presented, where we use GPU platform for accelerating the X-ray projection and back-projection, the interpolation and the gradient computations.},
	author = {Wang, Han and Desbat, Laurent and Legoupil, Samuel},
	month = jul,
	year = {2011},
}

@article{briand_theory_2018,
	title = {Theory and {Practice} of {Image} {B}-{Spline} {Interpolation}},
	volume = {8},
	url = {https://doi.org/10.5201%2Fipol.2018.221},
	doi = {10.5201/ipol.2018.221},
	journal = {Image Processing On Line},
	author = {Briand, Thibaud and Monasse, Pascal},
	month = jul,
	year = {2018},
	note = {Publisher: Image Processing On Line},
	pages = {99--141},
	file = {Briand and Monasse - 2018 - Theory and Practice of Image B-Spline Interpolatio.pdf:/home/david/Zotero/storage/R3AH248T/Briand and Monasse - 2018 - Theory and Practice of Image B-Spline Interpolatio.pdf:application/pdf},
}

@article{li_optimization_2018,
	title = {Optimization for {Blob}-{Based} {Image} {Reconstruction} {With} {Generalized} {Kaiser}–{Bessel} {Basis} {Functions}},
	volume = {4},
	url = {https://doi.org/10.1109%2Ftci.2018.2796302},
	doi = {10.1109/tci.2018.2796302},
	number = {2},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Li, Yusheng},
	month = jun,
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	keywords = {Image reconstruction, blob, body-centered cubic (BCC) lattice, face-centered cubic (FCC) lattice, FCC, Image quality, Kaiser–Bessel radial basis function, lattice, Lattices, optimization, Optimization, simple cubic (SC) lattice, tomographic reconstruction, Tomography},
	pages = {257--270},
	file = {Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:/home/david/Zotero/storage/LK6X4YEC/Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:application/pdf;IEEE Xplore Full Text PDF:/home/david/Zotero/storage/GFPKWTXR/Li - 2018 - Optimization for Blob-Based Image Reconstruction W.pdf:application/pdf},
}

@article{nilchian_optimized_2015,
	title = {Optimized {Kaiser}–{Bessel} {Window} {Functions} for {Computed} {Tomography}},
	volume = {24},
	url = {https://doi.org/10.1109%2Ftip.2015.2451955},
	doi = {10.1109/tip.2015.2451955},
	number = {11},
	journal = {IEEE Transactions on Image Processing},
	author = {Nilchian, Masih and Ward, John Paul and Vonesch, Cedric and Unser, Michael},
	month = nov,
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {3826--3833},
	file = {Nilchian et al. - 2015 - Optimized Kaiser–Bessel Window Functions for Compu.pdf:/home/david/Zotero/storage/AGAU67MZ/Nilchian et al. - 2015 - Optimized Kaiser–Bessel Window Functions for Compu.pdf:application/pdf},
}

@article{pendrill_full_2021,
	title = {Full {Issue} {Download} {Vol}. 13 {No}. 1 2021 {The} {Importance} of the {Measurement} {Infrastructure} in {Economic} {Recovery} from the {COVID}-19 {Pandemic} {Richard} {J}. {C}. {Brown} , {Fiona} {Auty}, {Eugenio} {Renedo}, {Mike} {King} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.1 {Publisher} {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 18-21 {Abstract}: {This} paper describes the many, evidenced-based benefits to the economy of a well-developed measurement infrastructure. {In} particular, it explains how assuring confidence in measurement may be used to accelerate economic recovery from the {COVID}-19 pandemic including in emerging sectors such as the digital economy. {Recommendations} are made for providing near term support for national economic recovery whilst also demonstrating the advantages of sustained development of the measurement infrastructure in the medium-term to maximize the potential of future innovative and disruptive technologies. {These} recommendations, whilst focused on consideration of the {UK}, should apply globally. {References}: [1] {G}. {Tassey}, "{Underinvestment} in public good technologies," {J} {Technol}. {Transfer}, {Vol}. 30, pp. 89-113, 2004. https://doi.org/10.1007/s10961-004-4360-0 [2] {M}. {King}, and {E}. {Renedo}, "{Achieving} the 2.4\% {GDP} target: {The} role of measurement in increasing investment in {R}\&{D} and innovation," {NPL} {Report} {IEA} 3, {NPL}, {Teddington}, {UK}, {March} 2020. [3] {M}. {King} and {G}. {Tellett}, "{The} {National} {Measurement} {System}: {A} {Customer} {Survey} for {Three} of the {Core} {Labs} in the {National} {Measurement} {System}," {NMS} {Customer} {Survey} {Report} 2018, {NPL} {Teddington}, {UK}, {April} 2020 [4] {H}. {Kunzmann}, {T}. {Pfeifer}, {R}. {Schmitt}, {H}. {Schwenke}, and {A}.{Weckenmann}, "{Productive} metrology-adding value to manufacture," {CIRP} {Annals}, vol. 54, pp. 155-168, 2005. https://doi.org/10.1016/{S0007}-8506(07)60024-9 [5] {N}. {G}. {Orji}, {R}. {G}. {Dixson}, {A}. {Cordes}, {B}. {D}. {Bunday}, and {J}. {A}. {Allgair}, "{Measurement} traceability and quality assurance in a nanomanufacturing environment," {Instrumentation}, {Metrology}, and {Standards} for {Nanomanufacturing} {III}, {Proceedings} {Vol}. 7405, 740505, {August} 2009. https://doi.org/10.1117/12.826606 [6] {Belmana}, {Analysis} for {Policy} "{Public} {Support} for {Innovation} and {Business} {Outcomes}," {Belmana}: {London}, {UK}, 2020. [7] {R}. {Hawkins}, {Standards}, systems of innovation and policy in {Handbook} of {Innovation} and {Standards}. {Cheltenham}, {UK}: {Edward} {Elgar}, 2019. [8] {N}. {Nwaigbo}, and {M}. {King}, "{Evaluating} the {Impact} of the {NMS} {Consultancy} {Projects} on {Supported} {Firms} ({Working} {Paper})" {NPL}, {Teddington}, {UK}, 2020. [9] {M}. {King}, {R}. {Lambert}, and {P}. {Temple}, {Measurement}, standards and productivity spillovers in {Handbook} of {Innovation} and {Standards}. {Cheltenham}, {UK}: {Edward} {Elgar}, 2017, p. 162. https://doi.org/10.4337/9781783470082.00016 [10] {A}. {Font}, {K}. de {Hoogh}, {M}. {Leal}-{Sanchez}, {D}. {C}. {Ashworth}, {R}. {J}. {C}. {Brown}, {A}. {L}. {Hansell}, and {G}. {W}. {Fuller}, "{Using} metal ratios to detect emissions from municipal waste incinerators in ambient air pollution data," {Atmos}. {Environ}., vol. 113, pp. 177-186, {July} 2015. https://doi.org/10.1016/j.atmosenv.2015.05.002 [11] {S}. {Giannis}, {M}. {R}. {L}. {Gower}, {G}. {D}. {Sims}, {G}. {Pask}, and {G}. {Edwards}, "{Increasing} {UK} competitiveness by enhancing the composite materials regulatory infrastructure," {NPL} {Report} {MAT} 90, {NPL}, {Teddington}, {UK}, {October} 2019. [12] {HM} {Government}, {UK} {Research} and {Development} {Roadmap}, {BEIS}, {London}, {July} 2020. [13] {M}. {R}. {Mehra}, {S}. {S}. {Desai}, {F}. {Ruschitzka}, and {A}. {N}. {Patel}, "{Hydroxychloroquine} or chloroquine with or without a macrolide for treatment of {COVID}-19: a multinational registry analysis," {Lancet}, 2020, https://doi.org/10.1016/{S0140}-6736(20)31180-6 ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) ©2021 {NCSL} {International} {Smart} {Power} {Supply} {Calibration} {System} {Iraj} {Vasaeli} , {Brandon} {Umansky} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.2 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 22-27 {Abstract}: {This} paper details the development of an automated procedure to conduct calibrations of power supplies at {Jet} {Propulsion} {Laboratory}, {California} {Institute} of {Technology} ({JPL}). {The} fundamentals of power supply calibrations are given, and discussion on the method by which this custom software handles that calibration. {Additionally}, this technique provides real time uncertainty quantification of the calibrations. {This} automated system has demonstrated a time savings over existing automated techniques in use today. {References}: [1] {Keysight}, "{Low}-{Profile} {Modular} {Power} {System} {Series} {N6700} {Service} {Guide}", {Part} {Number}: 5969 2938, {Edition} 7, {January} 2015. [2] {B}. {N}. {Taylor} and {C}. {E}. {Kuyatt}, "{Guidelines} for {Evaluating} and {Expressing} the {Uncertainty} of {NIST} {Measurement} {Results}", {NIST} {Technical} {Note} 1297, 1994. https://doi.org/10.6028/{NIST}.{TN}.1297 [3] {JCGM}, "{Evaluation} of measurement data - {Guide} to the expression of uncertainty in measurement," first edition ({GUM} 1995 with minor corrections)," {JCGM} 100, 2008. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Computer} {Aided} {Verification} of {Voltage} {Dips} and {Short} {Interruption} {Generators} for {Electromagnetic} {Compatibility} {Immunity} {Test} in {Accordance} with {IEC} 61000-4-11: 2004 + {AMD}: 2017 {Hau} {Wah} {Lai} , {Cho} {Man} {Tsui} , {Hing} {Wah} {Li} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.3 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 28-39 {Abstract}: {This} paper describes a procedure and a computer-aided system developed by the {Standards} and {Calibration} {Laboratory} ({SCL}) for verification of voltage dip and short interruption generators in accordance with the international standard {IEC} 61000-4-11:2004+{AMD1}:2017. {The} verification is done by calibrating the specified parameters and comparing with the requirements stated in the standard. {The} parameters that should be calibrated are the ratios of the residual voltages to the rated voltage, the accuracy of the phase angle at switching, and the rise time, fall time, overshoot and undershoot of the switching waveform. {A} specially built adapter is used to convert the high voltage output waveforms of the generators to lower level signals to be acquired by a digital oscilloscope. {The} other circuits required for the testing are also provided. {In} addition, the paper discusses the uncertainty evaluations for the measured parameters. {References}: [1] {T}. {Williams}, and {K}. {Armstrong}, "{EMC} for {Systems} and {Installations} {Part} 6 - {Low}-{Frequency} {Magnetics} {Fields} ({Emissions} and {Immunity}) {Mains} {Dips}, {Dropouts}, {Interruptions}, {Sags}, {Brownouts} and {Swells}," {EMC} {Compliance} {Journal}, {August} 2000. [2] {M}.{I}. {Montrose}, and {E}. {M}. {Nakauchi}, {Testing} for {EMC} {Compliance}: {Approaches} and {Techniques}, {Wiley} {Interscience}, 2004. https://doi.org/10.1002/{047164465X} [3] {International} {Standard} {IEC} 61000-4-11:2004+{AMD1}:2017:{Electromagnetic} {Compatibility} ({EMC}) {Part} 4-11: {Testing} and measurement techniques - {Voltage} dips, short interruptions and voltage variations immunity tests. [4] {Evaluation} of measurement data - {Guide} to the expression of uncertainty in measurement, {First} {Edition} {JCGM} 100:2008. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Validation} of the {Photometric} {Method} {Used} for {Micropipette} {Calibration} {Elsa} {Batista} , {Isabel} {Godinho}, {George} {Rodrigues}, {Doreen} {Rumery} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.4 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 40-45 {Abstract}: {There} are two methods generally used for calibration of micropipettes: the gravimetric method described in {ISO} 8655-6:2002 and the photometric method described in {ISO} 8655-7:2005. {In} order to validate the photometric method, several micropipettes of different capacities from 0.1 µ{L} to 1000 µ{L} were calibrated using both methods (gravimetric and photometric) in two different laboratories, {IPQ} ({Portuguese} {Institute} for {Quality}) and {Artel}. {These} tests were performed by six different operators. {The} uncertainty for both methods was determined and it was verified that the uncertainty component that has a higher contribution to the final uncertainty budget depends on the volume delivered. {In} the photometric method for small volumes, the repeatability of the pipette is the largest uncertainty component, but for volumes, larger than 100 µ{L}, the photometric instrument is the most significant source of uncertainty. {Based} on all the results obtained with this study, one may consider the photometric method validated. {References}: [1] {ISO} 8655-1/2/6/7, {Piston}-operated volumetric apparatus, 2002. [2] {BIPM}, {International} {Vocabulary} of {Metrology}, 3rd edition, {JCGM} 200:2012. [3] {George} {Rodrigues}, {Bias} and transferability in standards methods of pipette calibration, {Artel}, {June} 2003. [4] {Taylor}, et.al. {The} definition of primary method of measurement ({PMM}) of the 'highest metrological quality': a challenge in understanding and communication, {Accred}. {Qual}.{Assur} (2001) 6:103-106. https://doi.org/10.1007/{PL00010444} [5] {EURAMET} project 1353, {Volume} comparison on {Calibration} of micropipettes - {Gravimetric} and photometric methods. [6] {ASTM} {E542}: {Standard} {Practice} for {Calibration} of laboratory {Volumetric} {Apparatus}, 2000. [7] {ISO} 4787; {Laboratory} glassware - {Volumetric} glassware - {Methods} for use and testing of capacity, 2010 . [8] {ISO} 13528:2005 - {Statistical} methods used in proficiency testing by interlaboratory comparisons. [9] {BIPM} et al, {Guide} to the {Expression} of {Uncertainty} in {Measurement} ({GUM}), 2nd ed., {International} {Organization} for {Standardization}, {Genève}, 1995. [10] {EURAMET} guide, cg 19, - {Guidelines} on the determination of uncertainty in gravimetric volume calibration, version 3.0, 2012. [11] {E}. {Batista} et all, {A} {Study} of {Factors} that {Influence} {Micropipette} {Calibrations}, {Measure} {Vol}. 10 {No}. 1, 2015 https://doi.org/10.1080/19315775.2015.11721717 [12] www.{BIPM}.org. ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Material} {Flow} {Rate} {Estimation} in {Material} {Extrusion} {Additive} {Manufacturing} {G}. {P}. {Greeff} {NCSLI} {Measure} {\textbar} {Vol}. 13 {No}. 1 (2021) {\textbar} doi.org/10.51843/measure.13.1.5 {Publisher}: {NCSL} {International} {\textbar} {Published} {February} 2021 {\textbar} {Pages} 46-56 {Abstract}: {The} additive manufacturing of products promises exciting possibilities. {Measurement} methodologies, which measure an in-process dataset of these products and interpret the results, are essential. {However}, before developing such a level of quality assurance several in-process measurands must be realized. {One} of these is the material flow rate, or rate of adding material during the additive manufacturing process. {Yet}, measuring this rate directly in material extrusion additive manufacturing presents challenges. {This} work presents two indirect methods to estimate the volumetric flow rate at the liquefier exit in material extrusion, specifically in {Fused} {Deposition} {Modeling} or {Fused} {Filament} {Fabrication}. {The} methods are cost effective and may be applied in future sensor integration. {The} first method is an optical filament feed rate and width measurement and the second is based on the liquefier pressure. {Both} are used to indirectly estimate the volumetric flow rate. {The} work also includes a description of linking the {G}-code command to the final print result, which may be used to create a per extrusion command model of the part. {References}: [1] {T}. {Wohlers}, {I}. {Campbell}, {O}. {Diegel}, {J}. {Kowen}, {I}. {Fidan}, and {D}.{L}. {Bourell}, "{Wohlers} {Report} 2017: {3D} {Printing} and {Additive} {Manufacturing} {State} of the {Industry} {Annual} {Worldwide} {Progress} {Report}," 2017. [2] {Additive} manufacturing – {General} principles – {Terminology}. {Geneva}, {CH}: {International} {Organization} for {Standardization}, 2015. [3] {R}. {Jones} et al., "{Reprap} - {The} replicating rapid prototyper," {Robotica}, vol. 29, no. 1 {SPEC}. {ISSUE}, pp. 177-191, 2011, https://doi.org/10.1017/{S026357471000069X} [4] {T}. {Wohlers} and {T}. {Gornet}, "{History} of {Additive} {Manufacturing} 2017," 2017. [5] {S}. {A}. {M}. {Tofail}, {E}. {P}. {Koumoulos}, {A}. {Bandyopadhyay}, {S}. {Bose}, {L}. {O}'{Donoghue}, and {C}. {Charitidis}, "{Additive} manufacturing: scientific and technological challenges, market uptake and opportunities, "{Materials} {Today}, vol. 21, no. 1, pp. 22-37, {Jan}. 2018, https://doi.org/10.1016/j.mattod.2017.07.001 [6] {G}. {Moroni} and {S}. {Petrò}, "{Managing} uncertainty in the new manufacturing era," {Procedia} {CIRP}, vol. 75, pp. 1-2, 2018, https://doi.org/10.1016/j.procir.2018.07.001 [7] {R}. {Leach} et al., "{Information}-rich manufacturing metrology,"in {Eighth} {International} {Precision} {Assembly} {Seminar} ({IPAS}), 2018, no. {January}. https://doi.org/10.1007/978-3-030-05931-6\_14 [8] {S}. {Moylan}, {J}. {Slotwinski}, {A}. {Cooke}, {K}. {Jurrens}, {M}. {A}. {Donmez}, and {A}. {Donmez}, "{Proposal} for a {Standardized} {Test} {Artifact} for {Additive} {Manufacturing} {Machines} and {Processes}," {Solid} {Freeform} {Fabrication} {Symposium} {Proceedings}, pp. 902-920, 2012. https://doi.org/10.6028/{NIST}.{IR}.7858 [9] {ASME} {Y14}.46-2017 {Product} {Definition} for {Additive} {Manufacturing}. {New} {York}:{The} {American} {Society} of {Mechanical} {Engineers}, 2017. [10] {H}. {Li}, {T}. {Wang}, {J}. {Sun}, and {Z}. {Yu}, "{The} effect of process parameters in fused deposition modelling on bonding degree and mechanical properties," {Rapid} {Prototyping} {Journal}, vol. 24, no. 1, pp. 80-92, {Jan}. 2018, https://doi.org/10.1108/{RPJ}-06-2016-0090 [11] {A}. {W}. {Gebisa} and {H}. {G}. {Lemu}, "{Investigating} effects of {Fused}-deposition modeling ({FDM}) processing parameters on flexural properties of {ULTEM} 9085 using designed experiment, "{Materials}, vol.11, no. 4, pp. 1-23, 2018, https://doi.org/10.3390/ma11040500 {PMid}:29584674 {PMCid}:{PMC5951346} [12] {B}. {Wittbrodt} and {J}. {M}. {Pearce}, "{The} effects of {PLA} color on material properties of 3-{D} printed components," {Additive} {Manufacturing}, vol. 8, pp. 110-116, 2015, https://doi.org/10.1016/j.addma.2015.09.006 [13] {O}. {A}. {Mohamed}, {S}. {H}. {Masood}, and {J}. {L}. {Bhowmik}, "{Optimization} of fused deposition modeling process parameters: a review of current research and future prospects," {Advances} in {Manufacturing}, vol. 3, no. 1, pp. 42-53, {Mar}. 2015, https://doi.org/10.1007/s40436-014-0097-7 [14] {S}. {K}. {Everton}, {M}. {Hirsch}, {P}. {Stravroulakis}, {R}. {K}. {Leach} and {A}. {T}. {Clare}, "{Review} of in-situ process monitoring and in-situ metrology for metal additive manufacturing," {Materials} and {Design}, vol. 95, pp. 431-445, 2016, https://doi.org/10.1016/j.matdes.2016.01.099 [15] {P}. {K}. {Rao}, {J}. {P}. {Liu}, {D}. {Roberson}, {Z}. {J}. {Kong}, and {C}. {Williams},"{Online} {Real}-{Time} {Quality} {Monitoring} in {Additive} {Manufacturing} {Processes} {Using} {Heterogeneous} {Sensors}," {Journal} of {Manufacturing} {Science} and {Engineering}, vol. 137, no. 6, p.061007, {Sep}. 2015, https://doi.org/10.1115/1.4029823 [16] {J}. {Pellegrino}, {T}. {Makila}, {S}. {McQueen}, and {E}. {Taylor}, "{Measurement} science roadmap for polymer-based additive manufacturing," {Gaithersburg}, {MD}, {Dec}. 2016. https://doi.org/10.6028/{NIST}.{AMS}.100-5 [17] {T}. {R}. {Kramer}, {F}. {M}. {Proctor}, and {E}. {Messina}, "{The} {NIST} {RS274NGC} {Interpreter} -{Version} 3," {Gaithersburg}, {Maryland}, 2000. https://doi.org/10.6028/{NIST}.{IR}.6556 [18] {B}. {N}. {Turner}, {R}. {Strong}, and {S}. {A}. {Gold}, "{A} review of melt extrusion additive manufacturing processes: {I}. {Process} design and modeling," {Rapid} {Prototyping} {Journal}, vol. 20, no. 3, pp.192-204, {Apr}. 2014, https://doi.org/10.1108/{RPJ}-01-2013-0012 [19] {Conrad} {Electronic}, "{Renkforce} {RF1000} {3D} {Drucker}," 2016. https://www.conrad.de/de/renkforce-rf1000-3d-drucker-single-extruder-inkl-software-franzis-designcad-v24-3d-printrenkforce-edition-1007508.html (accessed {Sep}. 20, 2016). [20] {G}. {Hodgson}, {A}. {Ranellucci}, and {J}. {Moe}, "{Slic3r} {Manual} - {Flow} {Math}," 2016. http://manual.slic3r.org/advanced/flow-math (accessed {Jun}. 21, 2016). [21] {Repetier}, "{Repetier}-{Firmware} {Documentation}." https://www.repetier.com/documentation/repetier firmware/repetier-firmware-introduction/ (accessed {Apr}. 17, 2018). [22] {B}. {Weiss}, {D}. {W}. {Storti}, and {M}. {A}. {Ganter}, "{Low}-cost closedloop control of a {3D} printer gantry," {Rapid} {Prototyping} {Journal}, vol. 21, no. 5, pp. 482-490, {Aug}. 2015, https://doi.org/10.1108/{RPJ}-09-2014-0108 [23] {R}. {L}. {Zinniel} and {J}. {S}. {Batchelder}, "{Volumetric} {Feed} {Control} for {Flexible} {Filament}," {US} 6085957, 2000. [24] {W}. {J}. {Heij}, {Applied} {Metrology} in {Additive} {Manufacturing}. {Delft}: {Delft} {University} of {Technology}, 2016. [25] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Closed} loop control of slippage during filament transport in molten material extrusion," {Additive} {Manufacturing}, vol. 14, pp. 31-38, 2017, https://doi.org/10.1016/j.addma.2016.12.005 [26] {G}. {P}. {Greeff}, {Applied} {Metrology} in {Additive} {Manufacturing}, vol. 60. {Berlin}: {Mensch} und {Buch}, 2019. [27] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Comparing} {Retraction} {Methods} with {Volumetric} {Exit} {Flow} {Measurement} in {Molten} {Material} {Extrusion}," in {Special} {Interest} {Group} meeting on {Dimensional} {Accuracy} and {Surface} {Finish} in {Additive} {Manufacturing}, 2017, no. {October}, pp. 70-74. [28] {G}. {P}. {Greeff} and {M}. {Schilling}, "{Single} print optimisation of fused filament fabrication parameters," {The} {International} {Journal} of {Advanced} {Manufacturing} {Technology}, {Aug}. 2018, https://doi.org/10.1007/s00170-018-2518-4 [29] {A}. {Bellini}, {S}. {Güçeri}, and {M}. {Bertoldi}, "{Liquefier} {Dynamics} in {Fused} {Deposition}," {Journal} of {Manufacturing} {Science} and {Engineering}, vol. 126, no. 2, p. 237, 2004, https://doi.org/10.1115/1.1688377 [30] {P}. {Virtanen} et al., "{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}," {Nature} {Methods}, vol. 17, no. 3, pp. 261-272, {Mar}. 2020, https://doi.org/10.1038/s41592-019-0686-2 {PMid}:32015543 {PMCid}:{PMC7056644} ({Print}: {ISSN} 1931-5775) ({Online}: {ISSN} 2381-0580) © 2021 {NCSL} {International} {Software} to {Maximize} {End}-{User} {Uptake} of {Conformity} {Assessment} with {Measurement} {Uncertainty}, {Including} {Bivariate} {Cases}. {The} {European} {EMPIR} {CASoft} {Project}},
	volume = {13},
	url = {http://dx.doi.org/10.51843/measure.13.1.6},
	doi = {10.51843/measure.13.1.6},
	number = {1},
	journal = {NCSL International measure},
	author = {Pendrill, L.R. and Allard, A. and Fischer, N. and Harris, P.M. and Nguyen, J. and Smith, I.M.},
	month = jan,
	year = {2021},
	note = {Publisher: NCSL International},
	pages = {58--69},
	file = {Pendrill et al. - 2021 - Full Issue Download Vol. 13 No. 1 2021 The Importa.pdf:/home/david/Zotero/storage/NP73N643/Pendrill et al. - 2021 - Full Issue Download Vol. 13 No. 1 2021 The Importa.pdf:application/pdf},
}

@article{unser_fast_1991,
	title = {Fast {B}-spline transforms for continuous image representation and interpolation},
	volume = {13},
	url = {https://doi.org/10.1109%2F34.75515},
	doi = {10.1109/34.75515},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	month = mar,
	year = {1991},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {277--285},
	file = {Unser et al. - 1991 - Fast B-spline transforms for continuous image repr.pdf:/home/david/Zotero/storage/23CG7G43/Unser et al. - 1991 - Fast B-spline transforms for continuous image repr.pdf:application/pdf},
}

@article{unser_b-spline_1993,
	title = {B-spline signal processing. {II}. {Efficiency} design and applications},
	volume = {41},
	url = {http://dx.doi.org/10.1109/78.193221},
	doi = {10.1109/78.193221},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	year = {1993},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {834--848},
	file = {Unser et al. - 1993 - B-spline signal processing. II. Efficiency design .pdf:/home/david/Zotero/storage/J3RL864C/Unser et al. - 1993 - B-spline signal processing. II. Efficiency design .pdf:application/pdf},
}

@article{unser_b-spline_1993-1,
	title = {B-spline signal processing. {I}. {Theory}},
	volume = {41},
	url = {http://dx.doi.org/10.1109/78.193220},
	doi = {10.1109/78.193220},
	number = {2},
	journal = {IEEE Transactions on Signal Processing},
	author = {Unser, M. and Aldroubi, A. and Eden, M.},
	year = {1993},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {821--833},
	file = {Unser et al. - 1993 - B-spline signal processing. I. Theory.pdf:/home/david/Zotero/storage/YJN9BZ62/Unser et al. - 1993 - B-spline signal processing. I. Theory.pdf:application/pdf},
}

@article{kogel_fast_2011,
	title = {A {Fast} {Gradient} method for embedded linear predictive control},
	volume = {44},
	issn = {14746670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474667016437997},
	doi = {10.3182/20110828-6-IT-1002.03322},
	abstract = {This work considers the fast solution of model predictive control problems for linear systems with input constraints and a quadratic cost criterion. If the resulting optimization problem arising from the model predictive control is solved online using the Fast Gradient method one needs to determine the gradient of the cost function. We propose a method, tailored for embedded control purposes, that eﬃciently calculates the gradient taking the underlying structure of the system into account. Moreover, we discuss how the stability of the plant inﬂuences the required number of iterations to obtain a solution within a prescribed accuracy.},
	language = {en},
	number = {1},
	urldate = {2021-12-12},
	journal = {IFAC Proceedings Volumes},
	author = {Kögel, Markus and Findeisen, Rolf},
	month = jan,
	year = {2011},
	pages = {1362--1367},
	file = {Kögel and Findeisen - 2011 - A Fast Gradient method for embedded linear predict.pdf:/home/david/Zotero/storage/3QNSJV36/Kögel and Findeisen - 2011 - A Fast Gradient method for embedded linear predict.pdf:application/pdf},
}

@book{nesterov_introductory_2004,
	address = {Boston, MA},
	series = {Applied {Optimization}},
	title = {Introductory {Lectures} on {Convex} {Optimization}},
	volume = {87},
	isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
	url = {http://link.springer.com/10.1007/978-1-4419-8853-9},
	language = {en},
	urldate = {2021-12-12},
	publisher = {Springer US},
	author = {Nesterov, Yurii},
	editor = {Pardalos, Panos M. and Hearn, Donald W.},
	year = {2004},
	doi = {10.1007/978-1-4419-8853-9},
	file = {Nesterov - 2004 - Introductory Lectures on Convex Optimization.pdf:/home/david/Zotero/storage/AA9LNYKW/Nesterov - 2004 - Introductory Lectures on Convex Optimization.pdf:application/pdf},
}

@article{xu_investigation_2012,
	title = {Investigation of discrete imaging models and iterative image reconstruction in differential {X}-ray phase-contrast tomography},
	volume = {20},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-20-10-10724},
	doi = {10.1364/OE.20.010724},
	abstract = {Differential X-ray phase-contrast tomography (DPCT) refers to a class of promising methods for reconstructing the X-ray refractive index distribution of materials that present weak X-ray absorption contrast. The tomographic projection data in DPCT, from which an estimate of the refractive index distribution is reconstructed, correspond to onedimensional (1D) derivatives of the two-dimensional (2D) Radon transform of the refractive index distribution. There is an important need for the development of iterative image reconstruction methods for DPCT that can yield useful images from few-view projection data, thereby mitigating the long data-acquisition times and large radiation doses associated with use of analytic reconstruction methods. In this work, we analyze the numerical and statistical properties of two classes of discrete imaging models that form the basis for iterative image reconstruction in DPCT. We also investigate the use of one of the models with a modern image reconstruction algorithm for performing few-view image reconstruction of a tissue specimen.},
	language = {en},
	number = {10},
	urldate = {2021-12-13},
	journal = {Optics Express},
	author = {Xu, Qiaofeng and Sidky, Emil Y. and Pan, Xiaochuan and Stampanoni, Marco and Modregger, Peter and Anastasio, Mark A.},
	month = may,
	year = {2012},
	pages = {10724},
	file = {Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:/home/david/Zotero/storage/E4F9TVK6/Xu et al. - 2012 - Investigation of discrete imaging models and itera.pdf:application/pdf},
}

@misc{noauthor_why_nodate,
	title = {why is {BSpline}() only working for regular grids? · {Issue} \#131 · {JuliaMath}/{Interpolations}.jl},
	shorttitle = {why is {BSpline}() only working for regular grids?},
	url = {https://github.com/JuliaMath/Interpolations.jl/issues/131},
	abstract = {I was wondering how hard it would be to support irregular grids for BSpline interpolation. I am aware that you have Gridded for this case, however, I often end up missing features of Gridded which ...},
	language = {en},
	urldate = {2021-12-13},
	journal = {GitHub},
	file = {Snapshot:/home/david/Zotero/storage/M6KNQ676/Interpolations.html:text/html},
}

@article{thevenaz_interpolation_2000,
	title = {Interpolation revisited [medical images application]},
	volume = {19},
	issn = {1558-254X},
	doi = {10.1109/42.875199},
	abstract = {Based on the theory of approximation, this paper presents a unified analysis of interpolation and resampling techniques. An important issue is the choice of adequate basis functions. The authors show that, contrary to the common belief, those that perform best are not interpolating. By opposition to traditional interpolation, the authors call their use generalized interpolation; they involve a prefiltering step when correctly applied. The authors explain why the approximation order inherent in any basis function is important to limit interpolation artifacts. The decomposition theorem states that any basis function endowed with approximation order ran be expressed as the convolution of a B spline of the same order with another function that has none. This motivates the use of splines and spline-based functions as a tunable way to keep artifacts in check without any significant cost penalty. The authors discuss implementation and performance issues, and they provide experimental evidence to support their claims.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Thevenaz, P. and Blu, T. and Unser, M.},
	month = jul,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Biomedical imaging, Convolution, Cost function, Heart, Interpolation, Kernel, Performance analysis, Sampling methods, Spline},
	pages = {739--758},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/9VM4R4YE/Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/9VWGRF2K/875199.html:text/html;Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:/home/david/Zotero/storage/5I9E2DQJ/Thevenaz et al. - 2000 - Interpolation revisited [medical images applicatio.pdf:application/pdf},
}

@misc{noauthor_spline_nodate,
	title = {Spline {Interpolation}},
	url = {http://bigwww.epfl.ch/thevenaz/interpolation/},
	urldate = {2021-12-13},
	file = {Spline Interpolation:/home/david/Zotero/storage/PWU9L84S/interpolation.html:text/html},
}

@article{ratnani_b-splines_nodate,
	title = {B-{Splines} and {IsoGeometric} {Analysis}},
	language = {en},
	author = {Ratnani, A and Sonnendrücker, E},
	pages = {15},
	file = {Ratnani and Sonnendrücker - B-Splines and IsoGeometric Analysis.pdf:/home/david/Zotero/storage/JQVDYASZ/Ratnani and Sonnendrücker - B-Splines and IsoGeometric Analysis.pdf:application/pdf},
}

@article{mccann_fast_2016,
	title = {Fast {3D} reconstruction method for differential phase contrast {X}-ray {CT}},
	volume = {24},
	issn = {1094-4087},
	url = {https://www.osapublishing.org/abstract.cfm?URI=oe-24-13-14564},
	doi = {10.1364/OE.24.014564},
	abstract = {We present a fast algorithm for fully 3D regularized X-ray tomography reconstruction for both traditional and differential phase contrast measurements. In many applications, it is critical to reduce the X-ray dose while producing high-quality reconstructions. Regularization is an excellent way to do this, but in the differential phase contrast case it is usually applied in a slice-by-slice manner. We propose using fully 3D regularization to improve the dose/quality trade-off beyond what is possible using slice-by-slice regularization. To make this computationally feasible, we show that the two computational bottlenecks of our iterative optimization process can be expressed as discrete convolutions; the resulting algorithms for computation of the X-ray adjoint and normal operator are fast and simple alternatives to regridding. We validate this algorithm on an analytical phantom as well as conventional CT and differential phase contrast measurements from two real objects. Compared to the slice-by-slice approach, our algorithm provides a more accurate reconstruction of the analytical phantom and better qualitative appearance on one of the two real datasets.},
	language = {en},
	number = {13},
	urldate = {2021-12-13},
	journal = {Optics Express},
	author = {McCann, Michael T. and Nilchian, Masih and Stampanoni, Marco and Unser, Michael},
	month = jun,
	year = {2016},
	pages = {14564},
	file = {McCann et al. - 2016 - Fast 3D reconstruction method for differential pha.pdf:/home/david/Zotero/storage/BMGS9ZWI/McCann et al. - 2016 - Fast 3D reconstruction method for differential pha.pdf:application/pdf},
}

@article{sorzano_xmipp_2004,
	title = {{XMIPP}: a new generation of an open-source image processing package for electron microscopy},
	volume = {148},
	issn = {10478477},
	shorttitle = {{XMIPP}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047847704001261},
	doi = {10.1016/j.jsb.2004.06.006},
	abstract = {X-windows based microscopy image processing package (Xmipp) is a specialized suit of image processing programs, primarily aimed at obtaining the 3D reconstruction of biological specimens from large sets of projection images acquired by transmission electron microscopy. This public-domain software package was introduced to the electron microscopy ﬁeld eight years ago, and since then it has changed drastically. New methodologies for the analysis of single-particle projection images have been added to classiﬁcation, contrast transfer function correction, angular assignment, 3D reconstruction, reconstruction of crystals, etc. In addition, the package has been extended with functionalities for 2D crystal and electron tomography data. Furthermore, its current implementation in C++, with a highly modular design of well-documented data structures and functions, oﬀers a convenient environment for the development of novel algorithms. In this paper, we present a general overview of a new generation of Xmipp that has been re-engineered to maximize ﬂexibility and modularity, potentially facilitating its integration in future standardization eﬀorts in the ﬁeld. Moreover, by focusing on those developments that distinguish Xmipp from other packages available, we illustrate its added value to the electron microscopy community.},
	language = {en},
	number = {2},
	urldate = {2021-12-13},
	journal = {Journal of Structural Biology},
	author = {Sorzano, C.O.S. and Marabini, R. and Velázquez-Muriel, J. and Bilbao-Castro, J.R. and Scheres, S.H.W. and Carazo, J.M. and Pascual-Montano, A.},
	month = nov,
	year = {2004},
	pages = {194--204},
	file = {Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:/home/david/Zotero/storage/4WWCHNQ6/Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:application/pdf;Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:/home/david/Zotero/storage/48JAP5NK/Sorzano et al. - 2004 - XMIPP a new generation of an open-source image pr.pdf:application/pdf},
}

@misc{noauthor_xmipp_2021,
	title = {Xmipp},
	copyright = {GPL-3.0},
	url = {https://github.com/I2PC/xmipp/blob/81681048bdc75911293b78fcb50c77c037688ba1/src/xmipp/libraries/data/blobs.cpp},
	abstract = {Xmipp is a suite of image processing programs, primarily aimed at single-particle 3D electron microscopy.},
	urldate = {2021-12-13},
	publisher = {Instruct Image Processing Center},
	month = dec,
	year = {2021},
	note = {original-date: 2018-06-11T13:30:44Z},
}

@incollection{herman_computerized_2015,
	title = {Computerized {Tomography} {Reconstruction} {Methods}},
	isbn = {978-0-12-397316-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123970251002864},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Brain {Mapping}},
	publisher = {Elsevier},
	author = {Herman, G.T.},
	year = {2015},
	doi = {10.1016/B978-0-12-397025-1.00286-4},
	pages = {203--208},
	file = {Herman - 2015 - Computerized Tomography Reconstruction Methods.pdf:/home/david/Zotero/storage/SCFNNYII/Herman - 2015 - Computerized Tomography Reconstruction Methods.pdf:application/pdf},
}

@book{levakhina_three-dimensional_2014,
	address = {Wiesbaden},
	title = {Three-{Dimensional} {Digital} {Tomosynthesis}: {Iterative} {Reconstruction}, {Artifact} {Reduction} and {Alternative} {Acquisition} {Geometry}},
	shorttitle = {Three-{Dimensional} {Digital} {Tomosynthesis}},
	url = {http://link.springer.com/10.1007/978-3-658-05697-1},
	language = {en},
	urldate = {2021-12-13},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Levakhina, Yulia},
	year = {2014},
	doi = {10.1007/978-3-658-05697-1},
	file = {Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/TBFGYC39/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf;Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/AWN9BCTH/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf;Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:/home/david/Zotero/storage/BCFZIF4A/Levakhina - 2014 - Three-Dimensional Digital Tomosynthesis Iterative.pdf:application/pdf},
}

@article{schmitt_analysis_nodate,
	title = {Analysis of bias induced by various forward projection models in iterative reconstruction},
	abstract = {Discrete representation of the CT image is a major step in the design of iterative reconstruction algorithms, particularly because the decision being made at this level affects both bias and noise properties of the reconstruction, in addition to choices made later in the algorithm design. In this work, we examine the bias induced by popular image representation models, namely Joseph’s method and the basis function approach relying on B-splines and blobs. Our preliminary results highlight a common weakness in terms of overshoot and undershoot artifacts at sharp boundaries. They also show that the Blobs may perform only as well as the B-spline of order two in terms of bias, and that Joseph’s method tends to produce results that are fairly comparable to the B-spline of order one, with a slight advantage in favor of the latter.},
	language = {en},
	author = {Schmitt, K and Schondube, H and Stierstorfer, K and Hornegger, J and Noo, F},
	pages = {5},
	file = {Schmitt et al. - Analysis of bias induced by various forward projec.pdf:/home/david/Zotero/storage/HBHTAIXL/Schmitt et al. - Analysis of bias induced by various forward projec.pdf:application/pdf;Schmitt et al. - Analysis of bias induced by various forward projec.pdf:/home/david/Zotero/storage/YGP889DV/Schmitt et al. - Analysis of bias induced by various forward projec.pdf:application/pdf},
}

@misc{noauthor_kaiser_nodate,
	title = {Kaiser window approximation},
	url = {https://dsp.stackexchange.com/questions/37714/kaiser-window-approximation},
	urldate = {2021-12-13},
	journal = {Signal Processing Stack Exchange},
}

@incollection{govil_fast_2006,
	title = {A {Fast} {Algorithm} for {Spherical} {Basis} {Approximation}},
	volume = {282},
	isbn = {978-1-58488-636-5 978-1-4200-1138-8},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420011388.ch13},
	abstract = {Radial basis functions appear in a wide ﬁeld of applications in numerical mathematics and computer science. We present a fast algorithm for scattered data interpolation and approximation on the sphere with spherical radial basis functions of diﬀerent spatial density. We discuss three settings, each leading to a special structure of the interpolation matrix allowing for an eﬃcient implementation using discrete Fourier transforms. A numerical example is given to show the advantages of spherical radial basis functions with diﬀerent spatial densities.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Frontiers in {Interpolation} and {Approximation}},
	publisher = {Chapman and Hall/CRC},
	author = {Keiner, J and Prestin, J},
	collaborator = {Govil, N and Mhaskar, H and Mohapatra, Ram and Nashed, Zuhair and Szabados, J},
	month = jul,
	year = {2006},
	doi = {10.1201/9781420011388.ch13},
	note = {Series Title: Pure and Applied Mathematics},
	pages = {259--286},
	file = {Keiner and Prestin - 2006 - A Fast Algorithm for Spherical Basis Approximation.pdf:/home/david/Zotero/storage/QBMDC7UF/Keiner and Prestin - 2006 - A Fast Algorithm for Spherical Basis Approximation.pdf:application/pdf},
}

@inproceedings{levakhina_distance-driven_2010,
	address = {Knoxville, TN},
	title = {Distance-driven projection and backprojection for spherically symmetric basis functions in {CT}},
	isbn = {978-1-4244-9106-3},
	url = {http://ieeexplore.ieee.org/document/5874325/},
	doi = {10.1109/NSSMIC.2010.5874325},
	abstract = {Forward- and backprojecton pair plays an important role in computed tomography (CT). Since they are used in clinical routine for ﬁltered backprojection (FBP) reconstruction, in iterative reconstruction methods, for artifact correction and simulation purposes, they have to be fast, accurate and memory efﬁcient. Recently, a distance-driven approach for pixel basis functions has been proposed. At the moment, it is a stateof-the-art method that is almost artifact free, fast and has a predictable memory pattern access. In the work presented here, the distance-driven approach for the two-dimensional case is extended for spherically symmetric Kaiser-Bessel basis functions. Usage of these basis functions allows for constructing a smooth and continuous function for image representation.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {{IEEE} {Nuclear} {Science} {Symposuim} \& {Medical} {Imaging} {Conference}},
	publisher = {IEEE},
	author = {Levakhina, Y and Buzug, T M},
	month = oct,
	year = {2010},
	pages = {2894--2897},
	file = {Levakhina and Buzug - 2010 - Distance-driven projection and backprojection for .pdf:/home/david/Zotero/storage/7QQKY6AS/Levakhina and Buzug - 2010 - Distance-driven projection and backprojection for .pdf:application/pdf},
}

@article{levakhina_weighted_2013,
	title = {Weighted simultaneous algebraic reconstruction technique for tomosynthesis imaging of objects with high-attenuation features: \textbf{ \textit{ω} } {SART} for tomosynthesis of objects with high-attenuation features},
	volume = {40},
	issn = {00942405},
	shorttitle = {Weighted simultaneous algebraic reconstruction technique for tomosynthesis imaging of objects with high-attenuation features},
	url = {http://doi.wiley.com/10.1118/1.4789592},
	doi = {10.1118/1.4789592},
	abstract = {Purpose: This paper introduces a nonlinear weighting scheme into the backprojection operation within the simultaneous algebraic reconstruction technique (SART). It is designed for tomosynthesis imaging of objects with high-attenuation features in order to reduce limited angle artifacts.
Methods: The algorithm estimates which projections potentially produce artifacts in a voxel. The contribution of those projections into the updating term is reduced. In order to identify those projections automatically, a four-dimensional backprojected space representation is used. Weighting coefﬁcients are calculated based on a dissimilarity measure, evaluated in this space. For each combination of an angular view direction and a voxel position an individual weighting coefﬁcient for the updating term is calculated.
Results: The feasibility of the proposed approach is shown based on reconstructions of the following real three-dimensional tomosynthesis datasets: a mammography quality phantom, an apple with metal needles, a dried ﬁnger bone in water, and a human hand. Datasets have been acquired with a Siemens Mammomat Inspiration tomosynthesis device and reconstructed using SART with and without suggested weighting. Out-of-focus artifacts are described using line proﬁles and measured using standard deviation (STD) in the plane and below the plane which contains artifact-causing features. Artifacts distribution in axial direction is measured using an artifact spread function (ASF). The volumes reconstructed with the weighting scheme demonstrate the reduction of out-of-focus artifacts, lower STD (meaning reduction of artifacts), and narrower ASF compared to nonweighted SART reconstruction. It is achieved successfully for different kinds of structures: point-like structures such as phantom features, long structures such as metal needles, and ﬁne structures such as trabecular bone structures.
Conclusions: Results indicate the feasibility of the proposed algorithm to reduce typical tomosynthesis artifacts produced by high-attenuation features. The proposed algorithm assigns weighting coefﬁcients automatically and no segmentation or tissue-classiﬁcation steps are required. The algorithm can be included into various iterative reconstruction algorithms with an additive updating strategy. It can also be extended to computed tomography case with the complete set of angular data. © 2013 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4789592]},
	language = {en},
	number = {3},
	urldate = {2021-12-13},
	journal = {Medical Physics},
	author = {Levakhina, Y. M. and Müller, J. and Duschka, R. L. and Vogt, F. and Barkhausen, J. and Buzug, T. M.},
	month = feb,
	year = {2013},
	pages = {031106},
	file = {Levakhina et al. - 2013 - Weighted simultaneous algebraic reconstruction tec.pdf:/home/david/Zotero/storage/FFCW9GUQ/Levakhina et al. - 2013 - Weighted simultaneous algebraic reconstruction tec.pdf:application/pdf},
}

@inproceedings{levakhina_two-step_2010,
	title = {Two-step metal artifact reduction using {2D}-{NFFT} and spherically symmetric basis functions},
	doi = {10.1109/NSSMIC.2010.5874424},
	abstract = {In computed tomography metal objects lead to inconsistencies in the acquired data which result in image artifacts after reconstruction. To enhance the image quality and allow for a more accurate diagnosis, a metal artifact reduction is required. Approaches may base on a recomputation of metal influenced values or use an alternative image reconstruction technique, which is less sensitive to inconsistent raw data than the standard filtered backprojection. Here, a two-step algorithm for metal artifact reduction is proposed, which combines a 2D NFFT-based interpolation and an MLEM reconstruction with spherically symmetric basis functions (blobs). Reconstructed images have been evaluated visually and quantitatively. Experiments show that combination of a 2D NFFT-based interpolation and blobs offers an effective strategy for enhanced metal artifacts suppression.},
	booktitle = {{IEEE} {Nuclear} {Science} {Symposuim} {Medical} {Imaging} {Conference}},
	author = {Levakhina, Yulia and Kratz, Baerbel and Buzug, Thorsten M.},
	month = oct,
	year = {2010},
	note = {ISSN: 1082-3654},
	keywords = {Biomedical imaging, Interpolation, Computed tomography, Image reconstruction, Pixel, Image edge detection, Metals},
	pages = {3343--3345},
	file = {Levakhina et al. - 2010 - Two-step metal artifact reduction using 2D-NFFT an.pdf:/home/david/Zotero/storage/34UPBSU5/Levakhina et al. - 2010 - Two-step metal artifact reduction using 2D-NFFT an.pdf:application/pdf},
}

@misc{noauthor_approximation_nodate,
	title = {Approximation of functions},
	url = {http://hplgit.github.io/INF5620/doc/pub/sphinx-fem/._main_fem002.html},
	urldate = {2021-12-13},
	file = {Approximation of functions.html:/home/david/Zotero/storage/JW3A2UCB/Approximation of functions.html:text/html},
}

@book{buzug_computed_2008,
	address = {Berlin, Heidelberg},
	edition = {1},
	title = {Computed {Tomography}: {From} {Photon} {Statistics} to {Modern} {Cone}-{Beam} {CT}},
	url = {http://link.springer.com/10.1007/978-3-540-39408-2},
	abstract = {Tis book provides an overview of X-ray technology, the historic developmental milestones of modern CT systems, and gives a comprehensive insight into the main reconstruction methods used in computed tomography. Te basis of reconstr- tion is, undoubtedly, mathematics. However, the beauty of computed tomography cannot be understood without a detailed knowledge of X-ray generation, photon– matter interaction, X-ray detection, photon statistics, as well as fundamental signal processing concepts and dedicated measurement systems. Terefore, the reader will ?nd a number of references to these basic disciplines together with a brief introd- tion to the underlying principles of CT. Tis book is structured to cover the basics of CT: from photon statistics to m- ern cone-beam systems. However, the main focus of the book is concerned with - tailed derivations of reconstruction algorithms in ?D and modern ?D cone-beam systems. A thorough analysis of CT artifacts and a discussion of practical issues, such as dose considerations, provide further insight into modern CT systems. While mainly written for graduate students in biomedical engineering, medical engine- ing science, medical physics, medicine (radiology), mathematics, electrical eng- eering, and physics, experienced practitioners in these ?elds will bene?t from this book as well.},
	language = {en},
	urldate = {2021-12-13},
	publisher = {Springer Berlin Heidelberg},
	author = {Buzug, Thorsten M.},
	year = {2008},
	doi = {10.1007/978-3-540-39408-2},
	file = {Buzug - 2008 - Computed Tomography From Photon Statistics to Mod.pdf:/home/david/Zotero/storage/XQ3F48P9/Buzug - 2008 - Computed Tomography From Photon Statistics to Mod.pdf:application/pdf},
}

@misc{noauthor_toast_2021,
	title = {Toast++ - {Image} {Reconstruction} in {Optical} {Tomography}},
	copyright = {GPL-3.0},
	url = {https://github.com/toastpp/toastpp},
	abstract = {Toast++: Forward and inverse modelling in optical tomography},
	urldate = {2021-12-13},
	publisher = {Toast++},
	month = dec,
	year = {2021},
	note = {original-date: 2016-07-07T14:30:16Z},
}

@article{vrcej_efficient_2001,
	title = {Efficient implementation of all-digital interpolation},
	volume = {10},
	issn = {10577149},
	url = {http://ieeexplore.ieee.org/document/967392/},
	doi = {10.1109/83.967392},
	abstract = {B-splines are commonly used for continuous representation of discrete time signals. This kind of representation proves to be very useful in applications such as image interpolation, rotation and edge detection. In all these applications, the ﬁrst step is to compute the B-spline coeﬃcients of the signal, and this involves the use of an IIR noncausal ﬁlter called the direct B-spline ﬁlter. The signal reconstruction is achieved using the indirect B-spline ﬁlter, which in many applications operates at a higher rate.},
	language = {en},
	number = {11},
	urldate = {2021-12-14},
	journal = {IEEE Transactions on Image Processing},
	author = {Vrcej, B. and Vaidyanathan, P.P.},
	month = nov,
	year = {2001},
	pages = {1639--1646},
	file = {Vrcej and Vaidyanathan - 2001 - Efficient implementation of all-digital interpolat.pdf:/home/david/Zotero/storage/6RCG935B/Vrcej and Vaidyanathan - 2001 - Efficient implementation of all-digital interpolat.pdf:application/pdf},
}

@article{trampert_spherically_2017,
	title = {Spherically symmetric volume elements as basis functions for image reconstructions in computed laminography},
	abstract = {BACKGROUND: Laminography is a tomographic technique that allows three-dimensional imaging of ﬂat, elongated objects that stretch beyond the extent of a reconstruction volume. Laminography datasets can be reconstructed using iterative algorithms based on the Kaczmarz method.
OBJECTIVE: The goal of this study is to develop a reconstruction algorithm that provides superior reconstruction quality for a challenging class of problems.
METHODS: Images are represented in computer memory using coefﬁcients over basis functions, typically piecewise constant functions (voxels). By replacing voxels with spherically symmetric volume elements (blobs) based on generalized KaiserBessel window functions, we obtained an adapted version of the algebraic reconstruction technique.
RESULTS: Band-limiting properties of blob functions are beneﬁcial particular in the case of noisy projections and if only a limited number of projections is available. In this case, using blob basis functions improved the full-width-at-half-maximum resolution from 10.2 ± 1.0 to 9.9 ± 0.9 (p value = 2.3·10−4). For the same dataset, the signal-to-noise ratio improved from 16.1 to 31.0. The increased computational demand per iteration is compensated for by a faster convergence rate, such that the overall performance is approximately identical for blobs and voxels.
CONCLUSIONS: Despite the higher complexity, tomographic reconstruction from computed laminography data should be implemented using blob basis functions, especially if noisy data is expected.},
	language = {en},
	author = {Trampert, Patrick and Vogelgesang, Jonas and Schorr, Christian and Maisl, Michael and Bogachev, Sviatoslav and Marniok, Nico and Louis, Alfred and Dahmen, Tim and Slusallek, Philipp},
	year = {2017},
	keywords = {blob basis function, Computed laminography, Kaiser-Bessel window, SART, simultaneous algebraic reconstruction technique},
	pages = {14},
	file = {Trampert et al. - Spherically symmetric volume elements as basis fun.pdf:/home/david/Zotero/storage/LRF6XNWR/Trampert et al. - Spherically symmetric volume elements as basis fun.pdf:application/pdf;Full Text:/home/david/Zotero/storage/YVD8FT6N/Trampert et al. - 2017 - Spherically symmetric volume elements as basis fun.pdf:application/pdf},
}

@book{prautzsch_bezier_2002,
	address = {Berlin, Heidelberg},
	series = {Mathematics and {Visualization}},
	title = {Bézier and {B}-{Spline} {Techniques}},
	isbn = {978-3-642-07842-2 978-3-662-04919-8},
	url = {http://link.springer.com/10.1007/978-3-662-04919-8},
	language = {en},
	urldate = {2021-12-14},
	publisher = {Springer Berlin Heidelberg},
	author = {Prautzsch, Hartmut and Boehm, Wolfgang and Paluszny, Marco},
	editor = {Farin, Gerald and Hege, Hans-Christian and Hoffman, David and Johnson, Christopher R. and Polthier, Konrad},
	year = {2002},
	doi = {10.1007/978-3-662-04919-8},
	file = {Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:/home/david/Zotero/storage/ZKMQQT7S/Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:application/pdf;Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:/home/david/Zotero/storage/G56H24QZ/Prautzsch et al. - 2002 - Bézier and B-Spline Techniques.pdf:application/pdf},
}

@article{bilbaocastro_exploiting_2009,
	title = {Exploiting desktop supercomputing for three-dimensional electron microscopy reconstructions using {ART} with blobs},
	volume = {165},
	issn = {10478477},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047847708002323},
	doi = {10.1016/j.jsb.2008.09.009},
	abstract = {Three-dimensional electron microscopy allows direct visualization of biological macromolecules close to their native state. The high impact of this technique in the structural biology ﬁeld is highly correlated with the development of new image processing algorithms. In order to achieve subnanometer resolution, the size and number of images involved in a three-dimensional reconstruction increase and so do computer requirements. New chips integrating multiple processors are hitting the market at a reduced cost. This high-integration, low-cost trend has just begun and is expected to bring real supercomputers to our laboratory desktops in the coming years. This paper proposes a parallel implementation of a computation-intensive algorithm for three-dimensional reconstruction, ART, that takes advantage of the computational power in modern multicore platforms. ART is a sophisticated iterative reconstruction algorithm that has turned out to be well suited for the conditions found in threedimensional electron microscopy. In view of the performance obtained in this work, these modern platforms are expected to play an important role to face the future challenges in three-dimensional electron microscopy.},
	language = {en},
	number = {1},
	urldate = {2021-12-14},
	journal = {Journal of Structural Biology},
	author = {Bilbaocastro, J and Marabini, R and Sorzano, C and Garcia, I and Carazo, J and Fernandez, J},
	month = jan,
	year = {2009},
	pages = {19--26},
	file = {Bilbaocastro et al. - 2009 - Exploiting desktop supercomputing for three-dimens.pdf:/home/david/Zotero/storage/25ZICI85/Bilbaocastro et al. - 2009 - Exploiting desktop supercomputing for three-dimens.pdf:application/pdf},
}

@article{eberly_b-spline_nodate,
	title = {B-{Spline} {Interpolation} on {Lattices}},
	language = {en},
	author = {Eberly, David},
	pages = {41},
	file = {Eberly - B-Spline Interpolation on Lattices.pdf:/home/david/Zotero/storage/7ETA2HKS/Eberly - B-Spline Interpolation on Lattices.pdf:application/pdf},
}

@article{prochazkova_derivative_nodate,
	title = {{DERIVATIVE} {OF} {B}-{SPLINE} {FUNCTION}},
	abstract = {Derivatives are very important in computation in engineering practice on graphics structures. B-spline functions are deﬁned recursive, so direct computation is very diﬃcult. In this article is shown the proof of formula for simpler direct computation of derivatives and its application for derivatives of NURBS curves.},
	language = {en},
	author = {Prochazkova, Jana},
	pages = {6},
	file = {Prochazkova - DERIVATIVE OF B-SPLINE FUNCTION.pdf:/home/david/Zotero/storage/H86WX7IR/Prochazkova - DERIVATIVE OF B-SPLINE FUNCTION.pdf:application/pdf},
}

@article{censor_developments_2021,
	title = {Developments in {Mathematical} {Algorithms} and {Computational} {Tools} for {Proton} {CT} and {Particle} {Therapy} {Treatment} {Planning}},
	issn = {2469-7303},
	doi = {10.1109/TRPMS.2021.3107322},
	abstract = {We summarize recent results and ongoing activities in mathematical algorithms and computer science methods related to proton computed tomography (pCT) and intensity-modulated particle therapy (IMPT) treatment planning. Proton therapy necessitates a high level of delivery accuracy to exploit the selective targeting imparted by the Bragg peak. For this purpose, pCT utilizes the proton beam itself to create images. The technique works by sending a low-intensity beam of protons through the patient and measuring the position, direction, and energy loss of each exiting proton. The pCT technique allows reconstruction of the volumetric distribution of the relative stopping power (RSP) of the patient tissues for use in treatment planning and pre-treatment range verification. We have investigated new ways to make the reconstruction both efficient and accurate. Better accuracy of RSP also enables more robust inverse approaches to IMPT. For IMPT, we developed a framework for performing intensity-modulation of the proton pencil beams. We expect that these developments will lead to additional project work in the years to come, which requires a regular exchange between experts in the fields of mathematics, computer science, and medical physics. We have initiated such an exchange by organizing annual workshops on pCT and IMPT algorithm and technology developments. This report is, admittedly, tilted toward our interdisciplinary work and methods. We offer a comprehensive overview of results, problems, and challenges in pCT and IMPT with the aim of making other scientists wanting to tackle such issues and to strengthen their interdisciplinary collaboration by bringing together cutting-edge know-how from medicine, computer science, physics, and mathematics to bear on medical physics problems at hand.},
	journal = {IEEE Transactions on Radiation and Plasma Medical Sciences},
	author = {Censor, Yair and Schubert, Keith E. and Schulte, Reinhard W.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Radiation and Plasma Medical Sciences},
	keywords = {Image reconstruction, blob basis functions, data partitioning, digital phantoms, intensity-modulated therapy, Inverse problems, Medical treatment, Monte Carlo simulation, motion-adapted reconstruction., Perturbation methods, Planning, Plasmas, proton computed tomography, proton therapy, Protons, superiorization},
	pages = {1--1},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/I45JGSBD/Censor et al. - 2021 - Developments in Mathematical Algorithms and Comput.pdf:application/pdf},
}

@article{horbelt_discretization_2002,
	title = {Discretization of the radon transform and of its inverse by spline convolutions},
	volume = {21},
	issn = {1558-254X},
	doi = {10.1109/TMI.2002.1000260},
	abstract = {We present an explicit formula for B-spline convolution kernels; these are defined as the convolution of several B-splines of variable widths h/sub i/ and degrees n/sub i/. We apply our results to derive spline-convolution-based algorithms for two closely related problems: the computation of the Radon transform and of its inverse. First, we present an efficient discrete implementation of the Radon transform that is optimal in the least-squares sense. We then consider the reverse problem and introduce a new spline-convolution version of the filtered back-projection algorithm for tomographic reconstruction. In both cases, our explicit kernel formula allows for the use of high-degree splines; these offer better approximation performance than the conventional lower-degree formulations (e.g., piecewise constant or piecewise linear models). We present multiple experiments to validate our approach and to find the parameters that give the best tradeoff between image quality and computational complexity. In particular, we find that it can be computationally more efficient to increase the approximation degree than to increase the sampling rate.},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Horbelt, S. and Liebling, M. and Unser, M.},
	month = apr,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Kernel, Spline, Image reconstruction, Image quality, Tomography, Computational complexity, Discrete transforms, Image sampling, Piecewise linear approximation, Piecewise linear techniques},
	pages = {363--376},
	file = {Horbelt et al. - 2002 - Discretization of the radon transform and of its i.pdf:/home/david/Zotero/storage/HUAUNN4V/Horbelt et al. - 2002 - Discretization of the radon transform and of its i.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/HHGPFXCE/1000260.html:text/html},
}

@misc{noauthor_b-spline_nodate,
	title = {B-{Spline} {Convolution}-based {Radon} and {Inverse} {Radon} {Transforms}},
	url = {http://sybil.ece.ucsb.edu/pages/splineradon/splineradon.html},
	urldate = {2021-12-15},
	file = {B-Spline Convolution-based Radon and Inverse Radon.html:/home/david/Zotero/storage/NS55MF6W/B-Spline Convolution-based Radon and Inverse Radon.html:text/html},
}

@book{richter_inverse_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Geosystems} {Mathematics} and {Computing}},
	title = {Inverse {Problems}: {Basics}, {Theory} and {Applications} in {Geophysics}},
	isbn = {978-3-030-59316-2 978-3-030-59317-9},
	shorttitle = {Inverse {Problems}},
	url = {https://link.springer.com/10.1007/978-3-030-59317-9},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Springer International Publishing},
	author = {Richter, Mathias},
	year = {2020},
	doi = {10.1007/978-3-030-59317-9},
	file = {Richter - 2020 - Inverse Problems Basics, Theory and Applications .pdf:/home/david/Zotero/storage/ZBBTFXUR/Richter - 2020 - Inverse Problems Basics, Theory and Applications .pdf:application/pdf},
}

@book{vogel_computational_2002,
	title = {Computational {Methods} for {Inverse} {Problems}},
	isbn = {978-0-89871-550-7 978-0-89871-757-0},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898717570},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Vogel, Curtis R.},
	month = jan,
	year = {2002},
	doi = {10.1137/1.9780898717570},
	file = {Vogel - 2002 - Computational Methods for Inverse Problems - 9 Non.pdf:/home/david/Zotero/storage/NRGYWZBK/Vogel - 2002 - Computational Methods for Inverse Problems - 9 Non.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 8 Tot.pdf:/home/david/Zotero/storage/XSQBCD2M/Vogel - 2002 - Computational Methods for Inverse Problems - 8 Tot.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 6 Par.pdf:/home/david/Zotero/storage/4FZGLZUN/Vogel - 2002 - Computational Methods for Inverse Problems - 6 Par.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 7 Reg.pdf:/home/david/Zotero/storage/2MN6YHPI/Vogel - 2002 - Computational Methods for Inverse Problems - 7 Reg.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 4 Sta.pdf:/home/david/Zotero/storage/LIYLI3NK/Vogel - 2002 - Computational Methods for Inverse Problems - 4 Sta.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 3 Num.pdf:/home/david/Zotero/storage/BTKSSJBY/Vogel - 2002 - Computational Methods for Inverse Problems - 3 Num.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 5 Ima.pdf:/home/david/Zotero/storage/56VE67RI/Vogel - 2002 - Computational Methods for Inverse Problems - 5 Ima.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 2 Ana.pdf:/home/david/Zotero/storage/S5RRT4FC/Vogel - 2002 - Computational Methods for Inverse Problems - 2 Ana.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems - 1 Int.pdf:/home/david/Zotero/storage/T9A4SIW5/Vogel - 2002 - Computational Methods for Inverse Problems - 1 Int.pdf:application/pdf;Vogel - 2002 - Computational Methods for Inverse Problems.pdf:/home/david/Zotero/storage/NL73W2WJ/Vogel - 2002 - Computational Methods for Inverse Problems.pdf:application/pdf},
}

@book{kirsch_introduction_2021,
	address = {Cham},
	series = {Applied {Mathematical} {Sciences}},
	title = {An {Introduction} to the {Mathematical} {Theory} of {Inverse} {Problems}},
	volume = {120},
	isbn = {978-3-030-63342-4 978-3-030-63343-1},
	url = {http://link.springer.com/10.1007/978-3-030-63343-1},
	language = {en},
	urldate = {2021-12-18},
	publisher = {Springer International Publishing},
	author = {Kirsch, Andreas},
	year = {2021},
	doi = {10.1007/978-3-030-63343-1},
	file = {Kirsch - 2021 - An Introduction to the Mathematical Theory of Inve.pdf:/home/david/Zotero/storage/BQR246X7/Kirsch - 2021 - An Introduction to the Mathematical Theory of Inve.pdf:application/pdf},
}

@article{hanson_local_1985,
	title = {Local basis-function approach to computed tomography},
	volume = {24},
	issn = {0003-6935, 1539-4522},
	url = {https://www.osapublishing.org/abstract.cfm?URI=ao-24-23-4028},
	doi = {10.1364/AO.24.004028},
	language = {en},
	number = {23},
	urldate = {2021-12-19},
	journal = {Applied Optics},
	author = {Hanson, Kenneth M. and Wecksung, George W.},
	month = dec,
	year = {1985},
	pages = {4028},
	file = {Hanson and Wecksung - 1985 - Local basis-function approach to computed tomograp.pdf:/home/david/Zotero/storage/S2GK5W4M/Hanson and Wecksung - 1985 - Local basis-function approach to computed tomograp.pdf:application/pdf},
}

@article{unser_splines_1999,
	title = {Splines: a perfect fit for signal and image processing},
	volume = {16},
	issn = {10535888},
	shorttitle = {Splines},
	url = {http://ieeexplore.ieee.org/document/799930/},
	doi = {10.1109/79.799930},
	language = {en},
	number = {6},
	urldate = {2021-12-19},
	journal = {IEEE Signal Processing Magazine},
	author = {Unser, M.},
	month = nov,
	year = {1999},
	pages = {22--38},
	file = {Unser - 1999 - Splines a perfect fit for signal and image proces.pdf:/home/david/Zotero/storage/PPM7DBKS/Unser - 1999 - Splines a perfect fit for signal and image proces.pdf:application/pdf},
}

@incollection{hawkes_brief_2002,
	series = {Advances in {Imaging} and {Electron} {Physics}},
	title = {A {Brief} {Walk} {Through} {Sampling} {Theory}},
	volume = {124},
	url = {https://www.sciencedirect.com/science/article/pii/S1076567002800428},
	publisher = {Elsevier},
	author = {García, Antonio G.},
	editor = {Hawkes, Peter W.},
	year = {2002},
	doi = {https://doi.org/10.1016/S1076-5670(02)80042-8},
	note = {ISSN: 1076-5670},
	pages = {63--137},
}

@article{unser_sampling-50_2000,
	title = {Sampling-50 years after {Shannon}},
	volume = {88},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/843002/},
	doi = {10.1109/5.843002},
	language = {en},
	number = {4},
	urldate = {2021-12-19},
	journal = {Proceedings of the IEEE},
	author = {Unser, M.},
	month = apr,
	year = {2000},
	pages = {569--587},
	file = {Unser - 2000 - Sampling-50 years after Shannon.pdf:/home/david/Zotero/storage/TEZLLYM7/Unser - 2000 - Sampling-50 years after Shannon.pdf:application/pdf},
}

@article{harauz_interpolation_1983,
	title = {Interpolation in computing forward projections in direct three-dimensional reconstruction},
	volume = {28},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/28/12/007},
	doi = {10.1088/0031-9155/28/12/007},
	language = {en},
	number = {12},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {Harauz, G and Ottensmeyer, F P},
	month = dec,
	year = {1983},
	pages = {1419--1427},
	file = {Harauz and Ottensmeyer - 1983 - Interpolation in computing forward projections in .pdf:/home/david/Zotero/storage/67GNPXG6/Harauz and Ottensmeyer - 1983 - Interpolation in computing forward projections in .pdf:application/pdf},
}

@article{peters_algorithms_1981,
	title = {Algorithms for {Fast} {Back}- and {Re}-{Projection} in {Computed} {Tomography}},
	volume = {28},
	issn = {0018-9499},
	url = {http://ieeexplore.ieee.org/document/4331812/},
	doi = {10.1109/TNS.1981.4331812},
	abstract = {While the computation time for reconstructing images in C.T. is not a problem in commercial systems, there are many experimental and developmental applications where resources are limited and image reconstruction places a heavy burden on the computer system. This paper describes a very efficient backI projection algorithm which results in large time savings when implemented in machine code. Also described is a minor modification to this algorithm which converts it to a re-projection procedure with comparable efficiency.},
	language = {en},
	number = {4},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Nuclear Science},
	author = {Peters, T. M.},
	year = {1981},
	pages = {3641--3647},
	file = {Peters - 1981 - Algorithms for Fast Back- and Re-Projection in Com.pdf:/home/david/Zotero/storage/AV4FBMUV/Peters - 1981 - Algorithms for Fast Back- and Re-Projection in Com.pdf:application/pdf},
}

@article{joseph_improved_1982,
	title = {An {Improved} {Algorithm} for {Reprojecting} {Rays} through {Pixel} {Images}},
	volume = {1},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/4307572/},
	doi = {10.1109/TMI.1982.4307572},
	abstract = {It is often desired to calculate line integrals through a field of reconstructed CT density pixels for the purpose of improving CT image quality. Two algorithms widely published and discussed in the past are known to either degrade spatial resolution or generate errors in the results due to the discontinuous "square pixel" modeling of the reconstructed image. An algorithm is described, based on linear interpolation between pixels, which provides superior accuracy without unnecessary loss of resolution. It was tested on simulated data for a head section and on a narrow Gaussian density distribution. The experimental results demonstrated improved performance. The method is expected to prove useful for many types of post-reconstruction processing, including beam hardening, missing data, and noise supression algorithms.},
	language = {en},
	number = {3},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Joseph, Peter M.},
	month = nov,
	year = {1982},
	pages = {192--196},
	file = {Joseph - 1982 - An Improved Algorithm for Reprojecting Rays throug.pdf:/home/david/Zotero/storage/TWI9GP6K/Joseph - 1982 - An Improved Algorithm for Reprojecting Rays throug.pdf:application/pdf},
}

@article{siddon_fast_1985,
	title = {Fast calculation of the exact radiological path for a three-dimensional {CT} array},
	volume = {12},
	issn = {2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1118/1.595715},
	doi = {10.1118/1.595715},
	abstract = {Ready availability has prompted the use of computed tomography (CT) data in various applications in radiation therapy. For example, some radiation treatment planning systems now utilize CT data in heterogeneous dose calculation algorithms. In radiotherapy imaging applications, CT data are projected onto specified planes, thus producing “radiographs,” which are compared with simulator radiographs to assist in proper patient positioning and delineation of target volumes. All these applications share the common geometric problem of evaluating the radiological path through the CT array. Due to the complexity of the three-dimensional geometry and the enormous amount of CT data, the exact evaluation of the radiological path has proven to be a time consuming and difficult problem. This paper identifies the inefficient aspect of the traditional exact evaluation of the radiological path as that of treating the CT data as individual voxels. Rather than individual voxels, a new exact algorithm is presented that considers the CT data as consisting of the intersection volumes of three orthogonal sets of equally spaced, parallel planes. For a three-dimensional CT array of N3 voxels, the new exact algorithm scales with 3N, the number of planes, rather than N3, the number of voxels. Coded in fortran-77 on a VAX 11/780 with a floating point option, the algorithm requires approximately 5 ms to calculate an average radiological path in a 1003 voxel array.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Siddon, Robert L.},
	year = {1985},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1118/1.595715},
	keywords = {Biomedical imaging, Computed tomography, BIOMEDICAL RADIOGRAPHY, COMPUTERIZED TOMOGRAPHY, DIAGNOSTIC TECHNIQUES, Dosimetry, INHOMOGENEITY, Medical imaging, Non-ionizing radiation equipment and techniques, Radiation therapy, Radiation treatment, Radiography, RADIOTHERAPY},
	pages = {252--255},
	file = {Siddon - 1985 - Fast calculation of the exact radiological path fo.pdf:/home/david/Zotero/storage/YD5NEAPA/Siddon - 1985 - Fast calculation of the exact radiological path fo.pdf:application/pdf},
}

@inproceedings{zhao_fast_2004,
	address = {Portland, OR, USA},
	title = {Fast ray-tracing technique to calculate line integral paths in voxel arrays},
	isbn = {978-0-7803-8257-2},
	url = {http://ieeexplore.ieee.org/document/1352469/},
	doi = {10.1109/NSSMIC.2003.1352469},
	abstract = {The ray-driven projection and back-projection methods, frequently represented as calculating the path of line integration through a pixel or voxel space, are widely applied in various imaging research ﬁelds such as positron emission tomography (PET), computerized tomography (CT) and other volumetric ray tracing studies. For high resolution iterative image reconstruction with large amount of sample events, computing the projections event-by-event is an extremely time consuming task. This paper presents a novel ray-tracing calculation technique applying to a list-mode EM image reconstruction algorithm as a part of the system model to accelerate its projection operations. Compared to both the conventional Siddon algorithm and its accelerated methods, the new algorithm has less average computation operation time on each voxel. The results show that the new algorithm signiﬁcantly improves the calculation speed up to twice as fast than the incremental Siddon algorithm.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {2003 {IEEE} {Nuclear} {Science} {Symposium}. {Conference} {Record} ({IEEE} {Cat}. {No}.{03CH37515})},
	publisher = {IEEE},
	author = {Zhao, Huaxia and Reader, A.J.},
	year = {2004},
	pages = {2808--2812},
	file = {Zhao and Reader - 2004 - Fast ray-tracing technique to calculate line integ.pdf:/home/david/Zotero/storage/3NUNEXV8/Zhao and Reader - 2004 - Fast ray-tracing technique to calculate line integ.pdf:application/pdf},
}

@article{gullberg_attenuated_1985,
	title = {An attenuated projector-backprojector for iterative {SPECT} reconstruction},
	volume = {30},
	issn = {0031-9155},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/30/8/004},
	doi = {10.1088/0031-9155/30/8/004},
	abstract = {A new ray-driuen projector-backprojector which can easily be adapted for hardware implementation is described and simulated in software. The projector-backprcjector discretely models the attenuated Radon transform of a source distributed within an attenuating medium as line integrals of discrete pixels, obtained using the standard sampling technique of averaging the emission source or attenuation distribution over small\$square regions. Attenuation factors are calculated for each pixel during the projection and backprojection operations instead of using precalculated values. The calculation of the factors requires a specification of the attenuation distribution, estimated either from an assumed constant distribution and an approximate body outline or from transmission measurements. The distribution of attenuation coefficientsis stored in memory for efficient access during the projection and backprojection operations. The reconstruction of the source distribution is obtained by using a conjugate gradient or SIRTtype iterative algorithm which requires one projection and one backprojection operation for each iteration.},
	language = {en},
	number = {8},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {Gullberg, G T and Huesman, R H and Malko, J A and Pelc, N J and Budinger, T F},
	month = aug,
	year = {1985},
	pages = {799--816},
	file = {Gullberg et al. - 1985 - An attenuated projector-backprojector for iterativ.pdf:/home/david/Zotero/storage/DSMCP9TB/Gullberg et al. - 1985 - An attenuated projector-backprojector for iterativ.pdf:application/pdf},
}

@article{christiaens_fast_1999,
	title = {A fast, cache-aware algorithm for the calculation of radiological paths exploiting subword parallelism},
	volume = {45},
	issn = {13837621},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1383762198000381},
	doi = {10.1016/S1383-7621(98)00038-1},
	language = {en},
	number = {10},
	urldate = {2021-12-19},
	journal = {Journal of Systems Architecture},
	author = {Christiaens, Mark and De Sutter, Bjorn and De Bosschere, Koen and Van Campenhout, Jan and Lemahieu, Ignace},
	month = apr,
	year = {1999},
	pages = {781--790},
	file = {Christiaens et al. - 1999 - A fast, cache-aware algorithm for the calculation .pdf:/home/david/Zotero/storage/8RB4RBWU/Christiaens et al. - 1999 - A fast, cache-aware algorithm for the calculation .pdf:application/pdf},
}

@article{de_man_distance-driven_2004,
	title = {Distance-driven projection and backprojection in three dimensions},
	volume = {49},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/49/11/024},
	doi = {10.1088/0031-9155/49/11/024},
	abstract = {Projection and backprojection are operations that arise frequently in tomographic imaging. Recently, we proposed a new method for projection and backprojection, which we call distance-driven, and that offers low arithmetic cost and a highly sequential memory access pattern. Furthermore, distance-driven projection and backprojection avoid several artefact-inducing approximations characteristic of some other methods. We have previously demonstrated the application of this method to parallel and fan beam geometries. In this paper, we extend the distance-driven framework to three dimensions and demonstrate its application to cone beam reconstruction. We also present experimental results to demonstrate the computational performance, the artefact characteristics and the noise-resolution characteristics of the distance-driven method in three dimensions.},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {Physics in Medicine and Biology},
	author = {De Man, B. and Basu, S.},
	month = jun,
	year = {2004},
	pages = {2463--2475},
	file = {De Man and Basu - 2004 - Distance-driven projection and backprojection in t.pdf:/home/david/Zotero/storage/NVBE3FUL/De Man and Basu - 2004 - Distance-driven projection and backprojection in t.pdf:application/pdf},
}

@inproceedings{de_man_distance-driven_2002,
	title = {Distance-driven projection and backprojection},
	volume = {3},
	doi = {10.1109/NSSMIC.2002.1239600},
	abstract = {Projection and backprojection are important processes in computed tomography (CT). They are used in iterative reconstruction, simulation, and artifact correction, as well as routine (filtered-backprojection based) reconstruction. Existing methods either have poor performance or result in artifacts. A new method for projecting and backprojecting rays through pixels is presented that has good performance and eliminates artifacts, and could potentially enable routine iterative reconstruction in clinical CT systems. The new method, which we call distance-driven, reconciles the advantages of the common pixel-driven and ray-driven methods. It avoids an artifact-inducing approximation made by the previous methods and has very favorable computational properties. The method is applicable to parallel-beam, fanbeam, and cone-beam geometries. Its performance and artifact behavior are evaluated on a two-dimensional fan-beam geometry with flat detector and compared to the pixel-driven and ray-driven approaches. The distance-driven method prevents the artifacts that are generated in the pixel-driven projection and in the ray-driven backprojection. It outperforms pixel-driven and ray-driven methods from a computational standpoint and is amenable to hardware implementation.},
	booktitle = {2002 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	author = {De Man, B. and Basu, S.},
	month = nov,
	year = {2002},
	keywords = {Computed tomography, Detectors, Image reconstruction, Computational modeling, Geometry, Hardware, Image converters, Image generation, Iterative methods, X-ray imaging},
	pages = {1477--1480 vol.3},
	file = {De Man and Basu - 2002 - Distance-driven projection and backprojection.pdf:/home/david/Zotero/storage/YFYHSSJK/De Man and Basu - 2002 - Distance-driven projection and backprojection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/2GMTKNYD/1239600.html:text/html},
}

@article{liu_gpu-based_2017,
	title = {{GPU}-{Based} {Branchless} {Distance}-{Driven} {Projection} and {Backprojection}},
	volume = {3},
	issn = {2333-9403},
	doi = {10.1109/TCI.2017.2675705},
	abstract = {Projection and backprojection operations are essential in a variety of image reconstruction and physical correction algorithms in computed tomography (CT). The distance-driven (DD) projection and backprojection are widely used for their favorable image quality properties, highly sequential memory access pattern and low arithmetic cost. However, a typical DD implementation has an inner loop that adjusts the calculation depending on the relative position between voxel and detector cell boundaries. The irregularity of the branch behavior makes it inefficient to be implemented on massively parallel computing devices, such as graphics processing units (GPUs). Such irregular branch behaviors can be eliminated by factorizing the DD operation as three branchless steps: integration, linear interpolation, and differentiation, all of which are highly amenable to massive vectorization. In this paper, we implement and evaluate a highly parallel branchless DD algorithm for three-dimensional cone beam CT. The algorithm utilizes the texture memory and hardware interpolation on GPUs to achieve fast computational speed. The developed branchless DD algorithm achieved 137-fold speedup for forward projection and 188-fold speedup for backprojection relative to a single-thread CPU implementation. Compared with a state-of-the-art 32-thread CPU implementation, the proposed branchless DD achieved eight-fold acceleration for forward projection and ten-fold acceleration for backprojection. The GPU-based branchless DD method was evaluated by iterative reconstruction algorithms with both simulation and real datasets. It obtained visually identical images as the CPU reference algorithm.},
	number = {4},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Liu, Rui and Fu, Lin and De Man, Bruno and Yu, Hengyong},
	month = dec,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Computational Imaging},
	keywords = {Computed tomography, Detectors, Image reconstruction, Computational modeling, Algorithm design and analysis, backprojection, Branchless distance-driven, computed tomography, GPU, Graphics processing units, projection, reconstruction},
	pages = {617--632},
	file = {Liu et al. - 2017 - GPU-Based Branchless Distance-Driven Projection an.pdf:/home/david/Zotero/storage/2SUS8GEE/Liu et al. - 2017 - GPU-Based Branchless Distance-Driven Projection an.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GGRE8SD7/7866886.html:text/html},
}

@article{long_3d_2010,
	title = {{3D} {Forward} and {Back}-{Projection} for {X}-{Ray} {CT} {Using} {Separable} {Footprints}},
	volume = {29},
	issn = {1558-254X},
	doi = {10.1109/TMI.2010.2050898},
	abstract = {Iterative methods for 3D image reconstruction have the potential to improve image quality over conventional filtered back projection (FBP) in X-ray computed tomography (CT). However, the computation burden of 3D cone-beam forward and back-projectors is one of the greatest challenges facing practical adoption of iterative methods for X-ray CT. Moreover, projector accuracy is also important for iterative methods. This paper describes two new separable footprint (SF) projector methods that approximate the voxel footprint functions as 2D separable functions. Because of the separability of these footprint functions, calculating their integrals over a detector cell is greatly simplified and can be implemented efficiently. The SF-TR projector uses trapezoid functions in the transaxial direction and rectangular functions in the axial direction, whereas the SF-TT projector uses trapezoid functions in both directions. Simulations and experiments showed that both SF projector methods are more accurate than the distance-driven (DD) projector, which is a current state-of-the-art method in the field. The SF-TT projector is more accurate than the SF-TR projector for rays associated with large cone angles. The SF-TR projector has similar computation speed with the DD projector and the SF-TT projector is about two times slower.},
	number = {11},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Long, Yong and Fessler, Jeffrey A. and Balter, James M.},
	month = nov,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Computed tomography, Detectors, Image reconstruction, Image quality, Iterative methods, Argon, Cone-beam tomography, forward and back-projection, iterative tomographic image reconstruction, Permission, Postal services, Reconstruction algorithms, USA Councils},
	pages = {1839--1850},
	file = {Long et al. - 2010 - 3D Forward and Back-Projection for X-Ray CT Using .pdf:/home/david/Zotero/storage/KZKMICNW/Long et al. - 2010 - 3D Forward and Back-Projection for X-Ray CT Using .pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/4BP54L79/5482021.html:text/html},
}

@article{long_3d_2010-1,
	title = {{3D} {Forward} and {Back}-{Projection} for {X}-{Ray} {CT} {Using} {Separable} {Footprints} with {Trapezoid} {Functions}},
	abstract = {The greatest impediment to practical adoption of iterative methods for X-ray CT is the computation burden of cone-beam forward and back-projectors. Moreover, forward and back-projector accuracy is also crucial to iterative reconstruction methods. We previously described a computationally efﬁcient projector that approximates the voxel footprint functions by the 2D separable products of trapezoid functions in the transaxial plane and rectangular functions in the axial direction [1], [2]. The separability of these footprint functions simpliﬁes calculating their integrals over rectangular detector cells. We showed that this separable footprint (SF-TR) method was more accurate than the distance-driven (DD) method but with comparable computation time. This paper describes a new extension of that projector, called the SF-TT projector, that uses trapezoid functions in both directions. We show that using a trapezoid along the axial direction improves projector accuracy for voxels associated with larger cone angles. However, this improved accuracy requires increased computation compared to the rectangular approximation. Having both options available facilitates evaluation of the trade offs between accuracy and computation for different cone-beam geometries.},
	language = {en},
	author = {Long, Yong and Fessler, Jeffrey A},
	year = {2010},
	pages = {4},
	file = {Long and Fessler - 3D Forward and Back-Projection for X-Ray CT Using .pdf:/home/david/Zotero/storage/EB74DSN6/Long and Fessler - 3D Forward and Back-Projection for X-Ray CT Using .pdf:application/pdf},
}

@article{ziegler_efficient_2006,
	title = {Efficient projection and backprojection scheme for spherically symmetric basis functions in divergent beam geometry: {Efficient} projection and backprojection scheme},
	volume = {33},
	issn = {00942405},
	shorttitle = {Efficient projection and backprojection scheme for spherically symmetric basis functions in divergent beam geometry},
	url = {http://doi.wiley.com/10.1118/1.2388570},
	doi = {10.1118/1.2388570},
	language = {en},
	number = {12},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Ziegler, Andy and Köhler, Thomas and Nielsen, Tim and Proksa, Roland},
	month = nov,
	year = {2006},
	pages = {4653--4663},
	file = {Ziegler et al. - 2006 - Efficient projection and backprojection scheme for.pdf:/home/david/Zotero/storage/9YQV7PGE/Ziegler et al. - 2006 - Efficient projection and backprojection scheme for.pdf:application/pdf},
}

@article{jacobs_fast_1998,
	title = {A {Fast} {Algorithm} to {Calculate} the {Exact} {Radiological} {Path} through a {Pixel} or {Voxel} {Space}},
	volume = {6},
	abstract = {Calculating the exact radiological path through a pixel or voxel space is a frequently encountered problem in medical image reconstruction from projections and greatly in uences the reconstruction time. Currently, one of the fastest algorithms designed for this purpose was published in 1985 by Robert L. Siddon 1]. In this paper, we propose an improved version of Siddon's algorithm, resulting in a considerable speedup.},
	language = {en},
	number = {1},
	journal = {Journal of computing and information technology},
	author = {Jacobs, Filip and Sundermann, Erik and Sutter, Bjorn De and Christiaens, Mark and Lemahieu, Ignace},
	year = {1998},
	pages = {14},
	file = {Jacobs et al. - 1998 - A Fast Algorithm to Calculate the Exact Radiologic.pdf:/home/david/Zotero/storage/DKCJQJTR/Jacobs et al. - 1998 - A Fast Algorithm to Calculate the Exact Radiologic.pdf:application/pdf},
}

@article{gao_fast_2012,
	title = {Fast parallel algorithms for the x-ray transform and its adjoint: {Fast} parallel x-ray transform and its adjoint},
	volume = {39},
	issn = {00942405},
	shorttitle = {Fast parallel algorithms for the x-ray transform and its adjoint},
	url = {http://doi.wiley.com/10.1118/1.4761867},
	doi = {10.1118/1.4761867},
	abstract = {Purpose: Iterative reconstruction methods often offer better imaging quality and allow for reconstructions with lower imaging dose than classical methods in computed tomography. However, the computational speed is a major concern for these iterative methods, for which the x-ray transform and its adjoint are two most time-consuming components. The speed issue becomes even notable for the 3D imaging such as cone beam scans or helical scans, since the x-ray transform and its adjoint are frequently computed as there is usually not enough computer memory to save the corresponding system matrix. The purpose of this paper is to optimize the algorithm for computing the x-ray transform and its adjoint, and their parallel computation.
Methods: The fast and highly parallelizable algorithms for the x-ray transform and its adjoint are proposed for the inﬁnitely narrow beam in both 2D and 3D. The extension of these fast algorithms to the ﬁnite-size beam is proposed in 2D and discussed in 3D.
Results: The CPU and GPU codes are available at https://sites.google.com/site/fastxraytransform. The proposed algorithm is faster than Siddon’s algorithm for computing the x-ray transform. In particular, the improvement for the parallel computation can be an order of magnitude.
Conclusions: The authors have proposed fast and highly parallelizable algorithms for the x-ray transform and its adjoint, which are extendable for the ﬁnite-size beam. The proposed algorithms are suitable for parallel computing in the sense that the computational cost per parallel thread is O(1). © 2012 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4761867]},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Gao, Hao},
	month = nov,
	year = {2012},
	pages = {7110--7120},
	file = {Gao - 2012 - Fast parallel algorithms for the x-ray transform a.pdf:/home/david/Zotero/storage/N5RT6RMA/Gao - 2012 - Fast parallel algorithms for the x-ray transform a.pdf:application/pdf},
}

@inproceedings{wu_gpu_2011,
	title = {{GPU} acceleration of {3D} forward and backward projection using separable footprints for {X}-ray {CT} image reconstruction},
	volume = {6},
	booktitle = {Proc. {Intl}. {Mtg}. on {Fully} {3D} {Image} {Recon}. in {Rad}. and {Nuc}. {Med}},
	publisher = {Citeseer},
	author = {Wu, Meng and Fessler, Jeffrey A},
	year = {2011},
	pages = {021911},
	file = {Wu and Fessler - 2011 - GPU acceleration of 3D forward and backward projec.pdf:/home/david/Zotero/storage/LQHXSY2T/Wu and Fessler - 2011 - GPU acceleration of 3D forward and backward projec.pdf:application/pdf},
}

@book{carpio_inverse_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Mathematics}},
	title = {Inverse {Problems} and {Imaging}},
	volume = {1943},
	url = {http://link.springer.com/10.1007/978-3-540-78547-7},
	language = {en},
	urldate = {2021-12-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Carpio, Ana and Dorn, Oliver and Moscoso, Miguel and Natterer, Frank and Papanicolaou, George C. and Rapún, Maria Luisa and Teta, Alessandro},
	editor = {Bonilla, Luis L. and Morel, J. -M. and Takens, F. and Teissier, B.},
	year = {2008},
	doi = {10.1007/978-3-540-78547-7},
	file = {Carpio et al. - 2008 - Inverse Problems and Imaging.pdf:/home/david/Zotero/storage/WVVKLV8P/Carpio et al. - 2008 - Inverse Problems and Imaging.pdf:application/pdf},
}

@article{kaipio_statistical_2007,
	title = {Statistical inverse problems: {Discretization}, model reduction and inverse crimes},
	volume = {198},
	issn = {03770427},
	shorttitle = {Statistical inverse problems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377042705007296},
	doi = {10.1016/j.cam.2005.09.027},
	abstract = {The article discusses the discretization of linear inverse problems. When an inverse problem is formulated in terms of inﬁnitedimensional function spaces and then discretized for computational purposes, a discretization error appears. Since inverse problems are typically ill-posed, neglecting this error may have serious consequences to the quality of the reconstruction. The Bayesian paradigm provides tools to estimate the statistics of the discretization error that is made part of the measurement and modelling errors of the estimation problem. This approach also provides tools to reduce the dimensionality of inverse problems in a controlled manner. The ideas are demonstrated with a computed example.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Kaipio, Jari and Somersalo, Erkki},
	month = jan,
	year = {2007},
	pages = {493--504},
	file = {Kaipio and Somersalo - 2007 - Statistical inverse problems Discretization, mode.pdf:/home/david/Zotero/storage/3ZV3QANY/Kaipio and Somersalo - 2007 - Statistical inverse problems Discretization, mode.pdf:application/pdf},
}

@book{kaipio_statistical_2005,
	address = {New York},
	series = {Applied mathematical sciences},
	title = {Statistical and computational inverse problems},
	isbn = {978-0-387-22073-4},
	language = {en},
	number = {v. 160},
	publisher = {Springer},
	author = {Kaipio, Jari and Somersalo, Erkki},
	year = {2005},
	keywords = {Inverse problems (Differential equations), Numerical solutions},
	file = {Kaipio and Somersalo - 2005 - Statistical and computational inverse problems.pdf:/home/david/Zotero/storage/U5SPINME/Kaipio and Somersalo - 2005 - Statistical and computational inverse problems.pdf:application/pdf},
}

@article{wirgin_inverse_2004,
	title = {The inverse crime},
	url = {http://arxiv.org/abs/math-ph/0401050},
	abstract = {The inverse crime occurs when the same (or very nearly the same) theoretical ingredients are employed to synthesize as well as to invert data in an inverse problem. This act has been qualified as trivial and therefore to be avoided by Colton and Kress.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:math-ph/0401050},
	author = {Wirgin, Armand},
	month = jan,
	year = {2004},
	note = {arXiv: math-ph/0401050},
	keywords = {Mathematical Physics},
	file = {Wirgin - 2004 - The inverse crime.pdf:/home/david/Zotero/storage/EV2FD3WY/Wirgin - 2004 - The inverse crime.pdf:application/pdf},
}

@article{savanier_magnificationdriven_2021,
	title = {Magnification‐driven {B}‐spline interpolation for cone‐beam projection and backprojection},
	volume = {48},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/mp.15179},
	doi = {10.1002/mp.15179},
	abstract = {Purpose: Discretizing tomographic forward and backward operations is a crucial step in the design of model-based reconstruction algorithms. Standard projectors rely on linear interpolation, whose adjoint introduces discretization errors during backprojection. More advanced techniques are obtained through geometric footprint models that may present a high computational cost and an inner logic that is not suitable for implementation on massively parallel computing architectures. In this work, we take a fresh look at the discretization of resampling transforms and focus on the issue of magniﬁcation-induced local sampling variations by introducing a new magniﬁcation-driven interpolation approach for tomography.
Methods: Starting from the existing literature on spline interpolation for magniﬁcation purposes, we provide a mathematical formulation for discretizing a one-dimensional homography. We then extend our approach to two-dimensional representations in order to account for the geometry of cone-beam computed tomography with a ﬂat panel detector. Our new method relies on the decomposition of signals onto a space generated by nonuniform B-splines so as to capture the spatially varying magniﬁcation that locally affects sampling. We propose various degrees of approximations for a rapid implementation of the proposed approach. Our framework allows us to deﬁne a novel family of projector/backprojector pairs parameterized by the order of the employed B-splines. The state-of -the-art distance-driven interpolation appears to ﬁt into this family thus providing new insight and computational layout for this scheme. The question of data resampling at the detector level is handled and integrated with reconstruction in a single framework.
Results: Results on both synthetic data and real data using a quality assurance phantom, were performed to validate our approach. We show experimentally that our approximate implementations are associated with reduced complexity while achieving a near-optimal performance. In contrast with linear interpolation, B-splines guarantee full usage of all data samples, and thus the X-ray dose, leading to more uniform noise properties. In addition, higher-order B-splines allow analytical and iterative reconstruction to reach higher resolution. These beneﬁts appear more signiﬁcant when downsampling frames acquired by X-ray ﬂat-panel detectors with small pixels.
Conclusions: Magniﬁcation-driven B-spline interpolation is shown to provide high-accuracy projection operators with good-quality adjoints for iterative reconstruction. It equally applies to backprojection for analytical reconstruction and detector data downsampling.},
	language = {en},
	number = {10},
	urldate = {2021-12-19},
	journal = {Medical Physics},
	author = {Savanier, Marion and Riddell, Cyril and Trousset, Yves and Chouzenoux, Emilie and Pesquet, Jean‐Christophe},
	month = oct,
	year = {2021},
	pages = {6339--6361},
	file = {Savanier et al. - 2021 - Magnification‐driven B‐spline interpolation for co.pdf:/home/david/Zotero/storage/2PJDR8ND/Savanier et al. - 2021 - Magnification‐driven B‐spline interpolation for co.pdf:application/pdf},
}

@inproceedings{fehringer_versatile_2014,
	title = {A versatile tomographic forward- and backprojection approach on {Multi}-{GPUs}},
	doi = {10.1117/12.2043860},
	author = {Fehringer, Andreas and Lasser, Tobias and Zanette, Irene and Noël, Peter and Pfeiffer, Franz},
	month = mar,
	year = {2014},
	pages = {90344F},
	file = {Fehringer et al. - 2014 - A versatile tomographic forward- and backprojectio.pdf:/home/david/Zotero/storage/ZZLAIZKE/Fehringer et al. - 2014 - A versatile tomographic forward- and backprojectio.pdf:application/pdf},
}

@article{fehringer_real-time_nodate,
	title = {Real-time iterative reconstruction for x-ray computed tomography},
	language = {en},
	author = {Fehringer, Andreas},
	pages = {150},
	file = {Fehringer - Real-time iterative reconstruction for x-ray compu.pdf:/home/david/Zotero/storage/XJX6P8FE/Fehringer - Real-time iterative reconstruction for x-ray compu.pdf:application/pdf},
}

@article{xie_effective_2015,
	title = {An {Effective} {CUDA} {Parallelization} of {Projection} in {Iterative} {Tomography} {Reconstruction}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0142184},
	doi = {10.1371/journal.pone.0142184},
	abstract = {Projection and back-projection are the most computationally intensive parts in Computed Tomography (CT) reconstruction, and are essential to acceleration of CT reconstruction algorithms. Compared to back-projection, parallelization efficiency in projection is highly limited by racing condition and thread unsynchronization. In this paper, a strategy of Fixed Sampling Number Projection (FSNP) is proposed to ensure the operation synchronization in the ray-driven projection with Graphical Processing Unit (GPU). Texture fetching is also used utilized to further accelerate the interpolations in both projection and back-projection. We validate the performance of this FSNP approach using both simulated and real conebeam CT data. Experimental results show that compare to the conventional approach, the proposed FSNP method together with texture fetching is 10{\textasciitilde}16 times faster than the conventional approach based on global memory, and thus leads to more efficient iterative algorithm in CT reconstruction.},
	language = {en},
	number = {11},
	urldate = {2021-12-19},
	journal = {PLOS ONE},
	author = {Xie, Lizhe and Hu, Yining and Yan, Bin and Wang, Lin and Yang, Benqiang and Liu, Wenyuan and Zhang, Libo and Luo, Limin and Shu, Huazhong and Chen, Yang},
	editor = {Zhang, Qinghui},
	month = nov,
	year = {2015},
	pages = {e0142184},
	file = {Xie et al. - 2015 - An Effective CUDA Parallelization of Projection in.pdf:/home/david/Zotero/storage/YC2N49UR/Xie et al. - 2015 - An Effective CUDA Parallelization of Projection in.pdf:application/pdf},
}

@article{graetz_high_2020,
	title = {High performance volume ray casting: {A} branchless generalized {Joseph} projector},
	shorttitle = {High performance volume ray casting},
	url = {http://arxiv.org/abs/1609.00958},
	abstract = {A concise and highly performant branchless formulation of a Joseph-type interpolating ray-casting algorithm for the computation of X-ray projections is presented. It efﬁciently utilizes the hardware resources of modern graphics processing units at the scale of their theoretic maximum performance reaching access rates of 600 GB/s within read-and-write memory, and is further shown to do so without compromising on image quality. The computation of X-ray projections from discrete voxel grids is an ubiquitous task in many problems related to volume image processing, including tomographic reconstruction and visualization. Although its central role has given rise to numerous publications discussing the optimal modeling of rayvolume intersections, a unique benchmark in this respect does not exist. Here, a 3D Shepp-Logan phantom is used, which allows the computation of analytic reference projections that can further serve as input to iterative reconstructions without committing the inverse crime. The proposed algorithm (GJP) is compared to the competing and widely adopted digital differential analyzer (DDA), which computes exact line-box intersections. It is thereby found to outperform the DDA on recent graphics processors in all respects: Despite accessing twice as much memory, the GJP is still able to calculate projections twice as fast. It further exhibits considerably less discretization artifacts, and neither oversampling of the DDA nor a smooth interpolation kernel within the GJP are able to improve on these results in any respect.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1609.00958 [physics]},
	author = {Graetz, Jonas},
	month = aug,
	year = {2020},
	note = {arXiv: 1609.00958},
	keywords = {Physics - Medical Physics, Computer Science - Graphics},
	file = {Graetz - 2020 - High performance volume ray casting A branchless .pdf:/home/david/Zotero/storage/L4G45GGK/Graetz - 2020 - High performance volume ray casting A branchless .pdf:application/pdf},
}

@inproceedings{flohr_accelerating_2017,
	address = {Orlando, Florida, United States},
	title = {Accelerating separable footprint ({SF}) forward and back projection on {GPU}},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2252010},
	doi = {10.1117/12.2252010},
	abstract = {Statistical image reconstruction (SIR) methods for X-ray CT can improve image quality and reduce radiation dosages over conventional reconstruction methods, such as ﬁltered back projection (FBP). However, SIR methods require much longer computation time. The separable footprint (SF) forward and back projection technique simpliﬁes the calculation of intersecting volumes of image voxels and ﬁnite-size beams in a way that is both accurate and eﬃcient for parallel implementation. We propose a new method to accelerate the SF forward and back projection on GPU with NVIDIA’s CUDA environment. For the forward projection, we parallelize over all detector cells. For the back projection, we parallelize over all 3D image voxels. The simulation results show that the proposed method is faster than the acceleration method of the SF projectors proposed by Wu and Fessler.13 We further accelerate the proposed method using multiple GPUs. The results show that the computation time is reduced approximately proportional to the number of GPUs.},
	language = {en},
	urldate = {2021-12-19},
	author = {Xie, Xiaobin and McGaffin, Madison G. and Long, Yong and Fessler, Jeffrey A. and Wen, Minhua and Lin, James},
	editor = {Flohr, Thomas G. and Lo, Joseph Y. and Gilat Schmidt, Taly},
	month = mar,
	year = {2017},
	pages = {101322S},
	file = {Xie et al. - 2017 - Accelerating separable footprint (SF) forward and .pdf:/home/david/Zotero/storage/FV6ULIGW/Xie et al. - 2017 - Accelerating separable footprint (SF) forward and .pdf:application/pdf},
}

@inproceedings{ha_efficient_2016,
	address = {Bamberg},
	title = {Efficient {Area}-{Based} {Ray} {Integration} {Using} {Summed} {Area} {Tables} and {Regression} {Models}},
	abstract = {The increasing popularity of iterative reconstruction algorithms has raised the attention onto how to build more accurate, realistic CT system models. In our work, we model the CT projectors based on volume integrals. The higher computational complexity in computing the exact volume integration is hidden by memory-efficient, fast, and accurate look-up tables. For further reductions we also derive a simple linear regression model from the table. We demonstrate our ideas with data obtained with a fan-beam flat-detector CT system. We observe speed-ups of up to 30\% while keeping a higher or at least similar image quality than existing advanced CT system models.},
	language = {en},
	author = {Ha, Sungsoo and Li, Heyi and Mueller, Klaus},
	year = {2016},
	pages = {4},
	file = {Ha et al. - 2016 - Efficient Area-Based Ray Integration Using Summed .pdf:/home/david/Zotero/storage/HS9Y4JLD/Ha et al. - 2016 - Efficient Area-Based Ray Integration Using Summed .pdf:application/pdf},
}

@article{ha_look-up_2018,
	title = {A {Look}-{Up} {Table}-{Based} {Ray} {Integration} {Framework} for 2-{D}/3-{D} {Forward} and {Back} {Projection} in {X}-{Ray} {CT}},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/8013154/},
	doi = {10.1109/TMI.2017.2741781},
	abstract = {Iterative algorithms have become increasingly popular in Computed Tomography (CT) image reconstruction since they better deal with the adverse image artifacts arising from low radiation dose image acquisition. But iterative methods remain computationally expensive. The main cost emerges in the projection and backprojection operations where accurate CT system modeling can greatly improve the quality of the reconstructed image. We present a framework that improves upon one particular aspect – the accurate projection of the image basis functions. It differs from current methods in that it substitutes the high computational complexity associated with accurate voxel projection by a small number of memory operations. Coefﬁcients are computed in advance and stored in look-up tables parameterized by the CT system’s projection geometry. The look-up tables only require a few kilobytes of storage and can be efﬁciently accelerated on the GPU. We demonstrate our framework with both numerical and clinical experiments and compare its performance with the current state of the art scheme – the separable footprint method.},
	language = {en},
	number = {2},
	urldate = {2021-12-19},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ha, Sungsoo and Mueller, Klaus},
	month = feb,
	year = {2018},
	pages = {361--371},
	file = {Ha and Mueller - 2018 - A Look-Up Table-Based Ray Integration Framework fo.pdf:/home/david/Zotero/storage/GCDF7THL/Ha and Mueller - 2018 - A Look-Up Table-Based Ray Integration Framework fo.pdf:application/pdf},
}

@inproceedings{ha_study_2015,
	address = {Newport},
	title = {A {Study} of {Volume} {Integration} {Models} for {Iterative} {Cone}-{Beam} {Computed} {Tomography}},
	abstract = {With the help of modern parallel computers, iterative reconstruction algorithms have become a feasible research topic in the field of CT. These types of algorithms can greatly benefit from an accurate, realistic CT system model. In our study, we model the CT projection as volume integrals and propose a set of methods that can compute the volume integrals either exactly or approximately. Our approximate volume integral methods have a much smaller complexity algorithmically than the exact method, but their accuracy is close to it. More importantly, the proposed approximate methods can be easily ported to modern parallel processors to utilize their massive computation powers.},
	language = {en},
	author = {Ha, Sungsoo and Kumar, Ayush and Mueller, Klaus},
	year = {2015},
	pages = {4},
	file = {Ha et al. - 2015 - A Study of Volume Integration Models for Iterative.pdf:/home/david/Zotero/storage/2GD9XFFT/Ha et al. - 2015 - A Study of Volume Integration Models for Iterative.pdf:application/pdf},
}

@inproceedings{shewchuk_introduction_1994,
	title = {An {Introduction} to the {Conjugate} {Gradient} {Method} {Without} the {Agonizing} {Pain}},
	url = {https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf},
	urldate = {2021-12-23},
	publisher = {Carnegie-Mellon University. Department of Computer Scienc},
	author = {Shewchuk, Jonathan Richard},
	year = {1994},
	file = {Shewchuk - 1994 - An Introduction to the Conjugate Gradient Method W.pdf:/home/david/Zotero/storage/TUAWAFB2/Shewchuk - 1994 - An Introduction to the Conjugate Gradient Method W.pdf:application/pdf},
}

@article{clason_regularization_2021,
	title = {Regularization of {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2001.00617},
	abstract = {These lecture notes for a graduate class present the regularization theory for linear and nonlinear ill-posed operator equations in Hilbert spaces. Covered are the general framework of regularization methods and their analysis via spectral filters as well as the concrete examples of Tikhonov regularization, Landweber iteration, regularization by discretization for linear inverse problems. In the nonlinear setting, Tikhonov regularization and iterative regularization (Landweber, Levenberg-Marquardt, and iteratively regularized Gau\{{\textbackslash}ss\}-Newton methods) are discussed. The necessary background from functional analysis is also briefly summarized. The notes end with a brief outlook to statistical inverse problems from both a frequentist and a Bayesian point of view.},
	language = {en},
	urldate = {2021-12-27},
	journal = {arXiv:2001.00617 [cs, math]},
	author = {Clason, Christian},
	month = feb,
	year = {2021},
	note = {arXiv: 2001.00617},
	keywords = {Mathematics - Functional Analysis, Mathematics - Numerical Analysis},
	file = {Clason - 2021 - Regularization of Inverse Problems.pdf:/home/david/Zotero/storage/PGBIJLFG/Clason - 2021 - Regularization of Inverse Problems.pdf:application/pdf},
}

@article{tibshirani_lasso_2013,
	title = {The lasso problem and uniqueness},
	volume = {7},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/The-lasso-problem-and-uniqueness/10.1214/13-EJS815.full},
	doi = {10.1214/13-EJS815},
	abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p {\textgreater} n, the lasso criterion is not strictly convex, and hence it may not have a unique minimizer. An important question is: when is the lasso solution well-deﬁned (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to 1 penalized minimization problems over a wide range of loss functions.},
	language = {en},
	number = {none},
	urldate = {2021-12-27},
	journal = {Electronic Journal of Statistics},
	author = {Tibshirani, Ryan J.},
	month = jan,
	year = {2013},
	file = {Tibshirani - 2013 - The lasso problem and uniqueness.pdf:/home/david/Zotero/storage/EEJWPKG5/Tibshirani - 2013 - The lasso problem and uniqueness.pdf:application/pdf},
}

@article{tao_local_2015,
	title = {Local {Linear} {Convergence} of {ISTA} and {FISTA} on the {LASSO} {Problem}},
	url = {http://arxiv.org/abs/1501.02888},
	abstract = {We establish local linear convergence bounds for the ISTA and FISTA iterations on the model LASSO problem. We show that FISTA can be viewed as an accelerated ISTA process. Using a spectral analysis, we show that, when close enough to the solution, both iterations converge linearly, but FISTA slows down compared to ISTA, making it advantageous to switch to ISTA toward the end of the iteration processs. We illustrate the results with some synthetic numerical examples.},
	language = {en},
	urldate = {2021-12-27},
	journal = {arXiv:1501.02888 [math]},
	author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02888},
	keywords = {Mathematics - Optimization and Control},
	file = {Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:/home/david/Zotero/storage/DEREK838/Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:application/pdf},
}

@article{urimi_image_nodate,
	title = {{IMAGE} {RECONSTRUCTION} {TECHNIQUES} and {MEASURE} {OF} {QUALITY}: {CLASSICAL} vs. {MODERN} {APPROACHES}},
	language = {en},
	author = {Urimi, Lakshmi P},
	pages = {52},
	file = {Urimi - IMAGE RECONSTRUCTION TECHNIQUES and MEASURE OF QUA.pdf:/home/david/Zotero/storage/ANRHK82E/Urimi - IMAGE RECONSTRUCTION TECHNIQUES and MEASURE OF QUA.pdf:application/pdf},
}

@misc{noauthor_root-mean-square_2021,
	title = {Root-mean-square deviation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Root-mean-square_deviation&oldid=1037360077},
	abstract = {The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.
RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = aug,
	year = {2021},
	note = {Page Version ID: 1037360077},
	file = {Snapshot:/home/david/Zotero/storage/GKPR4G23/index.html:text/html},
}

@misc{noauthor_peak_2021,
	title = {Peak signal-to-noise ratio},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Peak_signal-to-noise_ratio&oldid=1062145991},
	abstract = {Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.
PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1062145991},
	file = {Snapshot:/home/david/Zotero/storage/P437AEZH/index.html:text/html},
}

@misc{noauthor_structural_2021,
	title = {Structural similarity},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Structural_similarity&oldid=1053007931},
	abstract = {The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos.  SSIM is used for measuring the similarity between two images. The SSIM index is a full reference metric; in other words, the measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as reference.
SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also incorporating important perceptual phenomena, including both luminance masking and contrast masking terms.  The difference with other techniques such as MSE or PSNR is that these approaches estimate absolute errors.  Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions (in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become less visible where there is significant activity or "texture" in the image.},
	language = {en},
	urldate = {2022-01-04},
	journal = {Wikipedia},
	month = nov,
	year = {2021},
	note = {Page Version ID: 1053007931},
	file = {Snapshot:/home/david/Zotero/storage/DJXTI8CV/index.html:text/html},
}

@article{lin_zhang_fsim_2011,
	title = {{FSIM}: {A} {Feature} {Similarity} {Index} for {Image} {Quality} {Assessment}},
	volume = {20},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{FSIM}},
	url = {http://ieeexplore.ieee.org/document/5705575/},
	doi = {10.1109/TIP.2011.2109730},
	abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural-similarity (SSIM) index brings IQA from pixel-based stage to structure-based stage. In this paper, a novel feature-similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS’ perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
	language = {en},
	number = {8},
	urldate = {2022-01-04},
	journal = {IEEE Transactions on Image Processing},
	author = {{Lin Zhang} and {Lei Zhang} and {Xuanqin Mou} and Zhang, D.},
	month = aug,
	year = {2011},
	pages = {2378--2386},
	file = {Lin Zhang et al. - 2011 - FSIM A Feature Similarity Index for Image Quality.pdf:/home/david/Zotero/storage/N3WVMQ7Z/Lin Zhang et al. - 2011 - FSIM A Feature Similarity Index for Image Quality.pdf:application/pdf},
}

@misc{noauthor_image_2022,
	title = {Image {Similarity} {Measures}},
	copyright = {MIT},
	url = {https://github.com/up42/image-similarity-measures},
	abstract = {:chart\_with\_upwards\_trend: Implementation of eight evaluation metrics to access the similarity between two images. The eight metrics are as follows: RMSE, PSNR, SSIM, ISSM, FSIM, SRE, SAM, and UIQ.},
	urldate = {2022-01-04},
	publisher = {UP42},
	month = jan,
	year = {2022},
	note = {original-date: 2020-04-06T07:27:49Z},
	keywords = {evaluation-metrics, image, machine-learning, metrics},
}

@article{mittal_making_2013,
	title = {Making a “{Completely} {Blind}” {Image} {Quality} {Analyzer}},
	volume = {20},
	issn = {1070-9908, 1558-2361},
	url = {http://ieeexplore.ieee.org/document/6353522/},
	doi = {10.1109/LSP.2012.2227726},
	abstract = {An important aim of research on the blind image quality assessment (IQA) problem is to devise perceptual models that can predict the quality of distorted images with as little prior knowledge of the images or their distortions as possible. Current state-of-the-art ‘general purpose’ no reference (NR) IQA algorithms require knowledge about anticipated distortions in the form of training examples and corresponding human opinion scores. However we have recently derived a blind IQA model that only makes use of measurable deviations from statistical regularities observed in natural images, without training on human-rated distorted images, and, indeed without any exposure to distorted images. Thus, it is ‘completely blind.’ The new IQA model, which we call the Natural Image Quality Evaluator (NIQE) is based on the construction of a ‘quality aware’ collection of statistical features based on a simple and successful space domain natural scene statistic (NSS) model. These features are derived from a corpus of natural, undistorted images. Experimental results show that the new index delivers performance comparable to top performing NR IQA models that require training on large databases of human opinions of distorted images. A software release is available at:http://live.ece.utexas.edu/research/quality/niqe release.zip.},
	language = {en},
	number = {3},
	urldate = {2022-01-04},
	journal = {IEEE Signal Processing Letters},
	author = {Mittal, A. and Soundararajan, R. and Bovik, A. C.},
	month = mar,
	year = {2013},
	pages = {209--212},
	file = {Mittal et al. - 2013 - Making a “Completely Blind” Image Quality Analyzer.pdf:/home/david/Zotero/storage/J7ZH6ASK/Mittal et al. - 2013 - Making a “Completely Blind” Image Quality Analyzer.pdf:application/pdf},
}

@inproceedings{venkatanath_n_blind_2015,
	address = {Mumbai, India},
	title = {Blind image quality evaluation using perception based features},
	isbn = {978-1-4799-6619-6},
	url = {http://ieeexplore.ieee.org/document/7084843/},
	doi = {10.1109/NCC.2015.7084843},
	abstract = {This paper proposes a novel no-reference Perception-based Image QUality Evaluator (PIQUE) for realworld imagery. A majority of the existing methods for blind image quality assessment rely on opinion-based supervised learning for quality score prediction. Unlike these methods, we propose an opinion unaware methodology that attempts to quantify distortion without the need for any training data. Our method relies on extracting local features for predicting quality. Additionally, to mimic human behavior, we estimate quality only from perceptually signiﬁcant spatial regions. Further, the choice of our features enables us to generate a ﬁne-grained block level distortion map. Our algorithm is competitive with the state-of-the-art based on evaluation over several popular datasets including LIVE IQA, TID \& CSIQ. Finally, our algorithm has low computational complexity despite working at the block-level.},
	language = {en},
	urldate = {2022-01-04},
	booktitle = {2015 {Twenty} {First} {National} {Conference} on {Communications} ({NCC})},
	publisher = {IEEE},
	author = {{Venkatanath N} and {Praneeth D} and {Maruthi Chandrasekhar Bh} and Channappayya, Sumohana S. and Medasani, Swarup S.},
	month = feb,
	year = {2015},
	pages = {1--6},
	file = {Venkatanath N et al. - 2015 - Blind image quality evaluation using perception ba.pdf:/home/david/Zotero/storage/5W5E6EB7/Venkatanath N et al. - 2015 - Blind image quality evaluation using perception ba.pdf:application/pdf},
}

@inproceedings{samajdar_analysis_2015,
	address = {New Delhi},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Analysis and {Evaluation} of {Image} {Quality} {Metrics}},
	isbn = {978-81-322-2247-7},
	doi = {10.1007/978-81-322-2247-7_38},
	abstract = {Image Quality Assessment (IQA) is a very difficult task, yet highly important characteristic for evaluation of the image quality. Widely popular IQA techniques, belonging to objective fidelity, like Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR) or subjective fidelity which corresponds to the human visual system (HVS), like, Universal Quality Index (UQI), Structural SIMilarity (SSIM), Feature SIMilarity (FSIM), Feature SIMilarity for color images (FSIMc), Gradient Magnitude Similarity (GSM) have been discussed in this paper. Also quality measured on basis of degradation model and Noise Quality Measure (NQM) has been discussed. Experiments have been conducted on IVC database available online at http://www.irccyn.ec-nantes.fr/ivcdb/ and verified from the CSIQ database and LAR database available online at http://vision.okstate.edu/?loc=csiq and http://www.irccyn.ec-nantes.fr/{\textasciitilde}autrusse/Databases/LAR/. On the basis of the obtained values judgements about the image distortion and hence the optimum image quality metric has been decided. It has been found from all the experiments conducted that FSIM is the best metric for the JPEG, JPEG2000, blur and LAR whereas UQI failed to give better results for all except JPEG2000.},
	language = {en},
	booktitle = {Information {Systems} {Design} and {Intelligent} {Applications}},
	publisher = {Springer India},
	author = {Samajdar, Tina and Quraishi, Md. Iqbal},
	editor = {Mandal, J. K. and Satapathy, Suresh Chandra and Kumar Sanyal, Manas and Sarkar, Partha Pratim and Mukhopadhyay, Anirban},
	year = {2015},
	keywords = {FSIM, FSIMc, GSM, HVS, IQA, MSE, NQM, PSNR, SSIM, UQI},
	pages = {369--378},
}

@misc{ballard_making_2020,
	title = {Making {Matplotlib} {Beautiful} {By} {Default}},
	url = {https://towardsdatascience.com/making-matplotlib-beautiful-by-default-d0d41e3534fd},
	abstract = {Use Seaborn to control Matplotlib defaults (and forget that shade of blue forever)},
	language = {en},
	urldate = {2022-01-13},
	journal = {Medium},
	author = {Ballard, Callum},
	month = may,
	year = {2020},
}

@misc{boston_university_tutorial_2016,
	title = {Tutorial: {Basics} of {Information} {Design} for {Scientific} {Figures}},
	shorttitle = {Tutorial},
	url = {https://www.youtube.com/watch?v=Lb4uG4rIwPA},
	abstract = {In this video, Kelly Krause, Creative Director at Nature, presented a short tutorial session on best practices for design of scientific figures for publication. Kelly spoke at an event sponsored by the Office of the Vice President and Associate Provost for Research. For more information, please see the video from Kelly’s overview of strategies for communicating science through visualization in a publishing context.

February 26, 2016},
	urldate = {2022-01-13},
	author = {{Boston University}},
	month = mar,
	year = {2016},
}

@book{press_fortran_1996,
	address = {Cambridge [England] ; New York},
	edition = {2nd ed},
	title = {{FORTRAN} numerical recipes},
	isbn = {978-0-521-43064-7 978-0-521-57439-6},
	language = {en},
	publisher = {Cambridge University Press},
	editor = {Press, William H.},
	year = {1996},
	keywords = {Computer programs, FORTRAN (Computer program language), Mathematics Computer programs, Numerical analysis, Science},
	file = {Press - 1996 - FORTRAN numerical recipes.pdf:/home/david/Zotero/storage/P4IRRI7H/Press - 1996 - FORTRAN numerical recipes.pdf:application/pdf},
}

@article{vogelgesang_semi-discrete_nodate,
	title = {Semi-{Discrete} {Iteration} {Methods} in {X}-{Ray} {Tomography}},
	language = {en},
	author = {Vogelgesang, Jonas},
	pages = {114},
	file = {Vogelgesang - Semi-Discrete Iteration Methods in X-Ray Tomograph.pdf:/home/david/Zotero/storage/USKLE36L/Vogelgesang - Semi-Discrete Iteration Methods in X-Ray Tomograph.pdf:application/pdf},
}

@inproceedings{horbelt_spline_2000,
	address = {Istanbul, Turkey},
	title = {Spline kernels for continuous-space image processing},
	volume = {4},
	isbn = {978-0-7803-6293-2},
	url = {http://ieeexplore.ieee.org/document/859272/},
	doi = {10.1109/ICASSP.2000.859272},
	abstract = {We present an explicit formula for spline kernels; these are deﬁned as the convolution of several B-splines of variable widths hi and degrees ni. The spline kernels are useful for continuous signal processing algorithms that involve Bspline inner-products or the convolution of several spline basis functions. We apply our results to the derivation of spline-based algorithms for two classes of problems. The ﬁrst is the resizing of images with arbitrary scaling factors. The second is the computation of the Radon transform and of its inverse; in particular, we present a new spline-based version of the ﬁltered backprojection algorithm for tomographic reconstruction. In both cases, our explicit kernel formula allows for the use of high-degree splines; these oﬀer better approximation performance than the conventional lower-order formulations (e.g., piecewise constant or piecewise linear models).},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100})},
	publisher = {IEEE},
	author = {Horbelt, S. and Munoz, A. and Blu, T. and Unser, M.},
	year = {2000},
	pages = {2191--2194},
	file = {Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:/home/david/Zotero/storage/7279UY5X/Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:application/pdf;Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:/home/david/Zotero/storage/ME9RIKEX/Horbelt et al. - 2000 - Spline kernels for continuous-space image processi.pdf:application/pdf},
}

@inproceedings{nilchian_differential_2012,
	title = {Differential phase-contrast {X}-ray computed tomography: {From} model discretization to image reconstruction},
	shorttitle = {Differential phase-contrast {X}-ray computed tomography},
	doi = {10.1109/ISBI.2012.6235491},
	abstract = {Our contribution in this paper is two fold. First, we propose a novel discretization of the forward model for differential phase-contrast imaging that uses B-spline basis functions. The approach yields a fast and accurate algorithm for implementing the forward model, which is based on the first derivative of the Radon transform. Second, as an alternative to the FBP-like approaches that are currently used in practice, we present an iterative reconstruction algorithm that remains more faithful to the data when the number of projections dwindles. Since the reconstruction is an ill-posed problem, we impose a total-variation (TV) regularization constraint. We propose to solve the reconstruction problem using the alternating direction method of multipliers (ADMM). A specificity of our system is the use of a preconditioner that improves the convergence rate of the linear solver in ADMM. Our experiments on test data suggest that our method can achieve the same quality as the standard direct reconstruction, while using only one-third of the projection data. We also find that the approach is much faster than the standard algorithms (ISTA and FISTA) that are typically used for solving linear inverse problems subject to the TV regularization constraint.},
	booktitle = {2012 9th {IEEE} {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Nilchian, Masih and Unser, Michael},
	month = may,
	year = {2012},
	note = {ISSN: 1945-8452},
	keywords = {Computed tomography, Image reconstruction, X-ray imaging, alternating direction method of multipliers (ADMM), differential phase-contrast imaging, filtered back projection (FBP), preconditioned conjugate gradient method, Radon transform, Splines (mathematics), Transforms, Vectors},
	pages = {90--93},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/YF9WSLLA/Nilchian and Unser - 2012 - Differential phase-contrast X-ray computed tomogra.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/Q99JW7GY/6235491.html:text/html},
}

@article{winkler_numerical_1993,
	title = {Numerical recipes in {C}: {The} art of scientific computing, second edition},
	volume = {17},
	issn = {01609327},
	shorttitle = {Numerical recipes in {C}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/016093279390069F},
	doi = {10.1016/0160-9327(93)90069-F},
	language = {en},
	number = {4},
	urldate = {2022-02-02},
	journal = {Endeavour},
	author = {Winkler, Joab R},
	month = jan,
	year = {1993},
	pages = {201},
	file = {Winkler - 1993 - Numerical recipes in C The art of scientific comp.pdf:/home/david/Zotero/storage/DNR7FMEP/Winkler - 1993 - Numerical recipes in C The art of scientific comp.pdf:application/pdf},
}

@misc{noauthor_libstdc_nodate,
	title = {libstdc++: modified\_bessel\_func.tcc {Source} {File}},
	url = {https://gcc.gnu.org/onlinedocs/libstdc++/libstdc++-html-USERS-4.3/a01938.html},
	urldate = {2022-02-02},
	file = {libstdc++\: modified_bessel_func.tcc Source File:/home/david/Zotero/storage/CQR84E5G/a01938.html:text/html},
}

@article{felsner_phase-sensitive_2018,
	title = {Phase-{Sensitive} {Region}-of-{Interest} {Computed} {Tomography}},
	url = {http://arxiv.org/abs/1805.09528},
	abstract = {X-Ray Phase-Contrast Imaging (PCI) yields absorption, differential phase, and dark-ﬁeld images. Computed Tomography (CT) of grating-based PCI can in principle provide high-resolution soft-tissue contrast. Recently, grating-based PCI took several hurdles towards clinical implementation by addressing, for example, acquisition speed, high X-ray energies, and system vibrations. However, a critical impediment in all grating-based systems lies in limits that constrain the grating diameter to few centimeters.},
	language = {en},
	urldate = {2022-02-05},
	journal = {arXiv:1805.09528 [physics]},
	author = {Felsner, Lina and Berger, Martin and Kaeppler, Sebastian and Bopp, Johannes and Ludwig, Veronika and Weber, Thomas and Pelzer, Georg and Michel, Thilo and Maier, Andreas and Anton, Gisela and Riess, Christian},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09528},
	keywords = {Physics - Medical Physics},
	file = {Felsner et al. - 2018 - Phase-Sensitive Region-of-Interest Computed Tomogr.pdf:/home/david/Zotero/storage/4AEJ7EWY/Felsner et al. - 2018 - Phase-Sensitive Region-of-Interest Computed Tomogr.pdf:application/pdf},
}

@article{bayer_reconstruction_2014,
	title = {Reconstruction of scalar and vectorial components in {X}-ray dark-field tomography},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1321080111},
	doi = {10.1073/pnas.1321080111},
	language = {en},
	number = {35},
	urldate = {2022-02-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bayer, F. L. and Hu, S. and Maier, A. and Weber, T. and Anton, G. and Michel, T. and Riess, C. P.},
	month = sep,
	year = {2014},
	pages = {12699--12704},
	file = {Bayer et al. - 2014 - Reconstruction of scalar and vectorial components .pdf:/home/david/Zotero/storage/G4UGQVME/Bayer et al. - 2014 - Reconstruction of scalar and vectorial components .pdf:application/pdf},
}

@article{lee_machine_2018,
	title = {Machine {Friendly} {Machine} {Learning}: {Interpretation} of {Computed} {Tomography} {Without} {Image} {Reconstruction}},
	shorttitle = {Machine {Friendly} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1812.01068},
	abstract = {Recent advancements in deep learning for automated image processing and classiﬁcation have accelerated many new applications for medical image analysis. However, most deep learning applications have been developed using reconstructed, human-interpretable medical images. While image reconstruction from raw sensor data is required for the creation of medical images, the reconstruction process only uses a partial representation of all the data acquired. Here we report the development of a system to directly process raw computed tomography (CT) data in sinogram-space, bypassing the intermediary step of image reconstruction. Two classiﬁcation tasks were evaluated for their feasibility for sinogram-space machine learning: body region identiﬁcation and intracranial hemorrhage (ICH) detection. Our proposed SinoNet performed favorably compared to conventional reconstructed image-space-based systems for both tasks, regardless of scanning geometries in terms of projections or detectors. Further, SinoNet performed signiﬁcantly better when using sparsely sampled sinograms than conventional networks operating in image-space. As a result, sinogram-space algorithms could be used in ﬁeld settings for binary diagnosis testing, triage, and in clinical settings where low radiation dose is desired. These ﬁndings also demonstrate another strength of deep learning where it can analyze and interpret sinograms that are virtually impossible for human experts.},
	language = {en},
	urldate = {2022-02-05},
	journal = {arXiv:1812.01068 [cs]},
	author = {Lee, Hyunkwang and Huang, Chao and Yune, Sehyo and Tajmir, Shahein H. and Kim, Myeongchan and Do, Synho},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.01068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lee et al. - 2018 - Machine Friendly Machine Learning Interpretation .pdf:/home/david/Zotero/storage/7XLGLB3Q/Lee et al. - 2018 - Machine Friendly Machine Learning Interpretation .pdf:application/pdf},
}

@inproceedings{modregger_artifacts_2011,
	address = {Chicago, Illinois, (USA)},
	title = {Artifacts in {X}-ray {Dark}-{Field} {Tomography}},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.3625356},
	doi = {10.1063/1.3625356},
	abstract = {Grating-based x-ray imaging provides three principle kinds of contrast: absorption, phase, and dark-field. Due to the availability of tomographic reconstruction algorithms for the dark-field contrast, it is now possible to take advantage of quantitative scatter information. However, the published algorithm is based on several assumptions that might be violated in reality. We use numerical simulations in order to identify artifacts in the reconstructions, which is crucial for the interpretation of experimental data.},
	language = {en},
	urldate = {2022-02-05},
	author = {Modregger, P. and Wang, Z. and Thuering, T. and Pinzer, B. and Stampanoni, M. and McNulty, Ian and Eyberger, Catherine and Lai, Barry},
	year = {2011},
	pages = {269--272},
	file = {Modregger et al. - 2011 - Artifacts in X-ray Dark-Field Tomography.pdf:/home/david/Zotero/storage/EWSZKLRM/Modregger et al. - 2011 - Artifacts in X-ray Dark-Field Tomography.pdf:application/pdf},
}

@misc{noauthor_carbon_nodate,
	title = {Carbon},
	url = {https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=one-dark&wt=none&l=text%2Fx-c%2B%2Bsrc&width=680&ds=true&dsyoff=20px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false&code=template%253Ctypename%2520data_t%253E%250Adata_t%2520reduce%28const%2520DataConatiner%253Cdata_t%253E%2520a%29%2520%257B%250A%2520%2520%2520%2520data_t%2520sum%2520%253D%2520data_t%280%29%253B%250A%2520%2520%2520%2520for%28int%2520i%253D0%253B%2520i%253Ca.size%28%29%253B%2520%252B%252Bi%29%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520sum%2520%252B%253D%2520a%255Bi%255D%253B%250A%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520return%2520sum%253B%250A%257D},
	abstract = {Carbon is the easiest way to create and share beautiful images of your source code.},
	language = {en},
	urldate = {2022-02-06},
}

@misc{pine_octrefpolacode_2022,
	title = {octref/polacode},
	url = {https://github.com/octref/polacode},
	abstract = {📸 Polaroid for your code},
	urldate = {2022-02-06},
	author = {Pine},
	month = feb,
	year = {2022},
	note = {original-date: 2018-02-09T23:10:07Z},
	keywords = {screenshot, snippets, visual-studio-code, vscode},
}

@article{vogel_tomographic_2015,
	title = {Tomographic {Reconstruction} beyond {Classical} {X}-ray {CT}},
	language = {de},
	author = {Vogel, Jakob},
	year = {2015},
	pages = {245},
	file = {Vogel - Tomographic Reconstruction beyond Classical X-ray .pdf:/home/david/Zotero/storage/S6NB8THJ/Vogel - Tomographic Reconstruction beyond Classical X-ray .pdf:application/pdf},
}

@article{horacsek_evaluating_nodate,
	title = {Evaluating {Box} {Splines} with {Reduced} {Complexity}},
	abstract = {For the class of non-degenerate box splines, we present a set construction scheme that yields the explicit piecewise polynomial form for an arbitrary box spline. While it is possible to use the well known recursive formulation to obtain these polynomial pieces, this is quite expensive. Our construction is theoretically less expensive than the recursive formulation and allows us to evaluate box splines with more direction vectors than what would be feasible under the recursive scheme. Finally, using the explicit polynomials in each region of the box spline, we show how to create fast evaluation schemes using this explicit characterization and a spatial data structure.},
	language = {en},
	author = {Horacsek, Joshua and Alim, Usman},
	pages = {25},
	file = {Horacsek and Alim - Evaluating Box Splines with Reduced Complexity.pdf:/home/david/Zotero/storage/8KZVDAEF/Horacsek and Alim - Evaluating Box Splines with Reduced Complexity.pdf:application/pdf},
}

@misc{noauthor_boxspline1_2019,
	title = {boxspline1},
	copyright = {GPL-3.0},
	url = {https://github.com/CFD-GO/boxspline1/blob/90d7964bdb1999088b08e12ecf20adf263de6d8b/R/R/boxspline1.R},
	abstract = {1D box-spline library for limited spline parametrizations},
	urldate = {2022-02-26},
	publisher = {CFD on the GO},
	month = jun,
	year = {2019},
	note = {original-date: 2019-06-14T15:23:02Z},
}

@article{mccann_high-quality_2017,
	title = {High-{Quality} {Parallel}-{Ray} {X}-{Ray} {CT} {Back} {Projection} {Using} {Optimized} {Interpolation}},
	volume = {26},
	issn = {1941-0042},
	doi = {10.1109/TIP.2017.2706521},
	abstract = {We propose a new, cost-efficient method for computing back projections in parallel-ray X-ray CT. Forward and back projections are the basis of almost all X-ray CT reconstruction methods, but computing these accurately is costly. In the special case of parallel-ray geometry, it turns out that reconstruction requires back projection only. One approach to accelerate the back projection is through interpolation: fit a continuous representation to samples of the desired signal, then sample it at the required locations. Instead, we propose applying a prefilter that has the effect of orthogonally projecting the underlying signal onto the space spanned by the interpolator, which can significantly improve the quality of the interpolation. We then build on this idea by using oblique projection, which simplifies the computation while giving effectively the same improvement in quality. Our experiments on analytical phantoms show that this refinement can improve the reconstruction quality for both filtered back projection and iterative reconstruction in the high-quality regime, i.e., with low noise and many measurements.},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {McCann, Michael T. and Unser, Michael},
	month = oct,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Interpolation, Kernel, Computed tomography, Image reconstruction, computed tomography, Transforms, Back, interpolation, reconstruction algorithms, Standards, X-ray tomography},
	pages = {4639--4647},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/HF4RDN6N/McCann and Unser - 2017 - High-Quality Parallel-Ray X-Ray CT Back Projection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/GBNU3HCK/7932483.html:text/html},
}

@article{ma_improvements_nodate,
	title = {Improvements to the {Algorithm} that {Uses} {Divided} {Differences} to {Determine} the {Coefficients} in {B}-{Spline} {Interpolation}},
	abstract = {In numerical measuring processes the interpolation is an important part. Some algorithms of B-spline interpolation were developed along the time. Here are presented two additional methods to reduce the interpolation errors. These methods could be used in any case for uniformly spaced data. These interpolation processes requires supplementary numerical calculations. Always is to choose between a smallest amount of computation and better results of interpolation.},
	language = {en},
	author = {Mâ, Liliana},
	pages = {4},
	file = {Mâ - Improvements to the Algorithm that Uses Divided Di.pdf:/home/david/Zotero/storage/ID4GLSRM/Mâ - Improvements to the Algorithm that Uses Divided Di.pdf:application/pdf},
}

@article{takekawa_fast_2021,
	title = {Fast parallel calculation of modified {Bessel} function of the second kind and its derivatives},
	url = {http://arxiv.org/abs/2108.11560},
	abstract = {There are three main types of numerical computations for the Bessel function of the second kind: series expansion, continued fraction, and asymptotic expansion. In addition, they are combined in the appropriate domain for each. However, there are some regions where the combination of these types requires suﬃcient computation time to achieve suﬃcient accuracy, however, eﬃciency is signiﬁcantly reduced when parallelized. In the proposed method, we adopt a simple numerical integration concept of integral representation. We coarsely reﬁne the integration range beforehand, and stabilize the computation time by performing the integration calculation at a ﬁxed number of intervals. Experiments demonstrate that the proposed method can achieve the same level of accuracy as existing methods in less than half the computation time.},
	language = {en},
	urldate = {2022-03-12},
	journal = {arXiv:2108.11560 [cs, math]},
	author = {Takekawa, Takashi},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.11560},
	keywords = {Mathematics - Numerical Analysis, 65D15, G.1.2, G.4},
	file = {Takekawa - 2021 - Fast parallel calculation of modified Bessel funct.pdf:/home/david/Zotero/storage/L6K2HGEU/Takekawa - 2021 - Fast parallel calculation of modified Bessel funct.pdf:application/pdf},
}

@article{gil_evaluation_2002,
	title = {Evaluation of the {Modified} {Bessel} {Function} of the {Third} {Kind} of {Imaginary} {Orders}},
	volume = {175},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999101968949},
	doi = {10.1006/jcph.2001.6894},
	language = {en},
	number = {2},
	urldate = {2022-03-12},
	journal = {Journal of Computational Physics},
	author = {Gil, Amparo and Segura, Javier and Temme, Nico M.},
	month = jan,
	year = {2002},
	pages = {398--411},
	file = {Gil et al. - 2002 - Evaluation of the Modified Bessel Function of the .pdf:/home/david/Zotero/storage/FRVRYISU/Gil et al. - 2002 - Evaluation of the Modified Bessel Function of the .pdf:application/pdf},
}

@article{temme_numerical_1975,
	title = {On the numerical evaluation of the modified bessel function of the third kind},
	volume = {19},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0021999175900820},
	doi = {10.1016/0021-9991(75)90082-0},
	language = {en},
	number = {3},
	urldate = {2022-03-12},
	journal = {Journal of Computational Physics},
	author = {Temme, N.M.},
	month = nov,
	year = {1975},
	pages = {324--337},
	file = {Temme - 1975 - On the numerical evaluation of the modified bessel.pdf:/home/david/Zotero/storage/JDEA9S49/Temme - 1975 - On the numerical evaluation of the modified bessel.pdf:application/pdf},
}

@misc{noauthor_r_nodate,
	title = {r - {Modified} {Bessel} functions of order (n)},
	url = {https://stackoverflow.com/questions/8797722/modified-bessel-functions-of-order-n},
	urldate = {2022-03-12},
	journal = {Stack Overflow},
	file = {Snapshot:/home/david/Zotero/storage/H2RRYICA/modified-bessel-functions-of-order-n.html:text/html},
}

@misc{noauthor_index_nodate,
	title = {Index of {Code} {Identifiers}},
	url = {https://www.astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/index_by_ident.htm},
	urldate = {2022-03-12},
	file = {Index of Code Identifiers:/home/david/Zotero/storage/RV4H9B4N/index_by_ident.html:text/html},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/code/bessel.h},
	urldate = {2022-03-12},
	file = {astro.umd.edu/~ricotti/NEWWEB/teaching/ASTR415/InClassExamples/NR3/code/bessel.h:/home/david/Zotero/storage/QW9ZHIZT/bessel.html:text/html},
}

@article{withers_x-ray_2021,
	title = {X-ray computed tomography},
	volume = {1},
	issn = {2662-8449},
	url = {http://www.nature.com/articles/s43586-021-00015-4},
	doi = {10.1038/s43586-021-00015-4},
	abstract = {X-r ay computed tomography (CT) can reveal the internal details of objects in three dimensions non-d estructively. In this Primer, we outline the basic principles of CT and describe the ways in which a CT scan can be acquired using X-r ay tubes and synchrotron sources, including the different possible contrast modes that can be exploited. We explain the process of computationally reconstructing three-d imensional (3D) images from 2D radiographs and how to segment the 3D images for subsequent visualization and quantification. Whereas CT is widely used in medical and heavy industrial contexts at relatively low resolutions, here we focus on the application of higher resolution X-r ay CT across science and engineering. We consider the application of X-ray CT to study subjects across the materials, metrology and manufacturing, engineering, food, biological, geological and palaeontological sciences. We examine how CT can be used to follow the structural evolution of materials in three dimensions in real time or in a time-lapse manner, for example to follow materials manufacturing or the in-s ervice behaviour and degradation of manufactured components. Finally, we consider the potential for radiation damage and common sources of imaging artefacts, discuss reproducibility issues and consider future advances and opportunities.},
	language = {en},
	number = {1},
	urldate = {2022-03-15},
	journal = {Nature Reviews Methods Primers},
	author = {Withers, Philip J. and Bouman, Charles and Carmignato, Simone and Cnudde, Veerle and Grimaldi, David and Hagen, Charlotte K. and Maire, Eric and Manley, Marena and Du Plessis, Anton and Stock, Stuart R.},
	month = dec,
	year = {2021},
	pages = {18},
	file = {Withers et al. - 2021 - X-ray computed tomography.pdf:/home/david/Zotero/storage/AZGEZT7Y/Withers et al. - 2021 - X-ray computed tomography.pdf:application/pdf},
}

@article{der_sarkissian_cone-beam_2019,
	title = {A cone-beam {X}-ray computed tomography data collection designed for machine learning},
	volume = {6},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-019-0235-y},
	doi = {10.1038/s41597-019-0235-y},
	abstract = {Abstract
            Unlike previous works, this open data collection consists of X-ray cone-beam (CB) computed tomography (CT) datasets specifically designed for machine learning applications and high cone-angle artefact reduction. Forty-two walnuts were scanned with a laboratory X-ray set-up to provide not only data from a single object but from a class of objects with natural variability. For each walnut, CB projections on three different source orbits were acquired to provide CB data with different cone angles as well as being able to compute artefact-free, high-quality ground truth images from the combined data that can be used for supervised learning. We provide the complete image reconstruction pipeline: raw projection data, a description of the scanning geometry, pre-processing and reconstruction scripts using open software, and the reconstructed volumes. Due to this, the dataset can not only be used for high cone-angle artefact reduction but also for algorithm development and evaluation for other tasks, such as image reconstruction from limited or sparse-angle (low-dose) scanning, super resolution, or segmentation.},
	language = {en},
	number = {1},
	urldate = {2022-03-15},
	journal = {Scientific Data},
	author = {Der Sarkissian, Henri and Lucka, Felix and van Eijnatten, Maureen and Colacicco, Giulia and Coban, Sophia Bethany and Batenburg, Kees Joost},
	month = dec,
	year = {2019},
	pages = {215},
	file = {Der Sarkissian et al. - 2019 - A cone-beam X-ray computed tomography data collect.pdf:/home/david/Zotero/storage/W7QC52EH/Der Sarkissian et al. - 2019 - A cone-beam X-ray computed tomography data collect.pdf:application/pdf},
}

@misc{der_sarkissian_cone-beam_2019-1,
	title = {Cone-{Beam} {X}-{Ray} {CT} {Data} {Collection} {Designed} for {Machine} {Learning}: {Samples} 1-8},
	shorttitle = {Cone-{Beam} {X}-{Ray} {CT} {Data} {Collection} {Designed} for {Machine} {Learning}},
	url = {https://zenodo.org/record/2686726},
	abstract = {This upload contains samples 1 - 8 from the data collection described in Henri Der Sarkissian, Felix Lucka, Maureen van Eijnatten, Giulia Colacicco, Sophia Bethany Coban, Kees Joost Batenburg, "A Cone-Beam X-Ray CT Data Collection Designed for Machine Learning", Sci Data 6, 215 (2019). https://doi.org/10.1038/s41597-019-0235-y or arXiv:1905.04787 (2019) Abstract: "Unlike previous works, this open data collection consists of X-ray cone-beam (CB) computed tomography (CT) datasets specifically designed for machine learning applications and high cone-angle artefact reduction: Forty-two walnuts were scanned with a laboratory X-ray setup to provide not only data from a single object but from a class of objects with natural variability. For each walnut, CB projections on three different orbits were acquired to provide CB data with different cone angles as well as being able to compute artefact-free, high-quality ground truth images from the combined data that can be used for supervised learning. We provide the complete image reconstruction pipeline: raw projection data, a description of the scanning geometry, pre-processing and reconstruction scripts using open software, and the reconstructed volumes. Due to this, the dataset can not only be used for high cone-angle artefact reduction but also for algorithm development and evaluation for other tasks, such as image reconstruction from limited or sparse-angle (low-dose) scanning, super resolution, or segmentation." The scans are performed using a custom-built, highly flexible X-ray CT scanner, the FleX-ray scanner, developed by XRE nvand located in the FleX-ray Lab at the Centrum Wiskunde \& Informatica (CWI) in Amsterdam, Netherlands. The general purpose of the FleX-ray Lab is to conduct proof of concept experiments directly accessible to researchers in the field of mathematics and computer science. The scanner consists of a cone-beam microfocus X-ray point source that projects polychromatic X-rays onto a 1536-by-1944 pixels, 14-bit flat panel detector (Dexella 1512NDT) and a rotation stage in-between, upon which a sample is mounted. All three components are mounted on translation stages which allow them to move independently from one another. Please refer to the paper for all further technical details. The complete data set can be found via the following links: 1-8, 9-16, 17-24, 25-32, 33-37, 38-42 The corresponding Python scripts for loading, pre-processing and reconstructing the projection data in the way described in the paper can be found on github For more information or guidance in using these dataset, please get in touch with henri.dersarkissian [at] gmail.com Felix.Lucka [at] cwi.nl},
	urldate = {2022-03-15},
	publisher = {Zenodo},
	author = {Der Sarkissian, Henri and Lucka, Felix and van Eijnatten, Maureen and Colacicco, Giulia and Coban, Sophia Bethany and Batenburg, K. Joost},
	month = may,
	year = {2019},
	doi = {10.5281/zenodo.2686726},
	note = {Type: dataset},
	keywords = {computed tomography, cone beam, deep learning, image reconstruction, machine learning, X-ray},
	file = {Zenodo Snapshot:/home/david/Zotero/storage/6VPDHAU3/2686726.html:text/html},
}

@inproceedings{gach_2d_2008,
	address = {Las Vegas, NV, USA},
	title = {{2D} \&\#x00026; {3D} {Shepp}-{Logan} {Phantom} {Standards} for {MRI}},
	isbn = {978-0-7695-3331-5},
	url = {http://ieeexplore.ieee.org/document/4616690/},
	doi = {10.1109/ICSEng.2008.15},
	abstract = {The Shepp-Logan phantom was created as a standard for computerized tomography (CT) image reconstruction simulations of the head. The phantom is also used frequently for magnetic resonance image (MRI) reconstruction and k-space simulations. However, while the CT version incorporated the radiation attenuation properties of the head and brain, the MRI version of the phantom was neither adapted to MR physics nor defined as a viable standard. As a result, it is not currently possible to compare and validate MR simulations from different research studies. In this paper, we present 2D and 3D SheppLogan MRI phantom standards and their analytic kspace representations based on existing CT standards after modification for MR physics. The standard includes T1 and T2 relaxation times for different fields strengths based on current literature.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {2008 19th {International} {Conference} on {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Gach, H. Michael and Tanase, Costin and Boada, Fernando},
	month = aug,
	year = {2008},
	pages = {521--526},
	file = {Gach et al. - 2008 - 2D &#x00026\; 3D Shepp-Logan Phantom Standards for .pdf:/home/david/Zotero/storage/GYBE7RNT/Gach et al. - 2008 - 2D &#x00026\; 3D Shepp-Logan Phantom Standards for .pdf:application/pdf},
}

@article{la_riviere_spline-based_1998,
	title = {Spline-based inverse {Radon} transform in two and three dimensions},
	volume = {45},
	issn = {0018-9499, 1558-1578},
	url = {https://ieeexplore.ieee.org/document/708352/},
	doi = {10.1109/23.708352},
	abstract = {While the exact inverse Radon transform is a continuous integral equation, the discrete nature of the data output by tomographic imaging systems generally demands that images be reconstructed using a discrete approximation to the transform. However, by fitting an analytic function to the projection data prior to reconstruction, one can avoid such approximations and preserve and exploit the continuous nature of the inverse transform.},
	language = {en},
	number = {4},
	urldate = {2022-03-18},
	journal = {IEEE Transactions on Nuclear Science},
	author = {La Riviere, P.J. and Pan, X.},
	month = aug,
	year = {1998},
	pages = {2224--2231},
	file = {La Riviere and Pan - 1998 - Spline-based inverse Radon transform in two and th.pdf:/home/david/Zotero/storage/4VSCFHND/La Riviere and Pan - 1998 - Spline-based inverse Radon transform in two and th.pdf:application/pdf},
}

@inproceedings{ye_box_2011,
	address = {Chicago, IL, USA},
	title = {Box spline based {3D} tomographic reconstruction of diffusion propagators from {MRI} data},
	isbn = {978-1-4244-4127-3},
	url = {http://ieeexplore.ieee.org/document/5872432/},
	doi = {10.1109/ISBI.2011.5872432},
	abstract = {This paper introduces a tomographic approach for reconstruction of diffusion propagators, P(r), in a box spline framework. Box splines are chosen as basis functions for high-order approximation of P(r) from the diffusion signal. Box splines are a generalization of B-splines to multivariate setting that are particularly useful in the context of tomographic reconstruction. The X-Ray or Radon transform of a (tensor-product B-spline or a non-separable) box spline is a box spline – the space of box splines is closed under the Radon transform.},
	language = {en},
	urldate = {2022-03-19},
	booktitle = {2011 {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {From} {Nano} to {Macro}},
	publisher = {IEEE},
	author = {Ye, Wenxing and Portnoy, Sharon and Entezari, Alireza and Vemuri, Baba C. and Blackband, Stephen J.},
	month = mar,
	year = {2011},
	pages = {397--400},
	file = {Ye et al. - 2011 - Box spline based 3D tomographic reconstruction of .pdf:/home/david/Zotero/storage/4MPYPVU4/Ye et al. - 2011 - Box spline based 3D tomographic reconstruction of .pdf:application/pdf},
}

@article{wan_high-performance_2012,
	title = {High-performance blob-based iterative three-dimensional reconstruction in electron tomography using multi-{GPUs}},
	volume = {13},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-S10-S4},
	doi = {10.1186/1471-2105-13-S10-S4},
	abstract = {Background: Three-dimensional (3D) reconstruction in electron tomography (ET) has emerged as a leading technique to elucidate the molecular structures of complex biological specimens. Blob-based iterative methods are advantageous reconstruction methods for 3D reconstruction in ET, but demand huge computational costs. Multiple graphic processing units (multi-GPUs) offer an affordable platform to meet these demands. However, a synchronous communication scheme between multi-GPUs leads to idle GPU time, and a weighted matrix involved in iterative methods cannot be loaded into GPUs especially for large images due to the limited available memory of GPUs.
Results: In this paper we propose a multilevel parallel strategy combined with an asynchronous communication scheme and a blob-ELLR data structure to efficiently perform blob-based iterative reconstructions on multi-GPUs. The asynchronous communication scheme is used to minimize the idle GPU time so as to asynchronously overlap communications with computations. The blob-ELLR data structure only needs nearly 1/16 of the storage space in comparison with ELLPACK-R (ELLR) data structure and yields significant acceleration.
Conclusions: Experimental results indicate that the multilevel parallel scheme combined with the asynchronous communication scheme and the blob-ELLR data structure allows efficient implementations of 3D reconstruction in ET on multi-GPUs.},
	language = {en},
	number = {S10},
	urldate = {2022-03-19},
	journal = {BMC Bioinformatics},
	author = {Wan, Xiaohua and Zhang, Fa and Chu, Qi and Liu, Zhiyong},
	month = jun,
	year = {2012},
	pages = {S4},
	file = {Wan et al. - 2012 - High-performance blob-based iterative three-dimens.pdf:/home/david/Zotero/storage/U3JHAGCL/Wan et al. - 2012 - High-performance blob-based iterative three-dimens.pdf:application/pdf},
}

@phdthesis{erlandsson_positron_1996,
	address = {Lund},
	title = {Positron emission tomography with three-dimensional reconstruction},
	language = {en},
	author = {Erlandsson, Kjell},
	year = {1996},
	note = {ISBN: 9789162821890
OCLC: 186220689},
	file = {Erlandsson - 1996 - Positron emission tomography with three-dimensiona.pdf:/home/david/Zotero/storage/HCFMWIVN/Erlandsson - 1996 - Positron emission tomography with three-dimensiona.pdf:application/pdf},
}

@article{zhang_convolutional_2019,
	title = {A {Convolutional} {Forward} and {Back}-{Projection} {Model} for {Fan}-{Beam} {Geometry}},
	url = {http://arxiv.org/abs/1907.10526},
	abstract = {Iterative methods for tomographic image reconstruction have great potential for enabling high quality imaging from low-dose projection data. The computational burden of iterative reconstruction algorithms, however, has been an impediment in their adoption in practical CT reconstruction problems. We present an approach for highly efﬁcient and accurate computation of forward model for image reconstruction in fan-beam geometry in X-ray CT. The efﬁciency of computations makes this approach suitable for large-scale optimization algorithms with on-the-ﬂy, memory-less, computations of the forward and back-projection. Our experiments demonstrate the improvements in accuracy as well as efﬁciency of our model, speciﬁcally for ﬁrst-order box splines (i.e., pixel-basis) compared to recently developed methods for this purpose, namely Look-up Table-based Ray Integration (LTRI) and Separable Footprints (SF) in 2-D.},
	language = {en},
	urldate = {2022-03-19},
	journal = {arXiv:1907.10526 [cs, eess]},
	author = {Zhang, Kai and Entezari, Alireza},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.10526},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Zhang and Entezari - 2019 - A Convolutional Forward and Back-Projection Model .pdf:/home/david/Zotero/storage/A96JJE6K/Zhang and Entezari - 2019 - A Convolutional Forward and Back-Projection Model .pdf:application/pdf},
}

@article{agulleiro_evaluation_2012,
	title = {Evaluation of a {Multicore}-{Optimized} {Implementation} for {Tomographic} {Reconstruction}},
	volume = {7},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0048261},
	doi = {10.1371/journal.pone.0048261},
	abstract = {Tomography allows elucidation of the three-dimensional structure of an object from a set of projection images. In life sciences, electron microscope tomography is providing invaluable information about the cell structure at a resolution of a few nanometres. Here, large images are required to combine wide fields of view with high resolution requirements. The computational complexity of the algorithms along with the large image size then turns tomographic reconstruction into a computationally demanding problem. Traditionally, high-performance computing techniques have been applied to cope with such demands on supercomputers, distributed systems and computer clusters. In the last few years, the trend has turned towards graphics processing units (GPUs). Here we present a detailed description and a thorough evaluation of an alternative approach that relies on exploitation of the power available in modern multicore computers. The combination of single-core code optimization, vector processing, multithreading and efficient disk I/O operations succeeds in providing fast tomographic reconstructions on standard computers. The approach turns out to be competitive with the fastest GPU-based solutions thus far.},
	language = {en},
	number = {11},
	urldate = {2022-03-19},
	journal = {PLoS ONE},
	author = {Agulleiro, Jose-Ignacio and Fernández, José Jesús},
	editor = {Frischknecht, Friedrich},
	month = nov,
	year = {2012},
	pages = {e48261},
	file = {Agulleiro and Fernández - 2012 - Evaluation of a Multicore-Optimized Implementation.pdf:/home/david/Zotero/storage/V4U5C5EH/Agulleiro and Fernández - 2012 - Evaluation of a Multicore-Optimized Implementation.pdf:application/pdf},
}

@article{kaufman_maximum_1993,
	title = {Maximum likelihood, least squares, and penalized least squares for {PET}},
	volume = {12},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/232249/},
	doi = {10.1109/42.232249},
	abstract = {The EM algorithm is the basic approach used to maximize the log likelihood objective function for the reconstruction problem in PET. The EM algorithm is a scaled steepest ascent algorithm that elegantly handles the nonnegativity constraints of the problem. We show that the same scaled steepest descent algorithm can be applied to the least squares merit function, and that it can be accelerated using the conjugate gradient approach. Our experiments suggest that one can cut the computation by about a factor of 3 by using this technique. Our results also apply to various penalized least squares functions which might be used to produce a smoother image.},
	language = {en},
	number = {2},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Kaufman, L.},
	month = jun,
	year = {1993},
	pages = {200--214},
	file = {Kaufman - 1993 - Maximum likelihood, least squares, and penalized l.pdf:/home/david/Zotero/storage/TXLSS8VP/Kaufman - 1993 - Maximum likelihood, least squares, and penalized l.pdf:application/pdf},
}

@article{ollinger_maximum-likelihood_1994,
	title = {Maximum-likelihood reconstruction of transmission images in emission computed tomography via the {EM} algorithm},
	volume = {13},
	issn = {02780062},
	url = {http://ieeexplore.ieee.org/document/276147/},
	doi = {10.1109/42.276147},
	abstract = {The expectation-maximization (EM) algm·ithm fm' computing maximum-likelihood estimates of transmission images in positron-emission tomogl'aJ{\textgreater}hy (PET) [1) is extended to include measurement enm·, accidental coincidences and Compton scattel'. A method for accomplishing the maximization stet{\textgreater} using one step of Newton's method is tn·oposed. The algorithm is regularized with the method of sieves. Evaluations using both Monte Carlo simulations and phantom studies on the Siemens 953B scanne1· suggest that the algorithm yields unbiased images with significantly lower variances than filtered-backprojcclion when the images are reconstructed to the intrinsic resolution. Large features in the images converge in under 200 iterations while the smallest features required up to 2,000 iterations. All but the smallest features in typical transmission scans converge In approximately 250 iterations. The initial implementation of the algorithm requires 50 sec per iteration on a DECStation 5000.},
	language = {en},
	number = {1},
	urldate = {2022-03-23},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Ollinger, J.M.},
	month = mar,
	year = {1994},
	pages = {89--101},
	file = {Ollinger - 1994 - Maximum-likelihood reconstruction of transmission .pdf:/home/david/Zotero/storage/ZIY2QA8M/Ollinger - 1994 - Maximum-likelihood reconstruction of transmission .pdf:application/pdf},
}

@book{halefoglu_computed_2017,
	title = {Computed {Tomography} - {Advanced} {Applications}},
	isbn = {978-953-51-3368-1},
	url = {https://www.intechopen.com/books/6005},
	abstract = {The advent and rapid diffusion of advanced multidetector-row scanner technology offers comprehensive evaluation of different anatomic structures in daily practice. The aim of this book is to introduce the applications of CT imaging in not only general medicine but also in different fields especially in veterinary medicine, dentistry, and engineering. Recent developments in CT technology have led to a widening of its applications on many areas like material testing in engineering, 3D evaluation of teeth, and the vascular and cardiac evaluations of small animals.},
	language = {en},
	urldate = {2022-03-24},
	author = {Halefoglu, Ahmet Mesrur},
	month = aug,
	year = {2017},
	doi = {10.5772/66614},
	file = {Snapshot:/home/david/Zotero/storage/CDYVVUZM/6005.html:text/html},
}

@book{herman_discrete_1999,
	address = {Boston, MA},
	title = {Discrete {Tomography} {Foundations}, {Algorithms}, and {Applications}.},
	isbn = {978-1-4612-1568-4},
	url = {http://public.eblib.com/choice/PublicFullRecord.aspx?p=6489295},
	language = {en},
	urldate = {2022-03-24},
	publisher = {Birkh??user Boston},
	author = {Herman, Gabor T and Kuba, Attila},
	year = {1999},
	note = {OCLC: 1245672421},
	file = {Herman and Kuba - 1999 - Discrete Tomography Foundations, Algorithms, and A.pdf:/home/david/Zotero/storage/5WAZT7JH/Herman and Kuba - 1999 - Discrete Tomography Foundations, Algorithms, and A.pdf:application/pdf},
}

@book{carmignato_industrial_2018,
	address = {Cham},
	edition = {1st ed. 2018},
	title = {Industrial {X}-{Ray} {Computed} {Tomography}},
	isbn = {978-3-319-59573-3},
	abstract = {This book covers all aspects of industrial X-Ray computed tomography (XCT) including history, basics, different instrument architectures, hardware and software, error sources, traceability and calibration, and applications. The book is intended for users and developers of XCT instrumentation and software in both industry and academia, being also suitable for postgraduate students},
	language = {en},
	publisher = {Springer International Publishing : Imprint: Springer},
	editor = {Carmignato, Simone and Dewulf, Wim and Leach, Richard},
	year = {2018},
	doi = {10.1007/978-3-319-59573-3},
	keywords = {Atomic, Molecular, Optical and Plasma Physics, Atoms, Characterization and Evaluation of Materials, Electronic materials, Manufactures, Manufacturing, Machines, Tools, Processes, Materials science, Optical and Electronic Materials, Optical materials, Physics},
	file = {Carmignato et al. - 2018 - Industrial X-Ray Computed Tomography.pdf:/home/david/Zotero/storage/UNEGITUR/Carmignato et al. - 2018 - Industrial X-Ray Computed Tomography.pdf:application/pdf},
}

@article{weitkamp_x-ray_2008,
	series = {Proceedings of the 5th {Medical} {Application} of {Synchrotron} {Radiation} 2007},
	title = {X-ray phase radiography and tomography of soft tissue using grating interferometry},
	volume = {68},
	issn = {0720-048X},
	url = {https://www.sciencedirect.com/science/article/pii/S0720048X08002738},
	doi = {10.1016/j.ejrad.2008.04.031},
	abstract = {X-ray phase and absorption radiographs and tomograms of the heart of a rat were taken with an X-ray grating interferometer with monochromatic synchrotron radiation at a photon energy of 17.5keV. The phase images show largely superior quality with respect to the absorption images taken with the same dose, particularly much better contrast and contrast-to-noise ratio. Different tissues can clearly be distinguished. The results demonstrate the potential of grating interferometry for two- and three-dimensional X-ray imaging of biological soft tissue in an aqueous environment.},
	language = {en},
	number = {3, Supplement},
	urldate = {2022-03-24},
	journal = {European Journal of Radiology},
	author = {Weitkamp, Timm and David, Christian and Bunk, Oliver and Bruder, Jens and Cloetens, Peter and Pfeiffer, Franz},
	month = dec,
	year = {2008},
	keywords = {Tomography, Radiography, Talbot interferometer, X-ray interferometer, X-ray phase contrast},
	pages = {S13--S17},
	file = {ScienceDirect Snapshot:/home/david/Zotero/storage/AHNDKVZB/S0720048X08002738.html:text/html},
}

@article{weitkamp_x-ray_2005,
	title = {X-ray phase imaging with a grating interferometer},
	abstract = {Using a high-efﬁciency grating interferometer for hard X rays (10–30 keV) and a phase-stepping technique, separate radiographs of the phase and absorption proﬁles of bulk samples can be obtained from a single set of measurements. Tomographic reconstruction yields quantitative three-dimensional maps of the X-ray refractive index, with a spatial resolution down to a few microns. The method is mechanically robust, requires little spatial coherence and monochromaticity, and can be scaled up to large ﬁelds of view, with a detector of correspondingly moderate spatial resolution. These are important prerequisites for use with laboratory X-ray sources.},
	language = {en},
	author = {Weitkamp, Timm and Diaz, Ana and David, Christian and Pfeiffer, Franz and Stampanoni, Marco and Cloetens, Peter and Ziegler, Eric},
	year = {2005},
	pages = {9},
	file = {Weitkamp et al. - 2005 - X-ray phase imaging with a grating interferometer.pdf:/home/david/Zotero/storage/8V67HT5L/Weitkamp et al. - 2005 - X-ray phase imaging with a grating interferometer.pdf:application/pdf},
}

@article{teuffenbach_grating-based_2017,
	title = {Grating-based phase-contrast and dark-field computed tomography: a single-shot method},
	doi = {https://doi.org/10.1038/s41598-017-06729-4},
	language = {en},
	journal = {SCienTifiC REPorts},
	author = {Teuffenbach, M.v. and Koehler, T. and Fehringer, A.},
	year = {2017},
	pages = {8},
	file = {Grating-based phase-contrast and dark-field comput.pdf:/home/david/Zotero/storage/G95PIVFX/Grating-based phase-contrast and dark-field comput.pdf:application/pdf;Teuffenbach et al. - 2017 - Grating-based phase-contrast and dark-field comput.pdf:/home/david/Zotero/storage/X9P6758U/Teuffenbach et al. - 2017 - Grating-based phase-contrast and dark-field comput.pdf:application/pdf},
}

@article{viermetz_dark-field_2022,
	title = {Dark-field computed tomography reaches the human scale},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2118799119},
	doi = {10.1073/pnas.2118799119},
	abstract = {Significance
            X-ray computed tomography (CT) is one of the most commonly used diagnostic three-dimensional imaging modalities today. Conventionally, this noninvasive technique generates contrast by measuring the X-ray attenuation properties of different tissues. Considering the wave nature of X-rays, complementary contrast can be achieved by further measuring their small-angle scattering (dark-field) properties. This provides additional valuable diagnostic information on otherwise unresolved tissue microstructure. In our work, we have translated this wave-optical mechanism from the optical bench to a human-sized prototype CT system. This involved the integration of an interferometer into a clinical CT gantry and overcoming several associated challenges regarding vibrations, continuous gantry rotation, and large field of view. This development puts complementary X-ray contrast within reach for real-word medical applications.
          , 
            X-ray computed tomography (CT) is one of the most commonly used three-dimensional medical imaging modalities today. It has been refined over several decades, with the most recent innovations including dual-energy and spectral photon-counting technologies. Nevertheless, it has been discovered that wave-optical contrast mechanisms—beyond the presently used X-ray attenuation—offer the potential of complementary information, particularly on otherwise unresolved tissue microstructure. One such approach is dark-field imaging, which has recently been introduced and already demonstrated significantly improved radiological benefit in small-animal models, especially for lung diseases. Until now, however, dark-field CT could not yet be translated to the human scale and has been restricted to benchtop and small-animal systems, with scan durations of several minutes or more. This is mainly because the adaption and upscaling to the mechanical complexity, speed, and size of a human CT scanner so far remained an unsolved challenge. Here, we now report the successful integration of a Talbot–Lau interferometer into a clinical CT gantry and present dark-field CT results of a human-sized anthropomorphic body phantom, reconstructed from a single rotation scan performed in 1 s. Moreover, we present our key hardware and software solutions to the previously unsolved roadblocks, which so far have kept dark-field CT from being translated from the optical bench into a rapidly rotating CT gantry, with all its associated challenges like vibrations, continuous rotation, and large field of view. This development enables clinical dark-field CT studies with human patients in the near future.},
	language = {en},
	number = {8},
	urldate = {2022-03-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Viermetz, Manuel and Gustschin, Nikolai and Schmid, Clemens and Haeusele, Jakob and von Teuffenbach, Maximilian and Meyer, Pascal and Bergner, Frank and Lasser, Tobias and Proksa, Roland and Koehler, Thomas and Pfeiffer, Franz},
	month = feb,
	year = {2022},
	pages = {e2118799119},
	file = {Viermetz et al. - 2022 - Dark-field computed tomography reaches the human s.pdf:/home/david/Zotero/storage/UF7NGGY3/Viermetz et al. - 2022 - Dark-field computed tomography reaches the human s.pdf:application/pdf},
}

@article{stefanoiu_what_2020,
	title = {What about computational super-resolution in fluorescence {Fourier} light field microscopy?},
	volume = {28},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-28-11-16554},
	doi = {10.1364/OE.391189},
	abstract = {Recently, Fourier light ﬁeld microscopy was proposed to overcome the limitations in conventional light ﬁeld microscopy by placing a micro-lens array at the aperture stop of the microscope objective instead of the image plane. In this way, a collection of orthographic views from diﬀerent perspectives are directly captured. When inspecting ﬂuorescent samples, the sensitivity and noise of the sensors are a major concern and large sensor pixels are required to cope with low-light conditions, which implies under-sampling issues. In this context, we analyze the sampling patterns in Fourier light ﬁeld microscopy to understand to what extent computational super-resolution can be triggered during deconvolution in order to improve the resolution of the 3D reconstruction of the imaged data.},
	language = {en},
	number = {11},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Stefanoiu, Anca and Scrofani, Gabriele and Saavedra, Genaro and Martínez-Corral, Manuel and Lasser, Tobias},
	month = may,
	year = {2020},
	pages = {16554},
	file = {Stefanoiu et al. - 2020 - What about computational super-resolution in fluor.pdf:/home/david/Zotero/storage/5BILIN4C/Stefanoiu et al. - 2020 - What about computational super-resolution in fluor.pdf:application/pdf},
}

@article{stefanoiu_artifact-free_2019,
	title = {Artifact-free deconvolution in light field microscopy},
	volume = {27},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-27-22-31644},
	doi = {10.1364/OE.27.031644},
	abstract = {The sampling patterns of the light ﬁeld microscope (LFM) are highly depth-dependent, which implies non-uniform recoverable lateral resolution across depth. Moreover, reconstructions using state-of-the-art approaches suﬀer from strong artifacts at axial ranges, where the LFM samples the light ﬁeld at a coarse rate. In this work, we analyze the sampling patterns of the LFM, and introduce a ﬂexible light ﬁeld point spread function model (LFPSF) to cope with arbitrary LFM designs. We then propose a novel aliasing-aware deconvolution scheme to address the sampling artifacts. We demonstrate the high potential of the proposed method on real experimental data.},
	language = {en},
	number = {22},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Stefanoiu, Anca and Page, Josue and Symvoulidis, Panagiotis and Westmeyer, Gil G. and Lasser, Tobias},
	month = oct,
	year = {2019},
	pages = {31644},
	file = {Stefanoiu et al. - 2019 - Artifact-free deconvolution in light field microsc.pdf:/home/david/Zotero/storage/QDEF2JHT/Stefanoiu et al. - 2019 - Artifact-free deconvolution in light field microsc.pdf:application/pdf},
}

@article{wieczorek_brain_2018,
	title = {Brain {Connectivity} {Exposed} by {Anisotropic} {X}-ray {Dark}-field {Tomography}},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-32023-y},
	doi = {10.1038/s41598-018-32023-y},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Wieczorek, Matthias and Schaff, Florian and Jud, Christoph and Pfeiffer, Daniela and Pfeiffer, Franz and Lasser, Tobias},
	month = dec,
	year = {2018},
	pages = {14345},
	file = {Wieczorek et al. - 2018 - Brain Connectivity Exposed by Anisotropic X-ray Da.pdf:/home/david/Zotero/storage/FAUJDD33/Wieczorek et al. - 2018 - Brain Connectivity Exposed by Anisotropic X-ray Da.pdf:application/pdf},
}

@article{sharma_design_2017,
	title = {Design of {Acquisition} {Schemes} and {Setup} {Geometry} for {Anisotropic} {X}-ray {Dark}-{Field} {Tomography} ({AXDT})},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-03329-0},
	doi = {10.1038/s41598-017-03329-0},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Sharma, Y. and Schaff, F. and Wieczorek, M. and Pfeiffer, F. and Lasser, T.},
	month = dec,
	year = {2017},
	pages = {3195},
	file = {Sharma et al. - 2017 - Design of Acquisition Schemes and Setup Geometry f.pdf:/home/david/Zotero/storage/JK6UFIHT/Sharma et al. - 2017 - Design of Acquisition Schemes and Setup Geometry f.pdf:application/pdf},
}

@article{pfeiffer_hard-x-ray_2008,
	title = {Hard-{X}-ray dark-field imaging using a grating interferometer},
	volume = {7},
	issn = {1476-1122, 1476-4660},
	url = {http://www.nature.com/articles/nmat2096},
	doi = {10.1038/nmat2096},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Nature Materials},
	author = {Pfeiffer, F. and Bech, M. and Bunk, O. and Kraft, P. and Eikenberry, E. F. and Brönnimann, Ch. and Grünzweig, C. and David, C.},
	month = feb,
	year = {2008},
	pages = {134--137},
	file = {Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:/home/david/Zotero/storage/JC5CASMM/Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:application/pdf},
}

@article{pfeiffer_phase_2006,
	title = {Phase retrieval and differential phase-contrast imaging with low-brilliance {X}-ray sources},
	volume = {2},
	issn = {1745-2473, 1745-2481},
	url = {http://www.nature.com/articles/nphys265},
	doi = {10.1038/nphys265},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Nature Physics},
	author = {Pfeiffer, Franz and Weitkamp, Timm and Bunk, Oliver and David, Christian},
	month = apr,
	year = {2006},
	pages = {258--261},
	file = {Pfeiffer et al. - 2006 - Phase retrieval and differential phase-contrast im.pdf:/home/david/Zotero/storage/G94ZMJNE/Pfeiffer et al. - 2006 - Phase retrieval and differential phase-contrast im.pdf:application/pdf},
}

@book{natterer_mathematics_1986,
	address = {Wiesbaden},
	title = {The {Mathematics} of {Computerized} {Tomography}},
	url = {http://link.springer.com/10.1007/978-3-663-01409-6},
	language = {en},
	urldate = {2022-03-25},
	publisher = {Vieweg+Teubner Verlag},
	author = {Natterer, F.},
	year = {1986},
	doi = {10.1007/978-3-663-01409-6},
	file = {Natterer - 1986 - The Mathematics of Computerized Tomography.pdf:/home/david/Zotero/storage/VAXL3RF4/Natterer - 1986 - The Mathematics of Computerized Tomography.pdf:application/pdf},
}

@article{hahn_statistical_2015,
	title = {Statistical iterative reconstruction algorithm for {X}-ray phase-contrast {CT}},
	volume = {5},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep10452},
	doi = {10.1038/srep10452},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Scientific Reports},
	author = {Hahn, Dieter and Thibault, Pierre and Fehringer, Andreas and Bech, Martin and Koehler, Thomas and Pfeiffer, Franz and Noël, Peter B.},
	month = sep,
	year = {2015},
	pages = {10452},
	file = {Hahn et al. - 2015 - Statistical iterative reconstruction algorithm for.pdf:/home/david/Zotero/storage/PXQS9TA9/Hahn et al. - 2015 - Statistical iterative reconstruction algorithm for.pdf:application/pdf},
}

@phdthesis{hahn_statistical_2014,
	address = {München},
	type = {Dissertation},
	title = {Statistical {Iterative} {Reconstruction} for {X}-ray {Phase}-{Contrast} {Computed} {Tomography}},
	school = {Technische Universität München},
	author = {Hahn, Dieter},
	year = {2014},
	keywords = {X-ray imaging, computed tomography, reconstruction, differential phase contrast},
	file = {Hahn - 2014 - Statistical Iterative Reconstruction for X-ray Pha.pdf:/home/david/Zotero/storage/82DSI6KV/Hahn - 2014 - Statistical Iterative Reconstruction for X-ray Pha.pdf:application/pdf},
}

@phdthesis{hehn_model-based_2020,
	address = {München},
	type = {Dissertation},
	title = {Model-{Based} {Iterative} {Reconstruction} for {X}-ray {Computed} {Tomography} {Using} {Attenuation} and {Propagation}-{Based} {Phase} {Contrast}},
	school = {Technische Universität München},
	author = {Hehn, Lorenz},
	year = {2020},
	file = {Hehn - 2020 - Model-Based Iterative Reconstruction for X-ray Com.pdf:/home/david/Zotero/storage/6GE3FT5F/Hehn - 2020 - Model-Based Iterative Reconstruction for X-ray Com.pdf:application/pdf},
}

@article{mcdonald_advanced_2009,
	title = {Advanced phase-contrast imaging using a grating interferometer},
	volume = {16},
	issn = {0909-0495},
	url = {http://scripts.iucr.org/cgi-bin/paper?S0909049509017920},
	doi = {10.1107/S0909049509017920},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Journal of Synchrotron Radiation},
	author = {McDonald, Samuel Alan and Marone, Federica and Hintermüller, Christoph and Mikuljan, Gordan and David, Christian and Pfeiffer, Franz and Stampanoni, Marco},
	month = jul,
	year = {2009},
	pages = {562--572},
	file = {McDonald et al. - 2009 - Advanced phase-contrast imaging using a grating in.pdf:/home/david/Zotero/storage/Y8932Y92/McDonald et al. - 2009 - Advanced phase-contrast imaging using a grating in.pdf:application/pdf},
}

@article{shanblatt_forward_2019,
	title = {Forward model for propagation-based x-ray phase contrast imaging in parallel- and cone-beam geometry},
	volume = {27},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-27-4-4504},
	doi = {10.1364/OE.27.004504},
	abstract = {We demonstrate a fast, flexible, and accurate paraxial wave propagation model to serve as a forward model for propagation-based X-ray phase contrast imaging (XPCI) in parallel-beam or cone-beam geometry. This model incorporates geometric cone-beam effects into the multi-slice beam propagation method. It enables rapid prototyping and is well suited to serve as a forward model for propagation-based X-ray phase contrast tomographic reconstructions. Furthermore, it is capable of modeling arbitrary objects, including those that are strongly or multi-scattering. Simulation studies were conducted to compare our model to other forward models in the X-ray regime, such as the Mie and full-wave Rytov solutions.},
	language = {en},
	number = {4},
	urldate = {2022-03-25},
	journal = {Optics Express},
	author = {Shanblatt, Elisabeth R. and Sung, Yongjin and Gupta, Rajiv and Nelson, Brandon J. and Leng, Shuai and Graves, William S. and McCollough, Cynthia H.},
	month = feb,
	year = {2019},
	pages = {4504},
	file = {Shanblatt et al. - 2019 - Forward model for propagation-based x-ray phase co.pdf:/home/david/Zotero/storage/EVYXJM7C/Shanblatt et al. - 2019 - Forward model for propagation-based x-ray phase co.pdf:application/pdf},
}

@article{pfeiffer_tomographic_2007,
	title = {Tomographic reconstruction of three-dimensional objects from hard {X}-ray differential phase contrast projection images},
	volume = {580},
	issn = {01689002},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168900207012910},
	doi = {10.1016/j.nima.2007.06.104},
	abstract = {We report on a method for tomographic phase contrast imaging of centimeter sized objects. As opposed to existing techniques, our approach can be used with low-brilliance, lab based X-ray sources and thus is of interest to a wide range of applications in medicine, biology, and non-destructive testing. The work is based on the recent development of a hard X-ray grating interferometer, which has been demonstrated to yield differential phase contrast projection images. Here we particularly focus on how this method can be used for tomographic reconstructions using ﬁltered backprojection algorithms to yield quantitative volumetric information of the real part of the samples’s refractive index.},
	language = {en},
	number = {2},
	urldate = {2022-03-25},
	journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
	author = {Pfeiffer, F. and Bunk, O. and Kottler, C. and David, C.},
	month = oct,
	year = {2007},
	pages = {925--928},
	file = {Pfeiffer et al. - 2007 - Tomographic reconstruction of three-dimensional ob.pdf:/home/david/Zotero/storage/WRRB2EGW/Pfeiffer et al. - 2007 - Tomographic reconstruction of three-dimensional ob.pdf:application/pdf},
}

@book{herman_fundamentals_2009,
	address = {London, UNITED KINGDOM},
	title = {Fundamentals of {Computerized} {Tomography}: {Image} {Reconstruction} from {Projections}},
	isbn = {978-1-84628-723-7},
	shorttitle = {Fundamentals of {Computerized} {Tomography}},
	url = {http://ebookcentral.proquest.com/lib/munchentech/detail.action?docID=602886},
	abstract = {This revised and updated second edition - now with two new chapters - is the only book to give a comprehensive overview of computer algorithms for image reconstruction. It covers the fundamentals of computerized tomography, including all the computational and mathematical procedures underlying data collection, image reconstruction and image display. Among the new topics covered are: spiral CT, fully 3D positron emission tomography, the linogram mode of backprojection, and state of the art 3D imaging results. It also includes two new chapters on comparative statistical evaluation of the 2D reconstruction algorithms and alternative approaches to image reconstruction.},
	urldate = {2022-04-02},
	publisher = {Springer London, Limited},
	author = {Herman, Gabor T.},
	year = {2009},
	keywords = {Image reconstruction},
	file = {ProQuest Ebook Snapshot:/home/david/Zotero/storage/MZBWLG7K/detail.html:text/html},
}

@article{solmon_x-ray_1976,
	title = {The {X}-ray transform},
	volume = {56},
	issn = {0022247X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0022247X76900081},
	doi = {10.1016/0022-247X(76)90008-1},
	language = {en},
	number = {1},
	urldate = {2022-04-02},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Solmon, Donald C},
	month = oct,
	year = {1976},
	pages = {61--83},
	file = {Solmon - 1976 - The X-ray transform.pdf:/home/david/Zotero/storage/5XMCRG8J/Solmon - 1976 - The X-ray transform.pdf:application/pdf},
}

@article{hyndman_another_2006,
	title = {Another look at measures of forecast accuracy},
	volume = {22},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207006000239},
	doi = {10.1016/j.ijforecast.2006.03.001},
	abstract = {We discuss and compare measures of accuracy of univariate time series forecasts. The methods used in the M-competition and the M3-competition, and many of the measures recommended by previous authors on this topic, are found to be degenerate in commonly occurring situations. Instead, we propose that the mean absolute scaled error become the standard measure for comparing forecast accuracy across multiple time series.},
	language = {en},
	number = {4},
	urldate = {2022-04-04},
	journal = {International Journal of Forecasting},
	author = {Hyndman, Rob J. and Koehler, Anne B.},
	month = oct,
	year = {2006},
	pages = {679--688},
	file = {Hyndman and Koehler - 2006 - Another look at measures of forecast accuracy.pdf:/home/david/Zotero/storage/P2NIPZWS/Hyndman and Koehler - 2006 - Another look at measures of forecast accuracy.pdf:application/pdf},
}

@article{pontius_components_2008,
	title = {Components of information for multiple resolution comparison between maps that share a real variable},
	volume = {15},
	issn = {1352-8505, 1573-3009},
	url = {http://link.springer.com/10.1007/s10651-007-0043-y},
	doi = {10.1007/s10651-007-0043-y},
	language = {en},
	number = {2},
	urldate = {2022-04-04},
	journal = {Environmental and Ecological Statistics},
	author = {Pontius, Robert Gilmore and Thontteh, Olufunmilayo and Chen, Hao},
	month = jun,
	year = {2008},
	pages = {111--142},
	file = {Pontius et al. - 2008 - Components of information for multiple resolution .pdf:/home/david/Zotero/storage/MLXXRXSB/Pontius et al. - 2008 - Components of information for multiple resolution .pdf:application/pdf},
}

@phdthesis{toft_radon_1996,
	title = {The {Radon} {Transform} - {Theory} and {Implementation}},
	shorttitle = {The {Radon} {Transform}},
	language = {en},
	school = {Technical University of Denmark},
	author = {Toft, Peter Aundal},
	year = {1996},
	file = {Aundal - The Radon Transform - Theory and Implementation.pdf:/home/david/Zotero/storage/EULSBVA8/Aundal - The Radon Transform - Theory and Implementation.pdf:application/pdf},
}

@article{lewitt_multidimensional_1990,
	title = {Multidimensional digital image representations using generalized {Kaiser}–{Bessel} window functions},
	volume = {7},
	issn = {1084-7529, 1520-8532},
	url = {https://opg.optica.org/abstract.cfm?URI=josaa-7-10-1834},
	doi = {10.1364/JOSAA.7.001834},
	language = {en},
	number = {10},
	urldate = {2022-04-04},
	journal = {Journal of the Optical Society of America A},
	author = {Lewitt, Robert M.},
	month = oct,
	year = {1990},
	pages = {1834},
}

@article{marabini_3d_1998,
	title = {{3D} reconstruction in electron microscopy using {ART} with smooth spherically symmetric volume elements (blobs)},
	volume = {72},
	issn = {03043991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304399197001277},
	doi = {10.1016/S0304-3991(97)00127-7},
	abstract = {Algebraic reconstruction techniques (ART) are iterative procedures for solving systems of linear equations. They have been used in tomography to recover objects from their projections. In this work we apply an ART approach in which the basis functions used to describe the objects are not based on voxels, but are much smoother functions named “blobs”. The data collection studied in this work follows the so-called “conical tilt geometry” that is commonly used in many applications of three-dimensional electron microscopy of biological macromolecules. The performance of ART with blobs is carefully compared with a currently well-known three dimensional (3D) reconstruction algorithm (weighted backprojection) using a methodology which assigns a level of statistical signiﬁcance to a claim of relative superiority of one algorithm over another for a particular task. The conclusion we reach is that ART with blobs produces high-quality reconstructions and is, in particular, superior to weighted backprojection in recovering features along the “vertical” direction. For the exact implementation recommended in this paper, the computational costs of ART are almost an order of magnitude smaller than those of WBP. 1998 Elsevier Science B.V. All rights reserved.},
	language = {en},
	number = {1-2},
	urldate = {2022-04-04},
	journal = {Ultramicroscopy},
	author = {Marabini, Roberto and Herman, Gabor T and Carazo, José M},
	month = apr,
	year = {1998},
	pages = {53--65},
	file = {Marabini et al. - 1998 - 3D reconstruction in electron microscopy using ART.pdf:/home/david/Zotero/storage/DZJ75P6L/Marabini et al. - 1998 - 3D reconstruction in electron microscopy using ART.pdf:application/pdf},
}

@article{chlewicki_noise_2004,
	title = {Noise reduction and convergence of {Bayesian} algorithms with blobs based on the {Huber} function and median root prior},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/20/004},
	doi = {10.1088/0031-9155/49/20/004},
	abstract = {Iterative image reconstruction algorithms have the potential to produce low noise images. Early stopping of the iteration process is problematic because some features of the image may converge slowly. On the other hand, there may be noise build-up with increased number of iterations. Therefore, we examined the stabilizing effect of using two different prior functions as well as image representation by blobs so that the number of iterations could be increased without noise build-up. Reconstruction was performed of simulated phantoms and of real data acquired by positron emission tomography. Image quality measures were calculated for images reconstructed with or without priors. Both priors stabilized the iteration process. The first prior based on the Huber function reduced the noise without significant loss of contrast recovery of small spots, but the drawback of the method was the difficulty in finding optimal values of two free parameters. The second method based on a median root prior has only one Bayesian parameter which was easy to set, but it should be taken into account that the image resolution while using that prior has to be chosen sufficiently high not to cause the complete removal of small spots. In conclusion, the Huber penalty function gives accurate and low noise images, but it may be difficult to determine the parameters. The median root prior method is not quite as accurate but may be used if image resolution is increased.},
	language = {en},
	number = {20},
	urldate = {2022-04-04},
	journal = {Physics in Medicine and Biology},
	author = {Chlewicki, W. and Hermansen, F. and Hansen, S. B.},
	month = sep,
	year = {2004},
	note = {Publisher: IOP Publishing},
	pages = {4717--4730},
}

@article{wang_3d_2004,
	title = {{3D} {RBI}-{EM} reconstruction with spherically-symmetric basis function for {SPECT} rotating slat collimator},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/11/011},
	doi = {10.1088/0031-9155/49/11/011},
	abstract = {A single photon emission computed tomography (SPECT) rotating slat collimator with strip detector acquires distance-weighted plane integral data, along with the attenuation factor and distance-dependent detector response. In order to image a 3D object, the slat collimator device has first to spin around its axis and then rotate around the object to produce 3D projection measurements. Compared to the slice-by-slice 2D reconstruction for the parallel-hole collimator and line integral data, a more complex 3D reconstruction is needed for the slat collimator and plane integral data. In this paper, we propose a 3D RBI-EM reconstruction algorithm with spherically-symmetric basis function, also called ‘blobs’, for the slat collimator. It has a closed and spherically symmetric analytical expression for the 3D Radon transform, which makes it easier to compute the plane integral than the voxel. It is completely localized in the spatial domain and nearly band-limited in the frequency domain. Its size and shape can be controlled by several parameters to have desired reconstructed image quality. A mathematical lesion phantom study has demonstrated that the blob reconstruction can achieve better contrast-noise trade-offs than the voxel reconstruction without greatly degrading the image resolution. A real lesion phantom study further confirmed this and showed that a slat collimator with CZT detector has better image quality than the conventional parallel-hole collimator with NaI detector. The improvement might be due to both the slat collimation and the better energy resolution of the CZT detector.},
	language = {en},
	number = {11},
	urldate = {2022-04-04},
	journal = {Physics in Medicine and Biology},
	author = {Wang, Wenli and Hawkins, William and Gagnon, Daniel},
	month = may,
	year = {2004},
	note = {Publisher: IOP Publishing},
	pages = {2273--2292},
}

@article{yendiki_comparison_2004,
	title = {A comparison of rotation- and blob-based system models for {3D} {SPECT} with depth-dependent detector response},
	volume = {49},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/49/11/003},
	doi = {10.1088/0031-9155/49/11/003},
	abstract = {We compare two different implementations of a 3D SPECT system model for iterative reconstruction, both of which compensate for non-uniform photon attenuation and depth-dependent system response. One implementation performs fast rotation of images represented using a basis of rectangular voxels, whereas the other represents images using a basis of rotationally symmetric volume elements. In our simulations the blob-based approach was found to slightly outperform the rotation-based one in terms of the bias-variance trade-off in the reconstructed images. Their difference can be significant, however, in terms of computational load. The rotation-based method is faster for many typical SPECT reconstruction problems, but the blob-based one can be better-suited to cases where the reconstruction algorithm needs to process one volume element at a time.},
	language = {en},
	number = {11},
	urldate = {2022-04-04},
	journal = {Physics in Medicine and Biology},
	author = {Yendiki, A. and Fessler, J. A.},
	month = may,
	year = {2004},
	note = {Publisher: IOP Publishing},
	pages = {2157--2168},
	file = {Full Text:/home/david/Zotero/storage/SHSW9Q4Q/Yendiki and Fessler - 2004 - A comparison of rotation- and blob-based system mo.pdf:application/pdf},
}

@article{jacobs_comparative_1999,
	title = {A {Comparative} {Study} of {2D} {Reconstruction} {Algorithms} {Using} {Pixels} and {Optimized} {Blobs} {Applied} to {Fourier} {Rebinned} {3D} {Data}},
	author = {Jacobs, Filip and Matej, Samuel and Lewitt, Robert},
	month = jan,
	year = {1999},
	file = {Jacobs et al. - A Comparative Study of 2D Reconstruction Algorithm.pdf:/home/david/Zotero/storage/P8GQ6UFM/Jacobs et al. - A Comparative Study of 2D Reconstruction Algorithm.pdf:application/pdf},
}

@inproceedings{carvalho_helical_2003,
	title = {Helical {CT} reconstruction from wide cone-beam angle data using {ART}},
	doi = {10.1109/SIBGRA.2003.1241031},
	abstract = {We report on new results on the use of algebraic reconstruction techniques (ART) for reconstructing from helical cone-beam computerized tomography (CT) data. We investigate two variants of ART for this task: a standard one that considers a single ray in an iterative step and a block version which groups several cone-beam projections when calculating an iterative step. Both algorithms were implemented using modified Kaiser-Bessel window functions, also known as blobs, placed on the body-centered cubic (bcc) grid. The algorithms were used to reconstruct a modified 3D Shepp-Logan phantom from data collected for the PI-geometry for two different maximum cone-beam angles (/spl plusmn/9.46/spl deg/ and /spl plusmn/18.43/spl deg/). Both scattering and quantum noise (for three different noise levels) were introduced to create noisy projections. The results presented here (for both noiseless and noisy data sets) point to the fact that, as opposed to filtered backprojection algorithms, the quality of the reconstructions produced by the ART methods does not suffer from the increase in the cone-beam angle.},
	booktitle = {16th {Brazilian} {Symposium} on {Computer} {Graphics} and {Image} {Processing} ({SIBGRAPI} 2003)},
	author = {Carvalho, B.M. and Herman, G.T.},
	month = oct,
	year = {2003},
	note = {ISSN: 1530-1834},
	keywords = {Computed tomography, Image reconstruction, X-ray imaging, Iterative algorithms, Noise level, Object detection, Sensor arrays, Subspace constraints, X-ray detection, X-ray detectors},
	pages = {363--370},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/MT5KSZ99/1241031.html:text/html},
}

@article{isola_motion-compensated_2008,
	title = {Motion-compensated iterative cone-beam {CT} image reconstruction with adapted blobs as basis functions},
	volume = {53},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/0031-9155/53/23/009},
	doi = {10.1088/0031-9155/53/23/009},
	abstract = {This paper presents a three-dimensional method to reconstruct moving objects from cone-beam X-ray projections using an iterative reconstruction algorithm and a given motion vector field. For the image representation, adapted blobs are used, which can be implemented efficiently as basis functions. Iterative reconstruction requires the calculation of line integrals (forward projections) through the image volume, which are compared with the actual measurements to update the image volume. In the existence of a divergent motion vector field, a change in the volumes of the blobs has to be taken into account in the forward and backprojections. An efficient method to calculate the line integral through the adapted blobs is proposed. It solves the problem, how to compensate for the divergence in the motion vector field on a grid of basis functions. The method is evaluated on two phantoms, which are subject to three different known motions. Moreover, a motion-compensated filtered back-projection reconstruction method is used, and the reconstructed images are compared. Using the correct motion vector field with the iterative motion-compensated reconstruction, sharp images are obtained, with a quality that is significantly better than gated reconstructions.},
	language = {en},
	number = {23},
	urldate = {2022-04-04},
	journal = {Physics in Medicine and Biology},
	author = {Isola, A. A. and Ziegler, A. and Koehler, T. and Niessen, W. J. and Grass, M.},
	month = nov,
	year = {2008},
	note = {Publisher: IOP Publishing},
	pages = {6777--6797},
}

@inproceedings{wu_breast_2010,
	address = {Berlin, Heidelberg},
	title = {Breast {Tomosynthesis} {Reconstruction} {Using} a {Grid} of {Blobs} with {Projection} {Matrices}},
	isbn = {978-3-642-13666-5},
	doi = {10.1007/978-3-642-13666-5_33},
	abstract = {Spherically symmetric basis functions (blobs) are alternatives to the more conventional cubic voxels for image reconstruction in breast tomosynthesis. The volume representation and its projection views (PV) are essential components of iterative algorithms for image reconstruction from data collected from an area detector. This paper addresses the forward projection and backprojection process of three-dimensional (3D) breast reconstruction obtained from cone-beam scans using tomosynthesis imaging equipment. The smoothness of the blob elements allows more realistic modeling of the breast, and the rotational symmetry of the elements leads to more efficient calculation of both directional projection of the represented volume, as required in iterative reconstruction techniques. The combination of blob volume elements and the projection matrix method improves tomosynthesis reconstruction in both accuracy and speed.},
	language = {en},
	booktitle = {Digital {Mammography}},
	publisher = {Springer},
	author = {Wu, Gang and Mainprize, James G. and Yaffe, Martin J.},
	editor = {Martí, Joan and Oliver, Arnau and Freixenet, Jordi and Martí, Robert},
	year = {2010},
	pages = {243--250},
}

@misc{noauthor_stdcyl_bessel_i_nodate,
	title = {std::cyl\_bessel\_i, std::cyl\_bessel\_if, std::cyl\_bessel\_il - cppreference.com},
	url = {https://en.cppreference.com/w/cpp/numeric/special_functions/cyl_bessel_i},
	urldate = {2022-04-04},
	file = {std\:\:cyl_bessel_i, std\:\:cyl_bessel_if, std\:\:cyl_bessel_il - cppreference.com:/home/david/Zotero/storage/I9SLYE3U/cyl_bessel_i.html:text/html},
}

@misc{noauthor_c_nodate,
	title = {C++ {Mathematical} {Special} {Functions} {Proposal}},
	url = {http://open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1422.html},
	urldate = {2022-04-04},
	file = {C++ Mathematical Special Functions Proposal:/home/david/Zotero/storage/TDXJDBJT/n1422.html:text/html},
}

@inproceedings{guedon_b-spline_1991,
	title = {B-spline {FBP} reconstructions for {SPECT}},
	doi = {10.1109/NSSMIC.1991.259085},
	abstract = {The authors describe a generalization of the filtered backprojection (FBP) algorithm based on a pixel intensity distribution (PIDM) chosen in spline spaces. A set of reconstructions is obtained according to the degree of spline. The single photon emission computed tomography (SPECT) implementation is discussed in order to provide efficient algorithms and take into account the limitations induced by the gamma -camera. A study based on Jaszczack phantom reconstructions shows different ways to distribute the acquired photons onto projections and their implications in terms of algorithms.{\textless}{\textgreater}},
	booktitle = {Conference {Record} of the 1991 {IEEE} {Nuclear} {Science} {Symposium} and {Medical} {Imaging} {Conference}},
	author = {Guedon, J.-P. and Barker, C. and Bizais, Y.},
	month = nov,
	year = {1991},
	keywords = {Biomedical imaging, Kernel, Spline, Image reconstruction, Pixel, Image sampling, Filters, Fourier transforms, Hospitals, Single photon emission computed tomography},
	pages = {1065--1069 vol.2},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/F44FLIDB/259085.html:text/html},
}

@inproceedings{reutter_fully_2007,
	title = {Fully 4-{D} dynamic cardiac {SPECT} image reconstruction using spatiotemporal {B}-spline voxelization},
	volume = {6},
	doi = {10.1109/NSSMIC.2007.4437048},
	abstract = {We developed fully 4-D penalized least-squares reconstruction methods that use overlapping multiresolution B-splines to represent radiopharmaceutical distributions that vary smoothly in space and time in human dynamic cardiac SPECT images. This approach does not require segmentation, and improves signal-to-noise and increases computational efficiency compared to methods based on small, non-overlapping cube-shaped voxels and rectangular time windows. The support of spatial B-splines was extended into the time dimension to obtain estimates of time-activity curves directly from projections for a human dynamic Tc-99m-sestamibi cardiac SPECT/CT study. Projection data were acquired in 1-sec time frames with an angular step of 5 deg per frame on a GE millennium VH Hawk-eye SPECT/CT scanner. Attenuation and depth-dependent collimator response were modeled, but not scatter. The 4-D B-splines were piecewise trilinear in space and piecewise quadratic in time. The splines were organized on a 3-D spatial grid that provided uniform sampling of 17.7 mm in each dimension, and on a 1-D temporal grid that provided nonuniform sampling intervals of 0-4, 4-15, 15-48, and 48-144 sec during the first two gantry rotations. The use of nonuniform time sampling with 4-D B-splines that varied quadratically in time yielded smooth time-activity curves that captured the relatively fast rise and fall of tracer in the right and left blood chambers, as well as uptake and retention of tracer in the left ventricular myocardium. These methods can also be applied to dynamic PET.},
	booktitle = {2007 {IEEE} {Nuclear} {Science} {Symposium} {Conference} {Record}},
	author = {Reutter, Bryan W. and Gullberg, Grant T. and Boutchko, Rostyslav and Balakrishnan, Karthikayan and Botvinick, Elias H. and Huesman, Ronald H.},
	month = oct,
	year = {2007},
	note = {ISSN: 1082-3654},
	keywords = {Sampling methods, Spline, Computed tomography, Image reconstruction, Reconstruction algorithms, Dynamic SPECT, fully fourdimensional reconstruction, Humans, Image resolution, Index Terms, penalized least-squares, Signal resolution, Spatial resolution, spatiotemporal B-splines, Spatiotemporal phenomena, SPECT/CT},
	pages = {4217--4221},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/4SE9NMU3/4437048.html:text/html},
}

@article{despres_review_2017,
	title = {A review of {GPU}-based medical image reconstruction},
	volume = {42},
	issn = {1724-191X},
	doi = {10.1016/j.ejmp.2017.07.024},
	abstract = {Tomographic image reconstruction is a computationally demanding task, even more so when advanced models are used to describe a more complete and accurate picture of the image formation process. Such advanced modeling and reconstruction algorithms can lead to better images, often with less dose, but at the price of long calculation times that are hardly compatible with clinical workflows. Fortunately, reconstruction tasks can often be executed advantageously on Graphics Processing Units (GPUs), which are exploited as massively parallel computational engines. This review paper focuses on recent developments made in GPU-based medical image reconstruction, from a CT, PET, SPECT, MRI and US perspective. Strategies and approaches to get the most out of GPUs in image reconstruction are presented as well as innovative applications arising from an increased computing capacity. The future of GPU-based image reconstruction is also envisioned, based on current trends in high-performance computing.},
	language = {eng},
	journal = {Physica medica: PM: an international journal devoted to the applications of physics to medicine and biology: official journal of the Italian Association of Biomedical Physics (AIFB)},
	author = {Després, Philippe and Jia, Xun},
	month = oct,
	year = {2017},
	pmid = {29173924},
	keywords = {Tomography, Medical imaging, Tomographic reconstruction, Humans, Algorithms, Computer Graphics, Computers, Graphics Processing Unit (GPU), Image Processing, Computer-Assisted, Software},
	pages = {76--92},
}

@inproceedings{niu_fully_2010,
	title = {Fully 5d reconstruction of gated dynamic cardiac {SPECT} images using temporal {B}-splines},
	doi = {10.1109/ISBI.2010.5490311},
	abstract = {In previous work we explored a reconstruction approach based on B-spline modeling for dynamic cardiac images from a gated acquisition in single photon emission computed tomography (SPECT), of which the goal is to obtain a single sequence showing both cardiac motion and kinetic tracer distribution change over time simultaneously. In this work we further develop this approach by extending it to reconstruction of fully five-dimensional (5D) images. Besides quantifying its reconstruction accuracy, we also demonstrate the feasibility of using kinetic information from reconstructed dynamic images with this approach for differentiating defects from normal cardiac perfusion. The proposed approach is evaluated using a dynamic version of the 4D NURBS-based cardiac-torso (NCAT) phantom to simulate a gated SPECT perfusion acquisition with Tc99m Teboroxime. Our results demonstrate that, despite the greatly underdetermined nature of the problem, the 5D procedure can lead to faithful reconstruction of gated dynamic images for both cardiac motion and perfusion defect detection.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Biomedical} {Imaging}: {From} {Nano} to {Macro}},
	author = {Niu, Xiaofeng and Yang, Yongyi and Jin, Mingwu},
	month = apr,
	year = {2010},
	note = {ISSN: 1945-8452},
	keywords = {Biomedical imaging, Spline, Image reconstruction, Medical treatment, Reconstruction algorithms, Dynamic SPECT, 5D reconstruction, B-spline, Data acquisition, gated SPECT, Kinetic theory, motion compensation, Motion compensation, Myocardium, Radiology},
	pages = {460--463},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/SB6TLQC6/5490311.html:text/html},
}

@article{tran_inverse_2014,
	title = {Inverse {Problem} {Approach} for the {Alignment} of {Electron} {Tomographic} {Series}},
	volume = {69},
	copyright = {© 2013, IFP Energies nouvelles},
	issn = {1294-4475, 1953-8189},
	url = {https://ogst.ifpenergiesnouvelles.fr/articles/ogst/abs/2014/02/ogst120175/ogst120175.html},
	doi = {10.2516/ogst/2013116},
	abstract = {In the refining industry, morphological measurements of particles have become an essential part in the characterization catalyst supports. Through these parameters, one can infer the specific physicochemical properties of the studied materials. One of the main acquisition techniques is electron tomography (or nanotomography). 3D volumes are reconstructed from sets of projections from different angles made by a Transmission Electron Microscope (TEM). This technique provides a real three-dimensional information at the nanometric scale. A major issue in this method is the misalignment of the projections that contributes to the reconstruction. The current alignment techniques usually employ fiducial markers such as gold particles for a correct alignment of the images. When the use of markers is not possible, the correlation between adjacent projections is used to align them. However, this method sometimes fails. In this paper, we propose a new method based on the inverse problem approach where a certain criterion is minimized using a variant of the Nelder and Mead simplex algorithm. The proposed approach is composed of two steps. The first step consists of an initial alignment process, which relies on the minimization of a cost function based on robust statistics measuring the similarity of a projection to its previous projections in the series. It reduces strong shifts resulting from the acquisition between successive projections. In the second step, the pre-registered projections are used to initialize an iterative alignment-refinement process which alternates between (i) volume reconstructions and (ii) registrations of measured projections onto simulated projections computed from the volume reconstructed in (i). At the end of this process, we have a correct reconstruction of the volume, the projections being correctly aligned. Our method is tested on simulated data and shown to estimate accurately the translation, rotation and scale of arbitrary transforms. We have successfully tested our method with real projections of different catalyst supports.},
	language = {en},
	number = {2},
	urldate = {2022-04-04},
	journal = {Oil \& Gas Science and Technology – Revue d’IFP Energies nouvelles},
	author = {Tran, V.-D. and Moreaud, M. and Thiébaut, É and Denis, L. and Becker, J. M.},
	year = {2014},
	note = {Number: 2
Publisher: Technip},
	pages = {279--291},
	file = {Snapshot:/home/david/Zotero/storage/NA6V2LNV/ogst120175.html:text/html;Full Text PDF:/home/david/Zotero/storage/4RQXL277/Tran et al. - 2014 - Inverse Problem Approach for the Alignment of Elec.pdf:application/pdf},
}

@inproceedings{tran_robust_2013,
	title = {Robust registration of electron tomography projections without fiducial markers},
	volume = {8657},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8657/86570R/Robust-registration-of-electron-tomography-projections-without-fiducial-markers/10.1117/12.2001128.full},
	doi = {10.1117/12.2001128},
	abstract = {A major issue in electron tomography is the misalignment of the projections contributing to the reconstruction. The current alignment techniques currently use fiducial markers such as gold particles. When the use of markers is not possible, the accurate alignment of the projections is a challenge. We describe a new method for the alignment of transmission electron microscopy (TEM) images series without the need of fiducial markers. The proposed approach is composed of two steps. The first step consists of an initial alignment process, which relies on the minimization of a cost function based on robust statistics measuring the similarity of a projection to its previous projections in the series. It reduces strong shifts resulting from the acquisition between successive projections. The second step aligns the projections finely. The issue is formalized as an inverse problem. The pre­ registered projections are used to initialize an iterative alignment-refinement process which alternates between (i) volume reconstructions and (ii) registrations of measured projections onto simulated projections computed from the volume reconstructed in (i). The accuracy of our method is very satisfying; we illustrate it on simulated data and real projections of different zeolite supports catalyst.},
	urldate = {2022-04-04},
	booktitle = {Computational {Imaging} {XI}},
	publisher = {SPIE},
	author = {Tran, Viet-Dung and Moreaud, Maxime and Thiébaut, Éric and Dénis, Loïc and Becker, Jean-Marie},
	month = feb,
	year = {2013},
	pages = {190--198},
}

@article{nichols_spatiotemporal_2002,
	title = {Spatiotemporal reconstruction of list-mode {PET} data},
	volume = {21},
	issn = {1558-254X},
	doi = {10.1109/TMI.2002.1000263},
	abstract = {We describe a method for computing a continuous time estimate of tracer density using list-mode positron emission tomography data. The rate function in each voxel is modeled as an inhomogeneous Poisson process whose rate function can be represented using a cubic B-spline basis. The rate functions are estimated by maximizing the likelihood of the arrival times of detected photon pairs over the control vertices of the spline, modified by quadratic spatial and temporal smoothness penalties and a penalty term to enforce nonnegativity. Randoms rate functions are estimated by assuming independence between the spatial and temporal randoms distributions. Similarly, scatter rate functions are estimated by assuming spatiotemporal independence and that the temporal distribution of the scatter is proportional to the temporal distribution of the trues. A quantitative evaluation was performed using simulated data and the method is also demonstrated in a human study using /sup 11/C-raclopride.},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Nichols, T.E. and Qi, Jinyi and Asma, E. and Leahy, R.M.},
	month = apr,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Spline, Image reconstruction, Spatial resolution, Spatiotemporal phenomena, Electromagnetic scattering, Maximum likelihood estimation, Optical control, Particle scattering, Performance evaluation, Positron emission tomography},
	pages = {396--404},
	file = {Full Text:/home/david/Zotero/storage/RLCSZG87/Nichols et al. - 2002 - Spatiotemporal reconstruction of list-mode PET dat.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/EKMF49ZZ/1000263.html:text/html},
}

@article{li_fast_2007,
	title = {A fast fully 4-{D} incremental gradient reconstruction algorithm for list mode {PET} data},
	volume = {26},
	issn = {0278-0062},
	doi = {10.1109/TMI.2006.884208},
	abstract = {We describe a fast and globally convergent fully four-dimensional incremental gradient (4DIG) algorithm to estimate the continuous-time tracer density from list mode positron emission tomography (PET) data. Detection of 511-keV photon pairs produced by positron-electron annihilation is modeled as an inhomogeneous Poisson process whose rate function is parameterized using cubic B-splines. The rate functions are estimated by minimizing the cost function formed by the sum of the negative log-likelihood of arrival times, spatial and temporal roughness penalties, and a negativity penalty. We first derive a computable bound for the norm of the optimal temporal basis function coefficients. Based on this bound we then construct and prove convergence of an incremental gradient algorithm. Fully 4-D simulations demonstrate the substantially faster convergence behavior of the 4DIG algorithm relative to preconditioned conjugate gradient. Four-dimensional reconstructions of real data are also included to illustrate the performance of this method.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on medical imaging},
	author = {Li, Quanzheng and Asma, Evren and Ahn, Sangtae and Leahy, Richard M.},
	month = jan,
	year = {2007},
	pmid = {17243584},
	keywords = {Algorithms, Animals, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Information Storage and Retrieval, Mice, Positron-Emission Tomography, Reproducibility of Results, Sensitivity and Specificity, Subtraction Technique, Time Factors},
	pages = {58--67},
}

@article{garduno_optimization_2001,
	title = {Optimization of {Basis} {Functions} for {Both} {Reconstruction} and {Visualization}},
	volume = {46},
	issn = {1571-0661},
	url = {https://www.sciencedirect.com/science/article/pii/S1571066104810016},
	doi = {https://doi.org/10.1016/S1571-0661(04)81001-6},
	abstract = {Algebraic Reconstruction Techniques (ART) for the reconstruction of distributions from projections have yielded improvements in diverse fields such as medical imaging and electron microscopy. An important property of these methods is that they allow the use of various basis functions. Recently spherically symmetric functions (blobs) have been introduced as efficacious basis functions for reconstruction. However, basis functions whose parameters were found to be appropriate for use in reconstruction are not necessarily good for visualization. We propose a method of selecting blob parameters for both reconstruction and visualization.},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Garduño, Edgar and Herman, Gabor T.},
	year = {2001},
	keywords = {electron microscopy},
	pages = {413--429},
	file = {Garduno and Herman - Optimization of Basis Functions for Both Reconstru.pdf:/home/david/Zotero/storage/PSIFILXV/Garduno and Herman - Optimization of Basis Functions for Both Reconstru.pdf:application/pdf},
}

@article{jacobs_iterative_1999,
	title = {Iterative {Image} {Reconstruction} {From} {Projections} {Based} {On} {Generalised} {Kaiser}-{Bessel} {Window} {Functions}},
	abstract = {Tomographic images are calculated from data provided by a scanner. The reconstruction algorithms used for this purpose are either analytical or iterative in nature. The paper focuses on an iterative algorithm called the row-action maximum-likelihood algorithm, and on transmission tomography. Iterative algorithms approximate the image as a linear combination of a limited set of basis functions. The paper introduces a new set of basis functions, called blobs, into the field of process tomography. It also presents preliminary results of a study which evaluates the advantage of using blobs, instead of pixels, for different amounts of available data and different noise levels. The results clearly show that the use of blobs is also beneficial for process tomography.},
	language = {en},
	author = {Jacobs, Filip and Lemahieu, Ignace},
	year = {1999},
	keywords = {transmission computed tomograpy},
	pages = {6},
	file = {Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:/home/david/Zotero/storage/8URXQ9M9/Jacobs and Lemahieu - Iterative Image Reconstruction From Projections Ba.pdf:application/pdf},
}

@book{abramowitz_handbook_1972,
	address = {New York},
	edition = {Unabridged, unaltered and corr. republ. of the 1964 ed},
	series = {Dover books on advanced mathematics},
	title = {Handbook of {Mathematical} {Functions}},
	isbn = {978-0-486-61272-0},
	shorttitle = {Handbook of mathematical functions},
	language = {eng},
	publisher = {Dover publ},
	author = {Abramowitz, Milton and Stegun, Irene A.},
	year = {1972},
	file = {Abramowitz and Stegun - 1972 - Handbook of mathematical functions with formulas,.pdf:/home/david/Zotero/storage/NYFGVKJA/Abramowitz and Stegun - 1972 - Handbook of mathematical functions with formulas,.pdf:application/pdf},
}

@misc{noauthor_libstdc_nodate-1,
	title = {libstdc++: {Mathematical} {Special} {Functions}},
	url = {https://gcc.gnu.org/onlinedocs/gcc-10.1.0/libstdc++/api/a01503.html},
	urldate = {2022-04-04},
	file = {libstdc++\: Mathematical Special Functions:/home/david/Zotero/storage/PZWUTI58/a01503.html:text/html},
}

@article{verhaeghe_investigation_2007,
	title = {An investigation of temporal regularization techniques for dynamic {PET} reconstructions using temporal splines},
	volume = {34},
	issn = {2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1118/1.2723879},
	doi = {10.1118/1.2723879},
	abstract = {The use of a temporal B-spline basis for the reconstruction of dynamic positron emission tomography data was investigated. Maximum likelihood (ML) reconstructions using an expectation maximization framework and maximum A-posteriori (MAP) reconstructions using the generalized expectation maximization framework were evaluated. Different parameters of the B-spline basis of such as order, number of basis functions and knot placing were investigated in a reconstruction task using simulated dynamic list-mode data. We found that a higher order basis reduced both the bias and variance. Using a higher number of basis functions in the modeling of the time activity curves (TACs) allowed the algorithm to model faster changes of the TACs, however, the TACs became noisier. We have compared ML, Gaussian postsmoothed ML and MAP reconstructions. The noise level in the ML reconstructions was controlled by varying the number of basis functions. The MAP algorithm penalized the integrated squared curvature of the reconstructed TAC. The postsmoothed ML was always outperformed in terms of bias and variance properties by the MAP and ML reconstructions. A simple adaptive knot placing strategy was also developed and evaluated. It is based on an arc length redistribution scheme during the reconstruction. The free knot reconstruction allowed a more accurate reconstruction while reducing the noise level especially for fast changing TACs such as blood input functions. Limiting the number of temporal basis functions combined with the adaptive knot placing strategy is in this case advantageous for regularization purposes when compared to the other regularization techniques.},
	language = {en},
	number = {5},
	urldate = {2022-03-29},
	journal = {Medical Physics},
	author = {Verhaeghe, Jeroen and D'Asseler, Yves and Vandenberghe, Stefaan and Staelens, Steven and Lemahieu, Ignace},
	year = {2007},
	keywords = {positron emission tomography},
	pages = {1766--1778},
}

@article{nilchian_spline_2015,
	title = {Spline based iterative phase retrieval algorithm for {X}-ray differential phase contrast radiography},
	volume = {23},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-23-8-10631},
	doi = {10.1364/OE.23.010631},
	abstract = {Differential phase contrast imaging using grating interferometer is a promising alternative to conventional X-ray radiographic methods. It provides the absorption, differential phase and scattering information of the underlying sample simultaneously. Phase retrieval from the differential phase signal is an essential problem for quantitative analysis in medical imaging. In this paper, we formalize the phase retrieval as a regularized inverse problem, and propose a novel discretization scheme for the derivative operator based on B-spline calculus. The inverse problem is then solved by a constrained regularized weighted-norm algorithm (CRWN) which adopts the properties of B-spline and ensures a fast implementation. The method is evaluated with a tomographic dataset and differential phase contrast mammography data. We demonstrate that the proposed method is able to produce phase image with enhanced and higher soft tissue contrast compared to conventional absorption-based approach, which can potentially provide useful information to mammographic investigations.},
	language = {en},
	number = {8},
	urldate = {2022-03-29},
	journal = {Optics Express},
	author = {Nilchian, Masih and Wang, Zhentian and Thuering, Thomas and Unser, Michael and Stampanoni, Marco},
	month = apr,
	year = {2015},
	pages = {10631},
	file = {Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:/home/david/Zotero/storage/92ZZMU8T/Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:application/pdf;Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:/home/david/Zotero/storage/VX65EYE5/Nilchian et al. - 2015 - Spline based iterative phase retrieval algorithm f.pdf:application/pdf},
}

@article{chapdelaine_new_2018,
	title = {New {GPU} implementation of {Separable} {Footprint} ({SF}) {Projector} and {Backprojector}: first results},
	abstract = {Model-based iterative reconstruction methods enable to improve the quality of reconstruction in 3D X-ray Computed Tomography (CT). The main computational burden of these methods lies in successive projection and backprojection operations. Among existing pairs of projector and backprojector, Separable Footprint (SF) pair combines computational efﬁciency and accurate modelling of X-rays passing through the volume to image. In order to accelerate these operators, implementations on Graphical Processor Units (GPUs) for parallel-computing have been proposed for SF pair. Due to a CPU-loop, these implementations involve many memory transfers between CPU and GPU which are known to be the main bottleneck for GPU computing. In this paper, we investigate a new GPU implementation of SF projector and backprojector in order to minimize these memory transfers. Our proposed GPU SF projector and backprojector have no CPU-loop, and use two ray-driven kernels for the projection and one voxel-driven kernel for the backprojection. After having described their implementations, we study these operators as single modules and validate it in a MBIR method. Perspectives for this work are GPU optimizations and comparisons with the other existing implementations of SF pair.},
	language = {en},
	author = {Chapdelaine, Camille and Gac, Nicolas and Djafari, Ali-Mohammad and Parra-Denis, Estelle},
	year = {2018},
	pages = {5},
	file = {Chapdelaine et al. - New GPU implementation of Separable Footprint (SF).pdf:/home/david/Zotero/storage/UTJJXTP3/Chapdelaine et al. - New GPU implementation of Separable Footprint (SF).pdf:application/pdf},
}

@article{de_greef_accelerated_2009,
	title = {Accelerated ray tracing for radiotherapy dose calculations on a {GPU}: {GPU} accelerated dose calculation},
	volume = {36},
	issn = {00942405},
	shorttitle = {Accelerated ray tracing for radiotherapy dose calculations on a {GPU}},
	url = {http://doi.wiley.com/10.1118/1.3190156},
	doi = {10.1118/1.3190156},
	abstract = {Purpose: The graphical processing unit ͑GPU͒ on modern graphics cards offers the possibility of accelerating arithmetically intensive tasks. By splitting the work into a large number of independent jobs, order-of-magnitude speedups are reported. In this article, the possible speedup of PLATO’s ray tracing algorithm for dose calculations using a GPU is investigated. Methods: A GPU version of the ray tracing algorithm was implemented using NVIDIA’s CUDA, which extends the standard C language with functionality to program graphics cards. The developed algorithm was compared based on the accuracy and speed to a multithreaded version of the PLATO ray tracing algorithm. This comparison was performed for three test geometries, a phantom and two radiotherapy planning CT datasets ͑a pelvic and a head-and-neck case͒. For each geometry, four different source positions were evaluated. In addition to this, for the head-and-neck case also a vertex ﬁeld was evaluated. Results: The GPU algorithm was proven to be more accurate than the PLATO algorithm by elimination of the look-up table for z indices that introduces discretization errors in the reference algorithm. Speedups for ray tracing were found to be in the range of 2.1–10.1, relative to the multithreaded PLATO algorithm running four threads. For dose calculations the speedup measured was in the range of 1.5–6.2. For the speedup of both the ray tracing and the dose calculation, a strong dependency on the tested geometry was found. This dependency is related to the fraction of air within the patient’s bounding box resulting in idle threads. Conclusions: With the use of a GPU, ray tracing for dose calculations can be performed accurately in considerably less time. Ray tracing was accelerated, on average, with a factor of 6 for the evaluated cases. Dose calculation for a single beam can typically be carried out in 0.6– 0.9 s for clinically realistic datasets. These ﬁndings can be used in conventional planning to enable ͑nearly͒ real-time dose calculations. Also the importance for treatment optimization techniques is evident. © 2009 American Association of Physicists in Medicine. ͓DOI: 10.1118/1.3190156͔},
	language = {en},
	number = {9Part1},
	urldate = {2022-03-27},
	journal = {Medical Physics},
	author = {de Greef, M. and Crezee, J. and van Eijk, J. C. and Pool, R. and Bel, A.},
	month = aug,
	year = {2009},
	pages = {4095--4102},
	file = {de Greef et al. - 2009 - Accelerated ray tracing for radiotherapy dose calc.pdf:/home/david/Zotero/storage/Q6SIMFGS/de Greef et al. - 2009 - Accelerated ray tracing for radiotherapy dose calc.pdf:application/pdf},
}

@article{xiao_efficient_2012,
	title = {Efficient implementation of the {3D}‐{DDA} ray traversal algorithm on {GPU} and its application in radiation dose calculation},
	volume = {39},
	issn = {0094-2405, 2473-4209},
	url = {https://onlinelibrary.wiley.com/doi/10.1118/1.4767755},
	doi = {10.1118/1.4767755},
	abstract = {Purpose: The three-dimensional digital differential analyzer (3D-DDA) algorithm is a widely used ray traversal method, which is also at the core of many convolution/superposition (C/S) dose calculation approaches. However, porting existing C/S dose calculation methods onto graphics processing unit (GPU) has brought challenges to retaining the efﬁciency of this algorithm. In particular, straightforward implementation of the original 3D-DDA algorithm inﬂicts a lot of branch divergence which conﬂicts with the GPU programming model and leads to suboptimal performance. In this paper, an efﬁcient GPU implementation of the 3D-DDA algorithm is proposed, which effectively reduces such branch divergence and improves performance of the C/S dose calculation programs running on GPU. Methods: The main idea of the proposed method is to convert a number of conditional statements in the original 3D-DDA algorithm into a set of simple operations (e.g., arithmetic, comparison, and logic) which are better supported by the GPU architecture. To verify and demonstrate the performance improvement, this ray traversal method was integrated into a GPU-based collapsed cone convolution/superposition (CCCS) dose calculation program. Results: The proposed method has been tested using a water phantom and various clinical cases on an NVIDIA GTX570 GPU. The CCCS dose calculation program based on the efﬁcient 3D-DDA ray traversal implementation runs 1.42 ∼ 2.67× faster than the one based on the original 3D-DDA implementation, without losing any accuracy. Conclusions: The results show that the proposed method can effectively reduce branch divergence in the original 3D-DDA ray traversal algorithm and improve the performance of the CCCS program running on GPU. Considering the wide utilization of the 3D-DDA algorithm, various applications can beneﬁt from this implementation method. © 2012 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4767755]},
	language = {en},
	number = {12},
	urldate = {2022-03-27},
	journal = {Medical Physics},
	author = {Xiao, Kai and Chen, Danny Z. and Hu, X. Sharon and Zhou, Bo},
	month = dec,
	year = {2012},
	pages = {7619--7625},
	file = {Xiao et al. - 2012 - Efficient implementation of the 3D‐DDA ray travers.pdf:/home/david/Zotero/storage/XPY3V4LF/Xiao et al. - 2012 - Efficient implementation of the 3D‐DDA ray travers.pdf:application/pdf},
}

@article{radon_determination_1986,
	title = {On the determination of functions from their integral values along certain manifolds},
	volume = {5},
	issn = {1558-254X},
	doi = {10.1109/TMI.1986.4307775},
	abstract = {When one integrates a function of two variables x,y - a point function f(P) in the plane - subject to suitable regularity conditions along an arbitrary straight line g then one obtains in the integral values F(g), a line function. In Part A of the present paper the problem which is solved is the inversion of this linear functional transformation, that is the following questions are answered: can every line function satisfying suitable regularity conditions be regarded as constructed in this way? If so, is f uniquely known from F and how can f be calculated? In Part B a solution of the dual problem of calculating a line function F(g) from its point mean values f(P) is solved in a certain sense. Finally, in Part C certain generalizations are discussed, prompted by consideration of non-Euclidean manifolds as well as higher dimensional spaces. The treatment of these problems, themselves of interest, gains enhanced importance through the numerous relationships that exist between this topic and the theory of logarithmic and Newtonian potentials. These are mentioned at appropriate places in the text.},
	number = {4},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Radon, Johann},
	month = dec,
	year = {1986},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Integral equations, Mathematics},
	pages = {170--176},
	file = {IEEE Xplore Abstract Record:/home/david/Zotero/storage/46Y4AX92/4307775.html:text/html},
}

@article{radon_uber_1917,
	title = {Über die {Bestimmung} von {Funktionen} durch ihre {Integralwerte} längs gewisser {Mannigfaltigkeiten}},
	volume = {69},
	journal = {Akad. Wiss.},
	author = {Radon, J.},
	year = {1917},
	keywords = {imported},
	pages = {262--277},
	file = {Radon_Deutsch_1917.pdf:/home/david/Zotero/storage/TBJ7A9UP/Radon_Deutsch_1917.pdf:application/pdf},
}

@article{hounsfield_computerized_1973,
	title = {Computerized transverse axial scanning (tomography): {Part} 1. {Description} of system},
	volume = {46},
	issn = {0007-1285},
	doi = {10.1259/0007-1285-46-552-1016},
	language = {eng},
	number = {552},
	journal = {The British Journal of Radiology},
	author = {Hounsfield, G. N.},
	month = dec,
	year = {1973},
	pmid = {4757352},
	keywords = {Physics, Humans, Computers, Adipose Tissue, Body Fluids, Bone and Bones, Head, Physical Phenomena, Radiation Dosage, Tomography, X-Ray},
	pages = {1016--1022},
}

@article{perry_computerized_1973,
	title = {Computerized transverse axial scanning (tomography): {Part} 3. {Radiation} dose considerations},
	volume = {46},
	issn = {0007-1285},
	doi = {10.1259/0007-1285-46-552-1048},
	language = {eng},
	number = {552},
	journal = {The British Journal of Radiology},
	author = {Perry, B. J. and Bridges, C.},
	month = dec,
	year = {1973},
	pmid = {4757354},
	keywords = {Computers, Head, Radiation Dosage, Tomography, X-Ray, Models, Structural, Radiation Monitoring},
	pages = {1048--1051},
}

@article{ambrose_computerized_1973,
	title = {Computerized transverse axial scanning (tomography): {Part} 2. {Clinical} application},
	volume = {46},
	issn = {0007-1285},
	shorttitle = {Computerized transverse axial scanning (tomography)},
	url = {https://www.birpublications.org/doi/10.1259/0007-1285-46-552-1023},
	doi = {10.1259/0007-1285-46-552-1023},
	abstract = {A new and fundamentally different X-ray method is described. The cranium is scanned in successive layers by a narrow beam of X rays, in such a way that the transmission of the X-ray photons across a particular layer can be measured, and by means of a computer, used to construct a picture of the internal structure.

Employing a suitably designed scanning gantry, a continuously operating X-ray tube, and a narrow collimated X-ray beam, the transmissions of X-ray photons across a slice of tissue may be measured by a system of crystal detectors in such a way that 28,800 readings are obtained. These form the basis of 28,000 simultaneous equations which are solved by a computer. The solutions are transformed into absorption coefficients and by means of a suitable algorithm related to their correct cells in a matrix of chosen size.

The results are stored, computed, and then made available from a magnetic disc to construct a picture on a CRT. The numerical results are available from a print-out.

The examination is, therefore, qualitative and quantitative. Pictures thus obtained are looked at in much the same way as radiographs. Structures are identified and shape, size, and position defined. Changes in tissue density are then looked for.

Lesions are seen as alterations of normal density and are interpreted in the light of pathological changes which are known to occur. Increased density may be due to blood clot, calcium deposition in tumours, and other lesions. In haemorrhage, once clotting has occurred, the concentrated blood constituents show up as an area of high density. Average tissue density is lowered in tissue necrosis, oedema, cyst formation, and haemorrhage where clotting has not occurred.

Tissue density may be artificially enhanced by the intravenous injection of substances containing large atoms;sodium iothalamate has been found to be ideal and the tissue density of a variety of tumours may be enhanced by this means.},
	number = {552},
	urldate = {2022-04-04},
	journal = {The British Journal of Radiology},
	author = {Ambrose, James},
	month = dec,
	year = {1973},
	note = {Publisher: The British Institute of Radiology},
	pages = {1023--1047},
}

@article{cormack_representation_1963,
	title = {Representation of a {Function} by {Its} {Line} {Integrals}, with {Some} {Radiological} {Applications}},
	volume = {34},
	issn = {0021-8979},
	url = {https://aip.scitation.org/doi/10.1063/1.1729798},
	doi = {10.1063/1.1729798},
	number = {9},
	urldate = {2022-04-04},
	journal = {Journal of Applied Physics},
	author = {Cormack, A. M.},
	month = sep,
	year = {1963},
	note = {Publisher: American Institute of Physics},
	pages = {2722--2727},
}

@incollection{olafsson_introduction_2006,
	address = {Providence, Rhode Island},
	title = {An introduction to {X}-ray tomography and {Radon} transforms},
	volume = {63},
	isbn = {978-0-8218-3930-0 978-0-8218-9278-7},
	url = {http://www.ams.org/psapm/063},
	abstract = {This article provides an introduction to the mathematics behind X-ray tomography. After explaining the mathematical model, we will consider some of the fundamental theoretical ideas in the ﬁeld, including the projection slice theorem, range theorem, inversion formula, and microlocal properties of the underlying Radon transform. We will use this microlocal analysis to predict which singularities of objects will be well reconstructed from limited tomographic data. We will introduce speciﬁc limited data problems: the exterior problem, region of interest tomography, and limited angle region of interest tomography, and we use some of the author’s reconstructions for these problems to illustrate the microlocal predictions about singularities. The appendix includes proofs of the basic microlocal properties of the Radon transform. Our overarching goal is to show some of the ways integral geometry and microlocal analysis can help one understand limited data tomography.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {Proceedings of {Symposia} in {Applied} {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Quinto, Eric Todd},
	editor = {Ólafsson, Gestur and Quinto, Eric},
	year = {2006},
	doi = {10.1090/psapm/063/2208234},
	pages = {1--23},
	file = {Quinto - 2006 - An introduction to X-ray tomography and Radon tran.pdf:/home/david/Zotero/storage/H64HWJQL/Quinto - 2006 - An introduction to X-ray tomography and Radon tran.pdf:application/pdf},
}

@article{faridani_introduction_nodate,
	title = {Introduction to the {Mathematics} of {Computed} {Tomography}},
	abstract = {Computed tomography (CT) entails the reconstruction of a function f from line integrals of f . This mathematical problem is encountered in a growing number of diverse settings in medicine, science, and technology. This introductory article is divided into three parts. The ﬁrst part is concerned with general theory and explores questions of uniqueness, stability and inversion, as well as detection of singularities. The second part is devoted to local tomography and is centered around a discussion of recently developed methods for computing jumps of a function from local tomographic data. The third part treats optimal sampling and has at its core a detailed error analysis of the parallel-beam ﬁltered backprojection algorithm. Matlab source code for the ﬁltered backprojection algorithm and the Feldkamp–Davis–Kress algorithm is included in an appendix.},
	language = {en},
	author = {Faridani, Adel},
	pages = {46},
	file = {Faridani - Introduction to the Mathematics of Computed Tomogr.pdf:/home/david/Zotero/storage/X3ZA2NS8/Faridani - Introduction to the Mathematics of Computed Tomogr.pdf:application/pdf},
}

@article{ramachandran_three-dimensional_1971,
	title = {Three-dimensional reconstruction from radiographs and electron micrographs: application of convolutions instead of {Fourier} transforms},
	volume = {68},
	issn = {0027-8424},
	shorttitle = {Three-dimensional reconstruction from radiographs and electron micrographs},
	doi = {10.1073/pnas.68.9.2236},
	abstract = {A new technique is proposed for the mathematical process of reconstruction of a three-dimensional object from its transmission shadowgraphs; it uses convolutions with functions defined in the real space of the object, without using Fourier transforms. The object is rotated about an axis at right angles to the direction of a parallel beam of radiation, and sections of it normal to the axis are reconstructed from data obtained by scanning the corresponding linear strips in the shadowgraphs at different angular settings. Since the formulae in the convolution method involve only summations over one variable at a time, while a two-dimensional reconstruction with the Fourier transform technique requires double summations, the convolution method is much faster (typically by a factor of 30); the relative increase in speed is larger where greater resolution is required. Tests of the convolution method with computer-simulated shadowgraphs show that it is also more accurate than the Fourier transform method. It has good potentialities for application in electron microscopy and x-radiography. A new method of reconstructing helical structures by this technique is also suggested.},
	language = {eng},
	number = {9},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Ramachandran, G. N. and Lakshminarayanan, A. V.},
	month = sep,
	year = {1971},
	pmid = {5289381},
	pmcid = {PMC389392},
	keywords = {Radiography, Computers, Mathematics, Microscopy, Electron, Optics and Photonics},
	pages = {2236--2240},
	file = {Full Text:/home/david/Zotero/storage/NZ5ZAGI3/Ramachandran and Lakshminarayanan - 1971 - Three-dimensional reconstruction from radiographs .pdf:application/pdf},
}

@article{pan_why_2009,
	title = {Why do commercial {CT} scanners still employ traditional, filtered back-projection for image reconstruction?},
	volume = {25},
	issn = {0266-5611},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2849113/},
	doi = {10.1088/0266-5611/25/12/123009},
	abstract = {Despite major advances in x-ray sources, detector arrays, gantry mechanical design and especially computer performance, one component of computed tomography (CT) scanners has remained virtually constant for the past 25 years—the reconstruction algorithm. Fundamental advances have been made in the solution of inverse problems, especially tomographic reconstruction, but these works have not been translated into clinical and related practice. The reasons are not obvious and seldom discussed. This review seeks to examine the reasons for this discrepancy and provides recommendations on how it can be resolved. We take the example of field of compressive sensing (CS), summarizing this new area of research from the eyes of practical medical physicists and explaining the disconnection between theoretical and application-oriented research. Using a few issues specific to CT, which engineers have addressed in very specific ways, we try to distill the mathematical problem underlying each of these issues with the hope of demonstrating that there are interesting mathematical problems of general importance that can result from in depth analysis of specific issues. We then sketch some unconventional CT-imaging designs that have the potential to impact on CT applications, if the link between applied mathematicians and engineers/physicists were stronger. Finally, we close with some observations on how the link could be strengthened. There is, we believe, an important opportunity to rapidly improve the performance of CT and related tomographic imaging techniques by addressing these issues.},
	number = {12},
	urldate = {2022-04-04},
	journal = {Inverse problems},
	author = {Pan, Xiaochuan and Sidky, Emil Y and Vannier, Michael},
	month = jan,
	year = {2009},
	pmid = {20376330},
	pmcid = {PMC2849113},
	pages = {1230009},
	file = {Accepted Version:/home/david/Zotero/storage/479W3ILE/Pan et al. - 2009 - Why do commercial CT scanners still employ traditi.pdf:application/pdf},
}

@book{kak_principles_2001,
	title = {Principles of {Computerized} {Tomographic} {Imaging}},
	isbn = {978-0-89871-494-4 978-0-89871-927-7},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9780898719277},
	language = {en},
	urldate = {2022-04-04},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kak, Avinash C. and Slaney, Malcolm},
	month = jan,
	year = {2001},
	doi = {10.1137/1.9780898719277},
	file = {Kak&Slaney-CTI-reduced.pdf:/home/david/Zotero/storage/EVSEKPCL/Kak&Slaney-CTI-reduced.pdf:application/pdf},
}

@misc{noauthor_principles_nodate,
	title = {Principles of {Computerized} {Tomographic} {Imaging}},
	url = {https://epubs-siam-org.eaccess.ub.tum.de/doi/book/10.1137/1.9780898719277},
	language = {en},
	urldate = {2022-04-04},
	journal = {Classics in Applied Mathematics},
	file = {Snapshot:/home/david/Zotero/storage/L6RI5YGS/1.html:text/html},
}

@article{gordon_algebraic_1970,
	title = {Algebraic {Reconstruction} {Techniques} ({ART}) for three-dimensional electron microscopy and {X}-ray photography},
	volume = {29},
	issn = {00225193},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0022519370901098},
	doi = {10.1016/0022-5193(70)90109-8},
	language = {en},
	number = {3},
	urldate = {2022-04-04},
	journal = {Journal of Theoretical Biology},
	author = {Gordon, Richard and Bender, Robert and Herman, Gabor T.},
	month = dec,
	year = {1970},
	pages = {471--481},
	file = {Gordon et al. - 1970 - Algebraic Reconstruction Techniques (ART) for thre.pdf:/home/david/Zotero/storage/GWYI2E84/Gordon et al. - 1970 - Algebraic Reconstruction Techniques (ART) for thre.pdf:application/pdf},
}

@article{strohmer_randomized_2007,
	title = {A randomized {Kaczmarz} algorithm with exponential convergence},
	url = {http://arxiv.org/abs/math/0702226},
	abstract = {The Kaczmarz method for solving linear systems of equations is an iterative algorithm that has found many applications ranging from computer tomography to digital signal processing. Despite the popularity of this method, useful theoretical estimates for its rate of convergence are still scarce. We introduce a randomized version of the Kaczmarz method for consistent, overdetermined linear systems and we prove that it converges with expected exponential rate. Furthermore, this is the ﬁrst solver whose rate does not depend on the number of equations in the system. The solver does not even need to know the whole system, but only a small random part of it. It thus outperforms all previously known methods on general extremely overdetermined systems. Even for moderately overdetermined systems, numerical simulations as well as theoretical analysis reveal that our algorithm can converge faster than the celebrated conjugate gradient algorithm. Furthermore, our theory and numerical simulations conﬁrm a prediction of Feichtinger et al. in the context of reconstructing bandlimited functions from nonuniform sampling.},
	language = {en},
	urldate = {2022-04-04},
	journal = {arXiv:math/0702226},
	author = {Strohmer, Thomas and Vershynin, Roman},
	month = feb,
	year = {2007},
	note = {arXiv: math/0702226},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {Strohmer and Vershynin - 2007 - A randomized Kaczmarz algorithm with exponential c.pdf:/home/david/Zotero/storage/I2UY6N9B/Strohmer and Vershynin - 2007 - A randomized Kaczmarz algorithm with exponential c.pdf:application/pdf},
}

@article{chen_kaczmarz_2010,
	title = {Kaczmarz {Algorithm}, row action methods, and statistical learning algorithms},
	abstract = {The Kaczmarz algorithm is an iterative row action method that typically solves an overdetermined linear system. The randomized Kaczmarz algorithm, which was introduced a decade ago, revived this simple algorithm and raised a lot of interest in this community. It has come to people’s attention that there is overlap between many other iterative methods, statistical learning algorithms and the (randomized) Kaczmarz algorithm. This note brings these methods together and discuss connections and theories, with a focus on the convergence rate of the deterministic Kaczmarz algorithm. Moreover, the convergence under noise for the deterministic case is discussed in Theorem 3.2.},
	language = {en},
	author = {Chen, Xuemei},
	year = {2010},
	pages = {12},
	file = {Chen - Kaczmarz Algorithm, row action methods, and statis.pdf:/home/david/Zotero/storage/ZTWZR4YB/Chen - Kaczmarz Algorithm, row action methods, and statis.pdf:application/pdf},
}

@book{scherzer_variational_2009,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Variational {Methods} in {Imaging}},
	volume = {167},
	url = {http://link.springer.com/10.1007/978-0-387-69277-7},
	language = {en},
	urldate = {2022-04-05},
	publisher = {Springer New York},
	author = {Scherzer, Otmar and Grasmair, Markus and Grossauer, Harald and Haltmeier, Markus and Lenzen, Frank},
	year = {2009},
	doi = {10.1007/978-0-387-69277-7},
	note = {ISSN: 0066-5452},
	file = {2009 - Variational Methods in Imaging.pdf:/home/david/Zotero/storage/AXBPFPYD/2009 - Variational Methods in Imaging.pdf:application/pdf},
}

@article{lucy_iterative_1974,
	title = {An iterative technique for the rectification of observed distributions},
	volume = {79},
	issn = {00046256},
	url = {http://adsabs.harvard.edu/cgi-bin/bib_query?1974AJ.....79..745L},
	doi = {10.1086/111605},
	language = {en},
	urldate = {2022-04-05},
	journal = {The Astronomical Journal},
	author = {Lucy, L. B.},
	month = jun,
	year = {1974},
	pages = {745},
	file = {Lucy - 1974 - An iterative technique for the rectification of ob.pdf:/home/david/Zotero/storage/TZKBIA9E/Lucy - 1974 - An iterative technique for the rectification of ob.pdf:application/pdf},
}

@article{adorf_hubble_1995,
	title = {Hubble {Space} {Telescope} image restoration in its fourth year},
	volume = {11},
	issn = {0266-5611, 1361-6420},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/11/4/003},
	doi = {10.1088/0266-5611/11/4/003},
	abstract = {The NASAlESA Hubble Space Telescope (HST), an asIronomical observatory launched in 1990into a low-altitudespace orbit, was designed to deliver near-diffractionlimited images, but its optics suffered from substantial spherical aberration. The HST image restoration problem is aggravated by insufficient image sampling, by a mixture of noise sources including spatiaUy non-stationary, non-Gaussian noise, and by the desire to quantitatively evaluare the restored data. Restoration efforts have helped to minimize the impad of Ihe data distorlions on HST’s scientific return. At the end of 1993 HST was refurbished, and its optics have largely been restored to meet the design goals. Nevertheless, image restoration remains important to ueat problems such as combining dithered, undersampled image frames, or to combine images with different resolutions and signal-to-noise ratios.},
	language = {en},
	number = {4},
	urldate = {2022-04-05},
	journal = {Inverse Problems},
	author = {Adorf, H -M},
	month = aug,
	year = {1995},
	pages = {639--653},
	file = {Adorf - 1995 - Hubble Space Telescope image restoration in its fo.pdf:/home/david/Zotero/storage/LFEFTXIB/Adorf - 1995 - Hubble Space Telescope image restoration in its fo.pdf:application/pdf},
}

@inproceedings{white_restoration_1992,
	title = {Restoration of images and spectra from the {Hubble} {Space} {Telescope}},
	volume = {25},
	booktitle = {Astronomical {Data} {Analysis} {Software} and {Systems} {I}},
	author = {White, Richard L},
	year = {1992},
	pages = {176},
	file = {White - 1992 - Restoration of images and spectra from the Hubble .pdf:/home/david/Zotero/storage/6BXMIPDU/White - 1992 - Restoration of images and spectra from the Hubble .pdf:application/pdf},
}

@article{hadamard_sur_1902,
	title = {Sur les problèmes aux dérivés partielles et leur signification physique},
	volume = {13},
	journal = {Princeton University Bulletin},
	author = {Hadamard, J.},
	year = {1902},
	keywords = {ill-posed, problem},
	pages = {49--52},
}

@article{kaczmarz_approximate_1993,
	title = {Approximate solution of systems of linear equations†},
	volume = {57},
	issn = {0020-7179, 1366-5820},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00207179308934446},
	doi = {10.1080/00207179308934446},
	language = {en},
	number = {6},
	urldate = {2022-04-05},
	journal = {International Journal of Control},
	author = {Kaczmarz, S.},
	month = jun,
	year = {1993},
	pages = {1269--1271},
	file = {Kaczmarz - 1993 - Approximate solution of systems of linear equation.pdf:/home/david/Zotero/storage/C3U7I94V/Kaczmarz - 1993 - Approximate solution of systems of linear equation.pdf:application/pdf},
}

@article{landweber_iteration_1951,
	title = {An {Iteration} {Formula} for {Fredholm} {Integral} {Equations} of the {First} {Kind}},
	volume = {73},
	issn = {00029327},
	url = {https://www.jstor.org/stable/2372313?origin=crossref},
	doi = {10.2307/2372313},
	language = {en},
	number = {3},
	urldate = {2022-04-05},
	journal = {American Journal of Mathematics},
	author = {Landweber, L.},
	month = jul,
	year = {1951},
	pages = {615},
	file = {Landweber - 1951 - An Iteration Formula for Fredholm Integral Equatio.pdf:/home/david/Zotero/storage/GCJGV49I/Landweber - 1951 - An Iteration Formula for Fredholm Integral Equatio.pdf:application/pdf},
}

@article{gilbert_iterative_1972,
	title = {Iterative methods for the three-dimensional reconstruction of an object from projections},
	volume = {36},
	issn = {0022-5193},
	url = {https://www.sciencedirect.com/science/article/pii/0022519372901804},
	doi = {10.1016/0022-5193(72)90180-4},
	abstract = {A method of reconstruction (ART) has recently been proposed (Gordon, Bender \& Herman, 1970) which consists in iteratively changing a trial structure until its projections are consistent with the original projections of the unknown structure. It is shown that in general ART produces erroneous reconstructions. An alternative iterative method is proposed which will give correct reconstructions under certain conditions. One of the potential applications of this method is in determining the three-dimensional structure of objects from electron micrographs.},
	language = {en},
	number = {1},
	urldate = {2022-04-05},
	journal = {Journal of Theoretical Biology},
	author = {Gilbert, Peter},
	month = jul,
	year = {1972},
	pages = {105--117},
	file = {Gilbert - 1972 - Iterative methods for the three-dimensional recons.pdf:/home/david/Zotero/storage/FV97BBAW/Gilbert - 1972 - Iterative methods for the three-dimensional recons.pdf:application/pdf},
}

@article{tihonov_solution_1963,
	title = {Solution of incorrectly formulated problems and the regularization method},
	volume = {4},
	journal = {Soviet Math.},
	author = {Tihonov, Andrei Nikolajevits},
	year = {1963},
	pages = {1035--1038},
}

@article{beck_fast_2009,
	title = {A {Fast} {Iterative} {Shrinkage}-{Thresholding} {Algorithm} for {Linear} {Inverse} {Problems}},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2022-04-05},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/home/david/Zotero/storage/3QJ4DJ8T/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}

@article{xiang_fista-net_2021,
	title = {{FISTA}-{Net}: {Learning} {A} {Fast} {Iterative} {Shrinkage} {Thresholding} {Network} for {Inverse} {Problems} in {Imaging}},
	volume = {40},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{FISTA}-{Net}},
	url = {http://arxiv.org/abs/2008.02683},
	doi = {10.1109/TMI.2021.3054167},
	abstract = {Inverse problems are essential to imaging applications. In this paper, we propose a model-based deep learning network, named FISTA-Net, by combining the merits of interpretability and generality of the model-based Fast Iterative Shrinkage/Thresholding Algorithm (FISTA) and strong regularization and tuning-free advantages of the data-driven neural network. By unfolding the FISTA into a deep network, the architecture of FISTA-Net consists of multiple gradient descent, proximal mapping, and momentum modules in cascade. Different from FISTA, the gradient matrix in FISTA-Net can be updated during iteration and a proximal operator network is developed for nonlinear thresholding which can be learned through end-to-end training. Key parameters of FISTA-Net including the gradient step size, thresholding value and momentum scalar are tuning-free and learned from training data rather than hand-crafted. We further impose positive and monotonous constraints on these parameters to ensure they converge properly. The experimental results, evaluated both visually and quantitatively, show that the FISTA-Net can optimize parameters for different imaging tasks, i.e. Electromagnetic Tomography (EMT) and X-ray Computational Tomography (X-ray CT). It outperforms the state-of-the-art model-based and deep learning methods and exhibits good generalization ability over other competitive learning-based approaches under different noise levels.},
	language = {en},
	number = {5},
	urldate = {2022-04-05},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Xiang, Jinxi and Dong, Yonggui and Yang, Yunjie},
	month = may,
	year = {2021},
	note = {arXiv: 2008.02683},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Applied Physics},
	pages = {1329--1339},
	file = {Xiang et al. - 2021 - FISTA-Net Learning A Fast Iterative Shrinkage Thr.pdf:/home/david/Zotero/storage/F4ZNFCB3/Xiang et al. - 2021 - FISTA-Net Learning A Fast Iterative Shrinkage Thr.pdf:application/pdf},
}

@article{tibshirani_lasso_2013-1,
	title = {The lasso problem and uniqueness},
	volume = {7},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-7/issue-none/The-lasso-problem-and-uniqueness/10.1214/13-EJS815.full},
	doi = {10.1214/13-EJS815},
	abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p {\textgreater} n, the lasso criterion is not strictly convex, and hence it may not have a unique minimizer. An important question is: when is the lasso solution well-deﬁned (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to 1 penalized minimization problems over a wide range of loss functions.},
	language = {en},
	number = {none},
	urldate = {2022-04-05},
	journal = {Electronic Journal of Statistics},
	author = {Tibshirani, Ryan J.},
	month = jan,
	year = {2013},
	file = {Tibshirani - 2013 - The lasso problem and uniqueness.pdf:/home/david/Zotero/storage/6JAAEYHU/Tibshirani - 2013 - The lasso problem and uniqueness.pdf:application/pdf},
}

@article{tao_local_2015-1,
	title = {Local {Linear} {Convergence} of {ISTA} and {FISTA} on the {LASSO} {Problem}},
	url = {http://arxiv.org/abs/1501.02888},
	abstract = {We establish local linear convergence bounds for the ISTA and FISTA iterations on the model LASSO problem. We show that FISTA can be viewed as an accelerated ISTA process. Using a spectral analysis, we show that, when close enough to the solution, both iterations converge linearly, but FISTA slows down compared to ISTA, making it advantageous to switch to ISTA toward the end of the iteration processs. We illustrate the results with some synthetic numerical examples.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:1501.02888 [math]},
	author = {Tao, Shaozhe and Boley, Daniel and Zhang, Shuzhong},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.02888},
	keywords = {Mathematics - Optimization and Control},
	file = {Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:/home/david/Zotero/storage/QBZ4YIL7/Tao et al. - 2015 - Local Linear Convergence of ISTA and FISTA on the .pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {We propose a new method for estimation in linear models. The 'lasso' minim residual sum of squares subject to the sum of the absolute value of the coefficients than a constant. Because of the nature of this constraint it tends to produce coefficients that are exactly 0 and hence gives interpretable models. Our simulatio suggest that the lasso enjoys some of the favourable properties of both subset sele ridge regression. It produces interpretable models like subset selection and exh stability of ridge regression. There is also an interesting relationship with recent adaptive function estimation by Donoho and Johnstone. The lasso idea is quite ge can be applied in a variety of statistical models: extensions to generalized regressio and tree-based models are briefly described.},
	language = {en},
	number = {1},
	urldate = {2022-04-05},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	month = jan,
	year = {1996},
	pages = {267--288},
	file = {Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:/home/david/Zotero/storage/5DXC6NQ8/Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@article{golub_tikhonov_1999,
	title = {Tikhonov {Regularization} and {Total} {Least} {Squares}},
	volume = {21},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/S0895479897326432},
	doi = {10.1137/S0895479897326432},
	abstract = {Discretizations of inverse problems lead to systems of linear equations with a highly ill-conditioned coeﬃcient matrix, and in order to compute stable solutions to these systems it is necessary to apply regularization methods. We show how Tikhonov’s regularization method, which in its original formulation involves a least squares problem, can be recast in a total least squares formulation suited for problems in which both the coeﬃcient matrix and the right-hand side are known only approximately. We analyze the regularizing properties of this method and demonstrate by a numerical example that, in certain cases with large perturbations, the new method is superior to standard regularization methods.},
	language = {en},
	number = {1},
	urldate = {2022-04-05},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Golub, Gene H. and Hansen, Per Christian and O'Leary, Dianne P.},
	month = jan,
	year = {1999},
	pages = {185--194},
	file = {Golub et al. - 1999 - Tikhonov Regularization and Total Least Squares.pdf:/home/david/Zotero/storage/BBD4KESY/Golub et al. - 1999 - Tikhonov Regularization and Total Least Squares.pdf:application/pdf},
}

@article{figueiredo_gradient_2007,
	title = {Gradient {Projection} for {Sparse} {Reconstruction}: {Application} to {Compressed} {Sensing} and {Other} {Inverse} {Problems}},
	volume = {1},
	issn = {1932-4553, 1941-0484},
	shorttitle = {Gradient {Projection} for {Sparse} {Reconstruction}},
	url = {http://ieeexplore.ieee.org/document/4407762/},
	doi = {10.1109/JSTSP.2007.910281},
	abstract = {Many problems in signal processing and statistical inference involve ﬁnding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. A standard approach consists in minimizing an objective function which includes a quadratic (squared ℓ2) error term combined with a sparseness-inducing (ℓ1) regularization term.Basis pursuit, the least absolute shrinkage and selection operator (LASSO), waveletbased deconvolution, and compressed sensing are a few wellknown examples of this approach. This paper proposes gradient projection (GP) algorithms for the bound-constrained quadratic programming (BCQP) formulation of these problems. We test variants of this approach that select the line search parameters in different ways, including techniques based on the BarzilaiBorwein method. Computational experiments show that these GP approaches perform well in a wide range of applications, often being signiﬁcantly faster (in terms of computation time) than competing methods. Although the performance of GP methods tends to degrade as the regularization term is de-emphasized, we show how they can be embedded in a continuation scheme to recover their efﬁcient practical performance.},
	language = {en},
	number = {4},
	urldate = {2022-04-05},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Figueiredo, MÁrio A. T. and Nowak, Robert D. and Wright, Stephen J.},
	month = dec,
	year = {2007},
	pages = {586--597},
	file = {Figueiredo et al. - 2007 - Gradient Projection for Sparse Reconstruction App.pdf:/home/david/Zotero/storage/MPKLA5SB/Figueiredo et al. - 2007 - Gradient Projection for Sparse Reconstruction App.pdf:application/pdf},
}

@article{daubechies_iterative_2003,
	title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
	url = {http://arxiv.org/abs/math/0307152},
	abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary pre–assigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted ℓp- penalties on the coeﬃcients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. If p {\textless} 2, regularized solutions of such lp-penalized problems will have sparser expansions, with respect to the basis under consideration. To compute the corresponding regularized solutions we propose an iterative algorithm which amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. We also review some potential applications of this method.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:math/0307152},
	author = {Daubechies, Ingrid and Defrise, Michel and De Mol, Christine},
	month = nov,
	year = {2003},
	note = {arXiv: math/0307152},
	keywords = {Mathematics - Functional Analysis, Mathematics - Numerical Analysis},
	file = {Daubechies et al. - 2003 - An iterative thresholding algorithm for linear inv.pdf:/home/david/Zotero/storage/YCV8KZLF/Daubechies et al. - 2003 - An iterative thresholding algorithm for linear inv.pdf:application/pdf},
}

@article{daubechies_iterative_2004,
	title = {An iterative thresholding algorithm for linear inverse problems with a sparsity constraint},
	volume = {57},
	issn = {0010-3640, 1097-0312},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.20042},
	doi = {10.1002/cpa.20042},
	abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted p- penalties on the coefﬁcients of such expansions, with 1 ≤ p ≤ 2, still regularizes the problem. Use of such p-penalized problems with p {\textless} 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. c 2004 Wiley Periodicals, Inc.},
	language = {en},
	number = {11},
	urldate = {2022-04-05},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Daubechies, I. and Defrise, M. and De Mol, C.},
	month = nov,
	year = {2004},
	pages = {1413--1457},
	file = {Daubechies et al. - 2004 - An iterative thresholding algorithm for linear inv.pdf:/home/david/Zotero/storage/GL6LYSMS/Daubechies et al. - 2004 - An iterative thresholding algorithm for linear inv.pdf:application/pdf},
}

@article{rontgen_new_1896,
	title = {On a {New} {Kind} of {Rays}},
	volume = {53},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/053274b0},
	doi = {10.1038/053274b0},
	language = {en},
	number = {1369},
	urldate = {2022-04-09},
	journal = {Nature},
	author = {Röntgen, Wilhelm Conrad},
	month = jan,
	year = {1896},
	pages = {274--276},
	file = {1896 - On a New Kind of Rays.pdf:/home/david/Zotero/storage/PII4KHHW/1896 - On a New Kind of Rays.pdf:application/pdf},
}

@article{rontgen_uber_1895,
	title = {Über eine neue {Art} von {Strahlen}},
	volume = {137},
	url = {https://www.xtal.iqfr.csic.es/Cristalografia/archivos_10/Uber_eine_neue_art_von_strahlen_ocr.pdf},
	urldate = {2022-04-09},
	journal = {Sitzungsberichte der Würzburger physik.-med. Gesellschaft Würzburg},
	author = {Röntgen, Wilhelm Conrad},
	year = {1895},
	pages = {132--141},
	file = {Uber_eine_neue_art_von_strahlen_ocr.pdf:/home/david/Zotero/storage/RUT9AJ2Q/Uber_eine_neue_art_von_strahlen_ocr.pdf:application/pdf},
}

@book{gagliardi_history_1996,
	title = {A {History} of the {Radiological} {Sciences}: {Diagnosis}. {Gagliardi} {RA}, {McClennan} {BL}, eds.},
	volume = {169},
	author = {Gagliardi, Raymond and McClennan, Bruce L.},
	year = {1996},
	note = {Publisher: AMERICAN ROENTGEN RAY SOCIETY},
}

@incollection{haygood_skeletal_1996,
	title = {Skeletal {Radiology}},
	volume = {169},
	booktitle = {A {History} of the {Radiological} {Sciences}: {Diagnosis}. {Gagliardi} {RA}, {McClennan} {BL}, eds.},
	author = {Haygood, Tamara Miner and Bohrer, Stanley P.},
	year = {1996},
	note = {Publisher: AMERICAN ROENTGEN RAY SOCIETY},
	pages = {108--130},
	file = {Haygood and Bohrer - 1996 - Skeletal Radiology.pdf:/home/david/Zotero/storage/BC272UV5/Haygood and Bohrer - 1996 - Skeletal Radiology.pdf:application/pdf},
}

@article{yu_radiation_2009,
	title = {Radiation dose reduction in computed tomography: techniques and future perspective},
	volume = {1},
	issn = {1755-5191, 1755-5205},
	shorttitle = {Radiation dose reduction in computed tomography},
	url = {http://www.futuremedicine.com/doi/abs/10.2217/iim.09.5},
	doi = {10.2217/iim.09.5},
	abstract = {Despite universal consensus that computed tomography (CT) overwhelmingly benefits patients when used for appropriate indications, concerns have been raised regarding the potential risk of cancer induction from CT due to the exponentially increased use of CT in medicine. Keeping radiation dose as low as reasonably achievable, consistent with the diagnostic task, remains the most important strategy for decreasing this potential risk. This article summarizes the general technical strategies that are commonly used for radiation dose management in CT. Dosemanagement strategies for pediatric CT, cardiac CT, dual-energy CT, CT perfusion and interventional CT are specifically discussed, and future perspectives on CT dose reduction are presented.},
	language = {en},
	number = {1},
	urldate = {2022-04-09},
	journal = {Imaging in Medicine},
	author = {Yu, Lifeng and Liu, Xin and Leng, Shuai and Kofler, James M and Ramirez-Giraldo, Juan C and Qu, Mingliang and Christner, Jodie and Fletcher, Joel G and McCollough, Cynthia H},
	month = oct,
	year = {2009},
	pages = {65--84},
	file = {Yu et al. - 2009 - Radiation dose reduction in computed tomography t.pdf:/home/david/Zotero/storage/L2J2AC6Z/Yu et al. - 2009 - Radiation dose reduction in computed tomography t.pdf:application/pdf},
}

@article{lewis_medical_2004,
	title = {Medical phase contrast x-ray imaging: current status and future prospects},
	volume = {49},
	issn = {0031-9155, 1361-6560},
	shorttitle = {Medical phase contrast x-ray imaging},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/49/16/005},
	doi = {10.1088/0031-9155/49/16/005},
	abstract = {The exploitation of phase contrast appears to offer the tantalising possibility of creating the biggest change in medical x-ray imaging since the invention of computed tomography. A considerable number of experiments performed by researchers across four continents have produced some extraordinary images. These images have demonstrated greatly enhanced contrast over conventional methods revealing soft tissue discrimination at micron scale resolutions. Contrast improvements can be achieved at doses rather less than those required by conventional x-ray imaging. The use of synchrotrons has revealed the possibilities offered by these techniques but unfortunately the application of these ideas in a clinical context requires that technology be pushed to its limits in a number of areas including x-ray sources, optics and detectors. The current state of the art is reviewed.},
	language = {en},
	number = {16},
	urldate = {2022-04-09},
	journal = {Physics in Medicine and Biology},
	author = {Lewis, R A},
	month = aug,
	year = {2004},
	pages = {3573--3583},
	file = {Lewis - 2004 - Medical phase contrast x-ray imaging current stat.pdf:/home/david/Zotero/storage/P8J8LUZJ/Lewis - 2004 - Medical phase contrast x-ray imaging current stat.pdf:application/pdf},
}

@article{pfeiffer_hard-x-ray_2008-1,
	title = {Hard-{X}-ray dark-field imaging using a grating interferometer},
	volume = {7},
	issn = {1476-1122, 1476-4660},
	url = {http://www.nature.com/articles/nmat2096},
	doi = {10.1038/nmat2096},
	language = {en},
	number = {2},
	urldate = {2022-04-09},
	journal = {Nature Materials},
	author = {Pfeiffer, F. and Bech, M. and Bunk, O. and Kraft, P. and Eikenberry, E. F. and Brönnimann, Ch. and Grünzweig, C. and David, C.},
	month = feb,
	year = {2008},
	pages = {134--137},
	file = {Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:/home/david/Zotero/storage/PSCETKEP/Pfeiffer et al. - 2008 - Hard-X-ray dark-field imaging using a grating inte.pdf:application/pdf},
}

@article{pfeiffer_x-ray_2009,
	title = {X-ray dark-field and phase-contrast imaging using a grating interferometer},
	volume = {105},
	issn = {0021-8979, 1089-7550},
	url = {http://aip.scitation.org/doi/10.1063/1.3115639},
	doi = {10.1063/1.3115639},
	language = {en},
	number = {10},
	urldate = {2022-04-09},
	journal = {Journal of Applied Physics},
	author = {Pfeiffer, F. and Bech, M. and Bunk, O. and Donath, T. and Henrich, B. and Kraft, P. and David, C.},
	month = may,
	year = {2009},
	pages = {102006},
	file = {Pfeiffer et al. - 2009 - X-ray dark-field and phase-contrast imaging using .pdf:/home/david/Zotero/storage/ILYC3MY2/Pfeiffer et al. - 2009 - X-ray dark-field and phase-contrast imaging using .pdf:application/pdf},
}

@article{taphorn_grating-based_2020,
	title = {Grating-based spectral {X}-ray dark-field imaging for correlation with structural size measures},
	volume = {10},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-70011-3},
	doi = {10.1038/s41598-020-70011-3},
	abstract = {Abstract
            X-ray dark-field (XDF) imaging accesses information on the small-angle scattering properties of the sample. With grating interferometry, the measured scattering signal is related to the sample’s autocorrelation function, which was previously demonstrated for simple samples, such as mono-dispersed microspheres for which the autocorrelation function is mathematically given. However, in potential clinical applications of XDF imaging, complex microstructures, such as lung parenchyma are under investigation. Their bahaviour in XDF imaging is not yet known and no mathematical description of the autocorrelation function is derived so far. In this work we demonstrate the previously established correlation of the XDF data of complex sample structures with their autocorrelation function to be impractical. Furthermore, we propose an applicable correlation between XDF and the sample’s structural parameter on the basis of mean chord length, a medically-approved measure for alveolar structure, known to be affected by structural lung diseases. Our findings reveal a correlation between energy-dependent XDF imaging and the sample’s mean chord length. By that, a connection between a medical measure for alveoli and XDF is achieved, which is particularly important regarding potential future XDF lung imaging applications for the assessment of alveoli size in diagnostic lung imaging.},
	language = {en},
	number = {1},
	urldate = {2022-04-09},
	journal = {Scientific Reports},
	author = {Taphorn, Kirsten and De Marco, Fabio and Andrejewski, Jana and Sellerer, Thorsten and Pfeiffer, Franz and Herzen, Julia},
	month = dec,
	year = {2020},
	pages = {13195},
	file = {Taphorn et al. - 2020 - Grating-based spectral X-ray dark-field imaging fo.pdf:/home/david/Zotero/storage/XGT7AZLK/Taphorn et al. - 2020 - Grating-based spectral X-ray dark-field imaging fo.pdf:application/pdf},
}

@article{gromann_-vivo_2017,
	title = {In-vivo {X}-ray {Dark}-{Field} {Chest} {Radiography} of a {Pig}},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-05101-w},
	doi = {10.1038/s41598-017-05101-w},
	language = {en},
	number = {1},
	urldate = {2022-04-09},
	journal = {Scientific Reports},
	author = {Gromann, Lukas B. and De Marco, Fabio and Willer, Konstantin and Noël, Peter B. and Scherer, Kai and Renger, Bernhard and Gleich, Bernhard and Achterhold, Klaus and Fingerle, Alexander A. and Muenzel, Daniela and Auweter, Sigrid and Hellbach, Katharina and Reiser, Maximilian and Baehr, Andrea and Dmochewitz, Michaela and Schroeter, Tobias J. and Koch, Frieder J. and Meyer, Pascal and Kunka, Danays and Mohr, Juergen and Yaroshenko, Andre and Maack, Hanns-Ingo and Pralow, Thomas and van der Heijden, Hendrik and Proksa, Roland and Koehler, Thomas and Wieberneit, Nataly and Rindt, Karsten and Rummeny, Ernst J. and Pfeiffer, Franz and Herzen, Julia},
	month = dec,
	year = {2017},
	pages = {4807},
	file = {Gromann et al. - 2017 - In-vivo X-ray Dark-Field Chest Radiography of a Pi.pdf:/home/david/Zotero/storage/XN4KIW47/Gromann et al. - 2017 - In-vivo X-ray Dark-Field Chest Radiography of a Pi.pdf:application/pdf},
}

@article{hahn_numerical_2012,
	title = {Numerical comparison of {X}-ray differential phase contrast and attenuation contrast},
	volume = {3},
	issn = {2156-7085, 2156-7085},
	url = {https://opg.optica.org/boe/abstract.cfm?uri=boe-3-6-1141},
	doi = {10.1364/BOE.3.001141},
	abstract = {We present a numerical tool to compare directly the contrastto-noise-ratio (CNR) of the attenuation- and differential phase-contrast signals available from grating-based X-ray imaging for single radiographs. The attenuation projection is differentiated to bring it into a modality comparable to the differential phase projection using a Gaussian derivative ﬁlter. A Relative Contrast Gain (RCG) is then deﬁned as the ratio of the CNR of image values in a region of interest (ROI) in the differential phase projection to the CNR of image values in the same ROI in the differential attenuation projection. We apply the method on experimental data of human breast tissue acquired using a grating interferometer to compare the two contrast modes for two regions of interest differing in the type of tissue. Our results indicate that the proposed method can be used as a local estimate of the spatial distribution of the ratio δ /β , i.e., real and imaginary part of the complex refractive index, across a sample.},
	language = {en},
	number = {6},
	urldate = {2022-04-09},
	journal = {Biomedical Optics Express},
	author = {Hahn, Dieter and Thibault, Pierre and Bech, Martin and Stockmar, Marco and Schleede, Simone and Zanette, Irene and Rack, Alexander and Weitkamp, Timm and Sztrókay, Aniko and Schlossbauer, Thomas and Bamberg, Fabian and Reiser, Maximilian and Pfeiffer, Franz},
	month = jun,
	year = {2012},
	pages = {1141},
	file = {Hahn et al. - 2012 - Numerical comparison of X-ray differential phase c.pdf:/home/david/Zotero/storage/4CFXPY6E/Hahn et al. - 2012 - Numerical comparison of X-ray differential phase c.pdf:application/pdf},
}

@article{thuring_non-linear_2011,
	title = {Non-linear regularized phase retrieval for unidirectional {X}-ray differential phase contrast radiography},
	volume = {19},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-19-25-25545},
	doi = {10.1364/OE.19.025545},
	abstract = {Phase retrieval from unidirectional radiographic differential phase contrast images requires integration of noisy data. A method is presented, which aims to suppress stripe artifacts arising from direct image integration. It is purely algorithmic and therefore, compared to alternative approaches, neither additional alignment nor an increased scan time is required. We report on the theory of this method and present results using numerical as well as experimental data. The method shows signiﬁcant improvements on the phase retrieval accuracy and enhances contrast in the phase image. Due to its general applicability, the proposed method provides a valuable tool for various 2D imaging applications using differential data.},
	language = {en},
	number = {25},
	urldate = {2022-04-09},
	journal = {Optics Express},
	author = {Thüring, Thomas and Modregger, Peter and Pinzer, Bernd R. and Wang, Zhentian and Stampanoni, Marco},
	month = dec,
	year = {2011},
	pages = {25545},
	file = {Thüring et al. - 2011 - Non-linear regularized phase retrieval for unidire.pdf:/home/david/Zotero/storage/LYREZ3QP/Thüring et al. - 2011 - Non-linear regularized phase retrieval for unidire.pdf:application/pdf},
}

@article{flohr_ct_2013,
	title = {{CT} {Systems}},
	volume = {1},
	issn = {2167-4825},
	url = {http://link.springer.com/10.1007/s40134-012-0005-5},
	doi = {10.1007/s40134-012-0005-5},
	abstract = {Computed tomography (CT) was introduced in the early 1970s, and has since then revolutionized diagnostic imaging. Today, CT is the backbone of radiology. In this article we review different CT system design concepts. We start with an overview of the ‘‘classic’’ four generations of CT systems: the ﬁrst generation head scanners relying on the ‘‘translate–rotate’’ principle, second generation scanners with a small detector array instead of a single detector, modern third generation ‘‘rotate–rotate’’ CT scanners with a detector fan wide enough to cover a whole-body scan ﬁeld of view, and ﬁnally the currently abandoned fourth generation CT systems with a rotating X-ray tube and stationary detector. We present basic concepts and a short history of multidetector row CT (MDCT), illustrating its potential and limitations. Then, we introduce novel system concepts, such as CT systems with area detectors and dual source CT, that aim at solving remaining limitations of MDCT. We explain the basic system options to acquire dual energy CT data or spectrally resolved CT data. Finally, we brieﬂy touch alternative system designs, such as electron beam CT, CT systems with inverse geometry, interior tomography CT, and phase contrast CT.},
	language = {en},
	number = {1},
	urldate = {2022-04-10},
	journal = {Current Radiology Reports},
	author = {Flohr, Thomas},
	month = mar,
	year = {2013},
	pages = {52--63},
	file = {Flohr - 2013 - CT Systems.pdf:/home/david/Zotero/storage/9G6W2SX7/Flohr - 2013 - CT Systems.pdf:application/pdf},
}

@article{van_aarle_fast_2016,
	title = {Fast and flexible {X}-ray tomography using the {ASTRA} toolbox},
	volume = {24},
	issn = {1094-4087},
	url = {https://opg.optica.org/abstract.cfm?URI=oe-24-22-25129},
	doi = {10.1364/OE.24.025129},
	abstract = {Object reconstruction from a series of projection images, such as in computed tomography (CT), is a popular tool in many diﬀerent application ﬁelds. Existing commercial software typically provides suﬃciently accurate and convenient-to-use reconstruction tools to the end-user. However, in applications where a non-standard acquisition protocol is used, or where advanced reconstruction methods are required, the standard software tools often are incapable of computing accurate reconstruction images. This article introduces the ASTRA Toolbox. Aimed at researchers across multiple tomographic application ﬁelds, the ASTRA Toolbox provides a highly eﬃcient and highly ﬂexible open source set of tools for tomographic projection and reconstruction. The main features of the ASTRA Toolbox are discussed and several use cases are presented.},
	language = {en},
	number = {22},
	urldate = {2022-04-11},
	journal = {Optics Express},
	author = {van Aarle, Wim and Palenstijn, Willem Jan and Cant, Jeroen and Janssens, Eline and Bleichrodt, Folkert and Dabravolski, Andrei and De Beenhouwer, Jan and Joost Batenburg, K. and Sijbers, Jan},
	month = oct,
	year = {2016},
	pages = {25129},
	file = {van Aarle et al. - 2016 - Fast and flexible X-ray tomography using the ASTRA.pdf:/home/david/Zotero/storage/PSCE3HDB/van Aarle et al. - 2016 - Fast and flexible X-ray tomography using the ASTRA.pdf:application/pdf},
}

@article{biguri_tigre_2016,
	title = {{TIGRE}: a {MATLAB}-{GPU} toolbox for {CBCT} image reconstruction},
	volume = {2},
	issn = {2057-1976},
	shorttitle = {{TIGRE}},
	url = {https://iopscience.iop.org/article/10.1088/2057-1976/2/5/055010},
	doi = {10.1088/2057-1976/2/5/055010},
	language = {en},
	number = {5},
	urldate = {2022-04-11},
	journal = {Biomedical Physics \& Engineering Express},
	author = {Biguri, Ander and Dosanjh, Manjit and Hancock, Steven and Soleimani, Manuchehr},
	month = sep,
	year = {2016},
	pages = {055010},
	file = {Biguri et al. - 2016 - TIGRE a MATLAB-GPU toolbox for CBCT image reconst.pdf:/home/david/Zotero/storage/6DA4DW5F/Biguri et al. - 2016 - TIGRE a MATLAB-GPU toolbox for CBCT image reconst.pdf:application/pdf},
}

@article{hoffman_technical_2016,
	title = {Technical {Note}: {FreeCT}\_wFBP: {A} robust, efficient, open‐source implementation of weighted filtered backprojection for helical, fan‐beam {CT}},
	volume = {43},
	issn = {0094-2405, 2473-4209},
	shorttitle = {Technical {Note}},
	url = {https://onlinelibrary.wiley.com/doi/10.1118/1.4941953},
	doi = {10.1118/1.4941953},
	abstract = {Purpose: With growing interest in quantitative imaging, radiomics, and CAD using CT imaging, the need to explore the impacts of acquisition and reconstruction parameters has grown. This usually requires extensive access to the scanner on which the data were acquired and its workflow is not designed for large-scale reconstruction projects. Therefore, the authors have developed a freely available, open-source software package implementing a common reconstruction method, weighted filtered backprojection (wFBP), for helical fan-beam CT applications.
Methods: FreeCT\_wFBP is a low-dependency, GPU-based reconstruction program utilizing  for the host code and Nvidia CUDA C for GPU code. The software is capable of reconstructing helical scans acquired with arbitrary pitch-values, and sampling techniques such as flying focal spots and a quarter-detector offset. In this work, the software has been described and evaluated for reconstruction speed, image quality, and accuracy. Speed was evaluated based on acquisitions of the ACR CT accreditation phantom under four different flying focal spot configurations. Image quality was assessed using the same phantom by evaluating CT number accuracy, uniformity, and contrast to noise ratio (CNR). Finally, reconstructed mass-attenuation coefficient accuracy was evaluated using a simulated scan of a FORBILD thorax phantom and comparing reconstructed values to the known phantom values.
Results: The average reconstruction time evaluated under all flying focal spot configurations was found to be 17.4 ± 1.0 s for a 512 row × 512 column × 32 slice volume. Reconstructions of the ACR phantom were found to meet all CT Accreditation Program criteria including CT number, CNR, and uniformity tests. Finally, reconstructed mass-attenuation coefficient values of water within the FORBILD thorax phantom agreed with original phantom values to within 0.0001 mm2/g (0.01\%).
Conclusions: FreeCT\_wFBP is a fast, highly configurable reconstruction package for third-generation CT available under the GNU GPL. It shows good performance with both clinical and simulated data. C 2016 American Association of Physicists in Medicine. [http://dx.doi.org/10.1118/1.4941953]},
	language = {en},
	number = {3},
	urldate = {2022-04-11},
	journal = {Medical Physics},
	author = {Hoffman, John and Young, Stefano and Noo, Frédéric and McNitt‐Gray, Michael},
	month = mar,
	year = {2016},
	pages = {1411--1420},
	file = {Hoffman et al. - 2016 - Technical Note FreeCT_wFBP A robust, efficient, .pdf:/home/david/Zotero/storage/5RXPN938/Hoffman et al. - 2016 - Technical Note FreeCT_wFBP A robust, efficient, .pdf:application/pdf},
}

@article{amanatides_fast_1987,
	title = {A {Fast} {Voxel} {Traversal} {Algorithm} for {Ray} {Tracing}},
	abstract = {A fast and simple voxel traversal algorithm through a 3D space partition is introduced. Going from one voxel to its neighbour requires only two ﬂoating point comparisons and one ﬂoating point addition. Also, multiple ray intersections with objects that are in more than one voxel are eliminated.},
	language = {en},
	author = {Amanatides, John and Woo, Andrew},
	year = {1987},
	pages = {6},
	file = {Amanatides and Woo - A Fast Voxel Traversal Algorithm for Ray Tracing.pdf:/home/david/Zotero/storage/4JBZ8H5M/Amanatides and Woo - A Fast Voxel Traversal Algorithm for Ray Tracing.pdf:application/pdf},
}

@article{shepp_fourier_1974,
	title = {The {Fourier} reconstruction of a head section},
	volume = {21},
	number = {3},
	journal = {IEEE Transactions on nuclear science},
	author = {Shepp, Lawrence A and Logan, Benjamin F},
	year = {1974},
	note = {Publisher: IEEE},
	pages = {21--43},
}

@book{hartley_multiple_2003,
	title = {Multiple view geometry in computer vision},
	publisher = {Cambridge university press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2003},
}

@article{van_der_walt_scikit-image_2014,
	title = {scikit-image: image processing in {Python}},
	volume = {2},
	issn = {2167-8359},
	shorttitle = {scikit-image},
	url = {https://peerj.com/articles/453},
	doi = {10.7717/peerj.453},
	language = {en},
	urldate = {2022-04-12},
	journal = {PeerJ},
	author = {van der Walt, Stéfan and Schönberger, Johannes L. and Nunez-Iglesias, Juan and Boulogne, François and Warner, Joshua D. and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
	month = jun,
	year = {2014},
	pages = {e453},
	file = {Full Text:/home/david/Zotero/storage/LN37BA3H/van der Walt et al. - 2014 - scikit-image image processing in Python.pdf:application/pdf},
}

@article{wang_image_2004,
	title = {Image {Quality} {Assessment}: {From} {Error} {Visibility} to {Structural} {Similarity}},
	volume = {13},
	issn = {1057-7149},
	shorttitle = {Image {Quality} {Assessment}},
	url = {http://ieeexplore.ieee.org/document/1284395/},
	doi = {10.1109/TIP.2003.819861},
	language = {en},
	number = {4},
	urldate = {2022-04-12},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	pages = {600--612},
	file = {Wang et al. - 2004 - Image Quality Assessment From Error Visibility to.pdf:/home/david/Zotero/storage/DVLK2WA2/Wang et al. - 2004 - Image Quality Assessment From Error Visibility to.pdf:application/pdf},
}

@article{wang_mean_2009,
	title = {Mean squared error: {Love} it or leave it? {A} new look at {Signal} {Fidelity} {Measures}},
	volume = {26},
	issn = {1053-5888},
	shorttitle = {Mean squared error},
	url = {http://ieeexplore.ieee.org/document/4775883/},
	doi = {10.1109/MSP.2008.930649},
	language = {en},
	number = {1},
	urldate = {2022-04-12},
	journal = {IEEE Signal Processing Magazine},
	author = {Wang, Zhou and Bovik, Alan .C.},
	month = jan,
	year = {2009},
	pages = {98--117},
	file = {Zhou Wang and Bovik - 2009 - Mean squared error Love it or leave it A new loo.pdf:/home/david/Zotero/storage/XV5JH873/Zhou Wang and Bovik - 2009 - Mean squared error Love it or leave it A new loo.pdf:application/pdf},
}

@article{avanaki_exact_2009,
	title = {Exact global histogram specification optimized for structural similarity},
	volume = {16},
	issn = {1340-6000, 1349-9432},
	url = {http://link.springer.com/10.1007/s10043-009-0119-z},
	doi = {10.1007/s10043-009-0119-z},
	language = {en},
	number = {6},
	urldate = {2022-04-12},
	journal = {Optical Review},
	author = {Avanaki, Alireza Nasiri},
	month = nov,
	year = {2009},
	pages = {613--621},
	file = {Submitted Version:/home/david/Zotero/storage/3T9EJ84N/Avanaki - 2009 - Exact global histogram specification optimized for.pdf:application/pdf},
}

@article{garduno_optimization_2004,
	title = {Optimization of basis functions for both reconstruction and visualization},
	volume = {139},
	issn = {0166218X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166218X03005353},
	doi = {10.1016/j.dam.2002.12.002},
	abstract = {Algebraic reconstruction techniques for the reconstruction of distributions from projections have yielded improvements in diverse ÿelds such as medical imaging and electron microscopy. An important property of these methods is that they allow the use of various basis functions. Recently spherically symmetric functions (blobs) have been introduced as e cacious basis functions for reconstruction. However, basis functions whose parameters were found to be appropriate for use in reconstruction are not necessarily good for visualization. We propose a method of selecting blob parameters for both reconstruction and visualization.},
	language = {en},
	number = {1-3},
	urldate = {2022-04-16},
	journal = {Discrete Applied Mathematics},
	author = {Garduño, Edgar and Herman, Gabor T.},
	month = apr,
	year = {2004},
	pages = {95--111},
	file = {Garduño and Herman - 2004 - Optimization of basis functions for both reconstru.pdf:/home/david/Zotero/storage/V4JP8WD7/Garduño and Herman - 2004 - Optimization of basis functions for both reconstru.pdf:application/pdf},
}

@article{benkarroum_blob_2015,
	title = {Blob parameter selection for image representation},
	volume = {32},
	issn = {1084-7529, 1520-8532},
	url = {https://opg.optica.org/abstract.cfm?URI=josaa-32-10-1898},
	doi = {10.1364/JOSAA.32.001898},
	language = {en},
	number = {10},
	urldate = {2022-04-16},
	journal = {Journal of the Optical Society of America A},
	author = {Benkarroum, Younes and Herman, Gabor T. and Rowland, Stuart W.},
	month = oct,
	year = {2015},
	pages = {1898},
	file = {Benkarroum et al. - 2015 - Blob parameter selection for image representation.pdf:/home/david/Zotero/storage/9KB664R9/Benkarroum et al. - 2015 - Blob parameter selection for image representation.pdf:application/pdf},
}

@article{schena_emerging_2015,
	title = {Emerging clinical applications of computed tomography},
	issn = {1179-1470},
	url = {http://www.dovepress.com/emerging-clinical-applications-of-computed-tomography-peer-reviewed-article-MDER},
	doi = {10.2147/MDER.S70630},
	abstract = {X-ray computed tomography (CT) has recently been experiencing remarkable growth as a result of technological advances and new clinical applications. This paper reviews the essential physics of X-ray CT and its major components. Also reviewed are recent promising applications of CT, ie, CT-guided procedures, CT-based thermometry, photon-counting technology, hybrid PET-CT, use of ultrafast-high pitch scanners, and potential use of dual-energy CT for material differentiations. These promising solutions and a better knowledge of their potentialities should allow CT to be used in a safe and effective manner in several clinical applications.},
	language = {en},
	urldate = {2022-04-16},
	journal = {Medical Devices: Evidence and Research},
	author = {Schena, Emiliano and Liguori, Carlo and Frauenfelder, Giulia and Massaroni, Carlo and Saccomandi, Paola and Giurazza, Francesco and Pitocco, Francesca and Marano, Riccardo},
	month = jun,
	year = {2015},
	pages = {265},
	file = {Schena et al. - 2015 - Emerging clinical applications of computed tomogra.pdf:/home/david/Zotero/storage/2MRX2LNY/Schena et al. - 2015 - Emerging clinical applications of computed tomogra.pdf:application/pdf},
}

@book{buchanan_advanced_2012,
	title = {Advanced {Medical} {Techniques} and {Equipments}.},
	isbn = {978-81-323-3223-7},
	url = {https://search-ebscohost-com.eaccess.ub.tum.de/login.aspx?direct=true&db=nlebk&AN=397007&site=ehost-live},
	abstract = {Introduction to Medical Equipment Chapter 1 - Electrocardiography Chapter 2 - Magnetic Resonance Imaging Chapter 3 - Endoscopy Chapter 4 - Medical Ultrasonography Chapter 5 - X-ray Computed Tomography Chapter 6 - Artificial Heart Chapter 7 - Anaesthetic Machine Chapter 8 - Electronic Fluency Devices Chapter 9 - Cardiopulmonary Bypass},
	publisher = {Research World},
	author = {Buchanan, Scott},
	year = {2012},
	file = {Buchanan - 2012 - Advanced Medical Techniques and Equipments..pdf:/home/david/Zotero/storage/L9XIL9X7/Buchanan - 2012 - Advanced Medical Techniques and Equipments..pdf:application/pdf},
}

@article{kalender_x-ray_2006,
	title = {X-ray computed tomography},
	volume = {51},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/51/13/R03},
	doi = {10.1088/0031-9155/51/13/R03},
	abstract = {X-ray computed tomography (CT), introduced into clinical practice in 1972, was the ﬁrst of the modern slice-imaging modalities. To reconstruct images mathematically from measured data and to display and to archive them in digital form was a novelty then and is commonplace today. CT has shown a steady upward trend with respect to technology, performance and clinical use independent of predictions and expert assessments which forecast in the 1980s that it would be completely replaced by magnetic resonance imaging. CT not only survived but exhibited a true renaissance due to the introduction of spiral scanning which meant the transition from slice-by-slice imaging to true volume imaging. Complemented by the introduction of array detector technology in the 1990s, CT today allows imaging of whole organs or the whole body in 5 to 20 s with sub-millimetre isotropic resolution. This review of CT will proceed in chronological order focussing on technology, image quality and clinical applications. In its ﬁnal part it will also brieﬂy allude to novel uses of CT such as dual-source CT, C-arm ﬂat-panel-detector CT and micro-CT. At present CT possibly exhibits a higher innovation rate than ever before. In consequence the topical and most recent developments will receive the greatest attention.},
	language = {en},
	number = {13},
	urldate = {2022-04-16},
	journal = {Physics in Medicine and Biology},
	author = {Kalender, Willi A},
	month = jul,
	year = {2006},
	pages = {R29--R43},
	file = {Kalender - 2006 - X-ray computed tomography.pdf:/home/david/Zotero/storage/7BIGTW3J/Kalender - 2006 - X-ray computed tomography.pdf:application/pdf},
}

@article{kalender_dose_2014,
	title = {Dose in x-ray computed tomography},
	volume = {59},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/59/3/R129},
	doi = {10.1088/0031-9155/59/3/R129},
	abstract = {Radiation dose in x-ray computed tomography (CT) has become a topic of high interest due to the increasing numbers of CT examinations performed worldwide. This review aims to present an overview of current concepts for both scanner output metrics and for patient dosimetry and will comment on their strengths and weaknesses. Controversial issues such as the appropriateness of the CT dose index (CTDI) are discussed in detail. A review of approaches to patient dose assessment presently in practice, of the dose levels encountered and options for further dose optimization are also given and discussed. Patient dose assessment remains a topic for further improvement and for international consensus. All approaches presently in use are based on Monte Carlo (MC) simulations. Estimates for effective dose are established, but they are crude and not patient-speciﬁc; organ dose estimates are rarely available. Patientand organ-speciﬁc dose estimates can be provided with adequate accuracy and independent of CTDI phantom measurements by fast MC simulations. Such information, in particular on 3D dose distributions, is important and helpful in optimization efforts. Dose optimization has been performed very successfully in recent years and even resulted in applications with effective dose values of below 1 mSv. In general, a trend towards lower dose values based on technical innovations has to be acknowledged. Effective dose values are down to clearly below 10 mSv on average, and there are a number of applications such as cardiac and pediatric CT which are performed routinely below 1 mSv on modern equipment.},
	language = {en},
	number = {3},
	urldate = {2022-04-16},
	journal = {Physics in Medicine and Biology},
	author = {Kalender, Willi A},
	month = feb,
	year = {2014},
	pages = {R129--R150},
	file = {Kalender - 2014 - Dose in x-ray computed tomography.pdf:/home/david/Zotero/storage/QT96SU88/Kalender - 2014 - Dose in x-ray computed tomography.pdf:application/pdf},
}

@article{pfeiffer_grating-based_2013,
	title = {Grating-based {X}-ray phase contrast for biomedical imaging applications},
	volume = {23},
	issn = {09393889},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0939388913000263},
	doi = {10.1016/j.zemedi.2013.02.002},
	abstract = {In this review article we describe the development of grating-based X-ray phase-contrast imaging, with particular emphasis on potential biomedical applications of the technology. We review the basics of image formation in grating-based phase-contrast and dark-ﬁeld radiography and present some exemplary multimodal radiography results obtained with laboratory X-ray sources. Furthermore, we discuss the theoretical concepts to extend grating-based multimodal radiography to quantitative transmission, phase-contrast, and dark-ﬁeld scattering computed tomography.},
	language = {en},
	number = {3},
	urldate = {2022-04-17},
	journal = {Zeitschrift für Medizinische Physik},
	author = {Pfeiffer, Franz and Herzen, Julia and Willner, Marian and Chabior, Michael and Auweter, Sigrid and Reiser, Maximilian and Bamberg, Fabian},
	month = sep,
	year = {2013},
	pages = {176--185},
	file = {Pfeiffer et al. - 2013 - Grating-based X-ray phase contrast for biomedical .pdf:/home/david/Zotero/storage/BBFDH4R6/Pfeiffer et al. - 2013 - Grating-based X-ray phase contrast for biomedical .pdf:application/pdf},
}

@article{donath_inverse_2009,
	title = {Inverse geometry for grating-based x-ray phase-contrast imaging},
	volume = {106},
	issn = {0021-8979, 1089-7550},
	url = {http://aip.scitation.org/doi/10.1063/1.3208052},
	doi = {10.1063/1.3208052},
	language = {en},
	number = {5},
	urldate = {2022-04-17},
	journal = {Journal of Applied Physics},
	author = {Donath, Tilman and Chabior, Michael and Pfeiffer, Franz and Bunk, Oliver and Reznikova, Elena and Mohr, Juergen and Hempel, Eckhard and Popescu, Stefan and Hoheisel, Martin and Schuster, Manfred and Baumann, Joachim and David, Christian},
	month = sep,
	year = {2009},
	pages = {054703},
	file = {Donath et al. - 2009 - Inverse geometry for grating-based x-ray phase-con.pdf:/home/david/Zotero/storage/CKQ582RG/Donath et al. - 2009 - Inverse geometry for grating-based x-ray phase-con.pdf:application/pdf},
}

@article{huesman_reclbl_1977,
	title = {{RECLBL} library users manual-donner algorithms for reconstruction tomography. {Lawrence} {Berkeley} {Lab}},
	journal = {Report PUB-214},
	author = {Huesman, RH and Gullberg, GT and Greenberg, WL and Budinger, TF},
	year = {1977},
	file = {Huesman et al. - 1977 - RECLBL library users manual-donner algorithms for .pdf:/home/david/Zotero/storage/3P3X95ZI/Huesman et al. - 1977 - RECLBL library users manual-donner algorithms for .pdf:application/pdf},
}

@article{kulvait_cutting_2021,
	title = {Cutting {Voxel} {Projector} a {New} {Approach} to {Construct} {3D} {Cone} {Beam} {CT} {Operator}},
	url = {http://arxiv.org/abs/2110.09841},
	abstract = {In this paper, we introduce a new class of projectors for 3D cone beam tomographic reconstruction. We ﬁnd analytical formulas for the relationship between the voxel volume projected onto a given detector pixel and its contribution to the extinction value detected on that pixel. Using this approach, we construct a near-exact projector and backprojector that can be used especially for algebraic reconstruction techniques. We have implemented this cutting voxel projector and a less accurate, speed-optimized version of it together with two established projectors, a ray tracing projector based on Siddon’s algorithm and a TT footprint projector. We show that the cutting voxel projector achieves, especially for large cone beam angles, noticeably higher accuracy than the TT projector. Moreover, our implementation of the relaxed version of the cutting voxel projector is signiﬁcantly faster than current footprint projector implementations. We further show that Siddon’s algorithm with comparable accuracy would be much slower than the cutting voxel projector. All algorithms are implemented within an open source framework for algebraic reconstruction in OpenCL 1.2 and C++ and are optimized for GPU computation. They are published as open-source software under the GNU GPL 3 license, see https://github.com/kulvait/KCT cbct.},
	language = {en},
	urldate = {2022-04-19},
	journal = {arXiv:2110.09841 [physics]},
	author = {Kulvait, Vojtěch and Rose, Georg},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09841},
	keywords = {Mathematics - Numerical Analysis, Computer Science - Computer Vision and Pattern Recognition, Physics - Medical Physics, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Kulvait and Rose - 2021 - Cutting Voxel Projector a New Approach to Construc.pdf:/home/david/Zotero/storage/P6G8YHWS/Kulvait and Rose - 2021 - Cutting Voxel Projector a New Approach to Construc.pdf:application/pdf},
}

@article{zeng_unmatched_2000,
	title = {Unmatched projector/backprojector pairs in an iterative reconstruction algorithm},
	volume = {19},
	issn = {1558-254X},
	doi = {10.1109/42.870265},
	abstract = {Computational burden is a major concern when an iterative algorithm is used to reconstruct a three-dimensional (3-D) image with attenuation, detector response, and scatter corrections. Most of the computation time is spent executing the projector and backprojector of an iterative algorithm. Usually, the projector and the backprojector are transposed operators of each other. The projector should model the imaging geometry and physics as accurately as possible. Some researchers have used backprojectors that are computationally less expensive than the projectors to reduce computation time. This paper points out that valid backprojectors should satisfy a condition that the projector/backprojector matrix must not contain negative eigenvalues. This paper also investigates the effects when unmatched projector/backprojector pairs are used.},
	number = {5},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Zeng, G.L. and Gullberg, G.T.},
	month = may,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Attenuation, Detectors, Eigenvalues and eigenfunctions, Geometry, Image reconstruction, Iterative algorithms, Physics, Reconstruction algorithms, Scattering, Solid modeling},
	pages = {548--555},
	file = {IEEE Xplore Full Text PDF:/home/david/Zotero/storage/LAX39LCT/Zeng and Gullberg - 2000 - Unmatched projectorbackprojector pairs in an iter.pdf:application/pdf;IEEE Xplore Abstract Record:/home/david/Zotero/storage/6DICZVWL/870265.html:text/html},
}

@article{zeng_counter_2019,
	title = {Counter examples for unmatched projector/backprojector in an iterative algorithm},
	volume = {1},
	issn = {2520-8985, 2520-8993},
	url = {http://link.springer.com/10.1007/s42058-019-00006-1},
	doi = {10.1007/s42058-019-00006-1},
	abstract = {It is rather controversial whether it is justified to use an unmatched projector/backprojector pair in an iterative image reconstruction algorithm. One common concern of using an unmatched projector/backprojector pair is that the optimal solution cannot be reached. This concern is misleading and must be clarified. We define a figure-of-merit in the image domain as the distance between the reconstructed image and the true image, as the normalized mean-squared-error (NMSE). The NMSE is used to determine whether an unmatched matched projector/backprojector pair can provide a better image than a matched projector/backprojector pair. Hot and cold lesion’s contrast-to-noise ratio is also used as an alternative secondary figureof-merit for algorithm comparison. Computer-generated counterexamples are used to test the performance for matched and unmatched projection/backprojection pairs for different reconstruction algorithms. The projectors are ray-driven, and the backprojectors are ray-driven and pixel-driven. For the attenuation-free data examples, the unmatched pixel-driven backprojector outperforms the matched ray-driven backprojector. For the attenuated data example, the matched ray-driven backprojector performs better. The ray-driven backprojector can be slightly improved by using an attenuation coefficient that is larger than the true one; in this case the backprojector becomes unmatched. Unmatched projector/backprojector pairs are fairly flexible. If the backprojector is properly chosen, good results can be obtained. However, we have not found a general rule to select a good backprojector.},
	language = {en},
	number = {1},
	urldate = {2022-04-19},
	journal = {Chinese Journal of Academic Radiology},
	author = {Zeng, Gengsheng L.},
	month = jul,
	year = {2019},
	pages = {13--24},
	file = {Zeng - 2019 - Counter examples for unmatched projectorbackproje.pdf:/home/david/Zotero/storage/XXVTERRR/Zeng - 2019 - Counter examples for unmatched projectorbackproje.pdf:application/pdf},
}

@article{kalender_dose_2014-1,
	title = {Dose in x-ray computed tomography},
	volume = {59},
	issn = {0031-9155, 1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/0031-9155/59/3/R129},
	doi = {10.1088/0031-9155/59/3/R129},
	abstract = {Radiation dose in x-ray computed tomography (CT) has become a topic of high interest due to the increasing numbers of CT examinations performed worldwide. This review aims to present an overview of current concepts for both scanner output metrics and for patient dosimetry and will comment on their strengths and weaknesses. Controversial issues such as the appropriateness of the CT dose index (CTDI) are discussed in detail. A review of approaches to patient dose assessment presently in practice, of the dose levels encountered and options for further dose optimization are also given and discussed. Patient dose assessment remains a topic for further improvement and for international consensus. All approaches presently in use are based on Monte Carlo (MC) simulations. Estimates for effective dose are established, but they are crude and not patient-speciﬁc; organ dose estimates are rarely available. Patientand organ-speciﬁc dose estimates can be provided with adequate accuracy and independent of CTDI phantom measurements by fast MC simulations. Such information, in particular on 3D dose distributions, is important and helpful in optimization efforts. Dose optimization has been performed very successfully in recent years and even resulted in applications with effective dose values of below 1 mSv. In general, a trend towards lower dose values based on technical innovations has to be acknowledged. Effective dose values are down to clearly below 10 mSv on average, and there are a number of applications such as cardiac and pediatric CT which are performed routinely below 1 mSv on modern equipment.},
	language = {en},
	number = {3},
	urldate = {2022-04-19},
	journal = {Physics in Medicine and Biology},
	author = {Kalender, Willi A},
	month = feb,
	year = {2014},
	pages = {R129--R150},
	file = {Kalender - 2014 - Dose in x-ray computed tomography.pdf:/home/david/Zotero/storage/YPVYDELL/Kalender - 2014 - Dose in x-ray computed tomography.pdf:application/pdf},
}

@article{willemink_evolution_2019,
	title = {The evolution of image reconstruction for {CT}—from filtered back projection to artificial intelligence},
	volume = {29},
	issn = {0938-7994, 1432-1084},
	url = {http://link.springer.com/10.1007/s00330-018-5810-7},
	doi = {10.1007/s00330-018-5810-7},
	abstract = {The first CT scanners in the early 1970s already used iterative reconstruction algorithms; however, lack of computational power prevented their clinical use. In fact, it took until 2009 for the first iterative reconstruction algorithms to come commercially available and replace conventional filtered back projection. Since then, this technique has caused a true hype in the field of radiology. Within a few years, all major CT vendors introduced iterative reconstruction algorithms for clinical routine, which evolved rapidly into increasingly advanced reconstruction algorithms. The complexity of algorithms ranges from hybrid-, modelbased to fully iterative algorithms. As a result, the number of scientific publications on this topic has skyrocketed over the last decade. But what exactly has this technology brought us so far? And what can we expect from future hardware as well as software developments, such as photon-counting CT and artificial intelligence? This paper will try answer those questions by taking a concise look at the overall evolution of CT image reconstruction and its clinical implementations. Subsequently, we will give a prospect towards future developments in this domain.},
	language = {en},
	number = {5},
	urldate = {2022-04-19},
	journal = {European Radiology},
	author = {Willemink, Martin J. and Noël, Peter B.},
	month = may,
	year = {2019},
	pages = {2185--2195},
	file = {Willemink and Noël - 2019 - The evolution of image reconstruction for CT—from .pdf:/home/david/Zotero/storage/LXT9KZ95/Willemink and Noël - 2019 - The evolution of image reconstruction for CT—from .pdf:application/pdf},
}
